<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>仰望星空</title>
  
  <subtitle>keep learning</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-02-27T07:33:34.937Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>王子晰</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>ResNet-18</title>
    <link href="http://yoursite.com/2020/02/27/ResNet-18/"/>
    <id>http://yoursite.com/2020/02/27/ResNet-18/</id>
    <published>2020-02-27T03:54:50.026Z</published>
    <updated>2020-02-27T07:33:34.937Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p><strong>Deep Residual Learning for Image Recognition</strong></p><p>ResNet是何凯明（微软亚洲AI研究院工作）提出的残差神经网络，曾经在Kaggle等平台上获得多次大奖。</p><a id="more"></a><h3 id="为什么提出ResNet"><a href="#为什么提出ResNet" class="headerlink" title="为什么提出ResNet"></a>为什么提出ResNet</h3><p>众所周知，随着神经网络的发展，深度越大，网络的表达性能就越好，可实际训练的时候，随着深度的加大，网络出现了梯度弥散（也有叫梯度消失等）的情况。</p><blockquote><p>比如说在原始的网络当中，输入变量每经过一层就通过一次sigmoid激活函数，由于sigmoid函数只在0附近梯度变化明显，远离0附近梯度变化趋近于0，因此随着网络的深化，梯度变化趋于0，相当于线性恒等映射，深化的网络是做了无用功。</p></blockquote><p>为了解决该问题，人们想了一些办法，比如说改变激活函数使用relu，Leaky—relu，或者Batch Normalization等，但是不能从根本上解决问题，因此何凯明提出了ResNet（残差神经网络）</p><p>其他参考</p><blockquote><p>虽然通过Batch Normalization或者正则初始化等能够训练了，但是又会出现另一个问题，就是<strong>退化问题</strong>，网络层数增加，但是在训练集上的准确率却饱和甚至下降了。<br>退化问题说明了深度网络不能很简单地被很好地优化</p></blockquote><h3 id="ResNet是什么？怎么解决梯度弥散以及退化问题？"><a href="#ResNet是什么？怎么解决梯度弥散以及退化问题？" class="headerlink" title="ResNet是什么？怎么解决梯度弥散以及退化问题？"></a>ResNet是什么？怎么解决梯度弥散以及退化问题？</h3><h4 id="两种mapping"><a href="#两种mapping" class="headerlink" title="两种mapping"></a>两种mapping</h4><p>何凯明在ResNet中提出两种mapping：</p><ol><li>identity mapping(处理图像中也称叫feature map)：就是本身，即下图中的x</li><li>residual mapping：指的公式中的$F(x)$ <h4 id="残差函数"><a href="#残差函数" class="headerlink" title="残差函数"></a>残差函数</h4>ResNet中通过学习残差函数来解决问题，学习差值比学习梯度变化要容易的多,摘录知乎解释：<blockquote><p>F是求和前网络映射，H是从输入到求和后的网络映射。比如把5映射到5.1，那么引入残差前是F’(5)=5.1，引入残差后是H(5)=5.1, H(5)=F(5)+5, F(5)=0.1。这里的F’和F都表示网络参数映射，引入残差后的映射对输出的变化更敏感。比如s输出从5.1变到5.2，映射F’的输出增加了1/51=2%，而对于残差结构输出从5.1到5.2，映射F是从0.1到0.2，增加了100%。明显后者输出变化对权重的调整作用更大，所以效果更好。残差的思想都是去掉相同的主体部分，从而突出微小的变化，看到残差网络我第一反应就是差分放大器…（博主：哈哈，刚开始接触ResNet的时候我还没学到差分放大器）</p></blockquote></li></ol><p>上述中的$H(x)=F(x)+x$，$F(x)$为残差函数，如果$F(x)=0$，则为恒等映射，这样设计网络可以保证随着深度的加大，不论怎么训练，至少层数更深的网络训练的效果不会比层数浅的网络效果差，网络会一直处于最优状态(理论上)，且残差拟合更加容易，学习速度更快。</p><h4 id="Residual-Block"><a href="#Residual-Block" class="headerlink" title="Residual Block"></a>Residual Block</h4><p><img src="https://upload-images.jianshu.io/upload_images/6095626-49ac0caeb5525b93.png" alt=""></p><p>上图为Residual Block，可以看到输入变量x，通过两层网络和x（通过shortcut）进行element-wise add（就是对应元素加到一起，element-wise是对应元素相乘），然后再经过一个relu输出就是一个Residual Block。</p><p><img src="https://upload-images.jianshu.io/upload_images/6095626-287fc59a3cd86488.png" alt=""></p><p>这两种结构常用在ResNet18，ResNet34(左图)，ResNet50/101/152(右图)，其中右图又被称作”bottleneck”</p><h3 id="ResNet-18-Cifar-10"><a href="#ResNet-18-Cifar-10" class="headerlink" title="ResNet-18 Cifar-10"></a>ResNet-18 Cifar-10</h3><h4 id="Cifar-10"><a href="#Cifar-10" class="headerlink" title="Cifar-10"></a>Cifar-10</h4><p>这个数据集包含60000张32*32的彩色图片，这些图片一共被分成10类，有小猫，小狗等……详见：<a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></p><h4 id="ResNet-18网络结构"><a href="#ResNet-18网络结构" class="headerlink" title="ResNet-18网络结构"></a>ResNet-18网络结构</h4><p><img src="https://img-blog.csdn.net/20180426215052446?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1bnFpYW5kZTg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><blockquote><p>上图中虚线表示channel改变，实线表示channel不变</p></blockquote><h4 id="实现代码1"><a href="#实现代码1" class="headerlink" title="实现代码1"></a>实现代码1</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  torch</span><br><span class="line"><span class="keyword">from</span>    torch <span class="keyword">import</span>  nn</span><br><span class="line"><span class="keyword">from</span>    torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span>    torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span>    torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span>    torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span>    torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># from    torchvision.models import resnet18</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlk</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    resnet block</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ch_in, ch_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param ch_in:</span></span><br><span class="line"><span class="string">        :param ch_out:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ResBlk, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(ch_out)</span><br><span class="line">        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(ch_out)</span><br><span class="line"></span><br><span class="line">        self.extra = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> ch_out != ch_in:</span><br><span class="line">            <span class="comment"># [b, ch_in, h, w] =&gt; [b, ch_out, h, w]</span></span><br><span class="line">            self.extra = nn.Sequential(</span><br><span class="line">                nn.Conv2d(ch_in, ch_out, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>),<span class="comment">#既然维度不一样，为什么用卷积而不用别的呢？</span></span><br><span class="line">                nn.BatchNorm2d(ch_out)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param x: [b, ch, h, w]</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        out = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        out = self.bn2(self.conv2(out))</span><br><span class="line">        <span class="comment"># short cut.</span></span><br><span class="line">        <span class="comment"># extra module: [b, ch_in, h, w] =&gt; [b, ch_out, h, w]</span></span><br><span class="line">        <span class="comment"># element-wise add:</span></span><br><span class="line">        out = self.extra(x) + out  </span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet18</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ResNet18, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># followed 4 blocks</span></span><br><span class="line">        <span class="comment"># [b, 64, h, w] =&gt; [b, 128, h ,w]</span></span><br><span class="line">        self.blk1 = ResBlk(<span class="number">16</span>, <span class="number">16</span>)</span><br><span class="line">        <span class="comment"># [b, 128, h, w] =&gt; [b, 256, h, w]</span></span><br><span class="line">        self.blk2 = ResBlk(<span class="number">16</span>, <span class="number">32</span>)</span><br><span class="line">        <span class="comment"># # [b, 256, h, w] =&gt; [b, 512, h, w]</span></span><br><span class="line">        <span class="comment"># self.blk3 = ResBlk(128, 256)</span></span><br><span class="line">        <span class="comment"># # [b, 512, h, w] =&gt; [b, 1024, h, w]</span></span><br><span class="line">        <span class="comment"># self.blk4 = ResBlk(256, 512)</span></span><br><span class="line"></span><br><span class="line">        self.outlayer = nn.Linear(<span class="number">32</span>*<span class="number">32</span>*<span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param x:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [b, 64, h, w] =&gt; [b, 1024, h, w]</span></span><br><span class="line">        x = self.blk1(x)</span><br><span class="line">        x = self.blk2(x)</span><br><span class="line">        <span class="comment"># x = self.blk3(x)</span></span><br><span class="line">        <span class="comment"># x = self.blk4(x)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        x = self.outlayer(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    batchsz = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">    cifar_train = datasets.CIFAR10(<span class="string">'cifar'</span>, <span class="literal">True</span>, transform=transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">    ]), download=<span class="literal">True</span>)</span><br><span class="line">    cifar_train = DataLoader(cifar_train, batch_size=batchsz, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    cifar_test = datasets.CIFAR10(<span class="string">'cifar'</span>, <span class="literal">False</span>, transform=transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">    ]), download=<span class="literal">True</span>)</span><br><span class="line">    cifar_test = DataLoader(cifar_test, batch_size=batchsz, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    x, label = iter(cifar_train).next()</span><br><span class="line">    print(<span class="string">'x:'</span>, x.shape, <span class="string">'label:'</span>, label.shape)</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">'cuda'</span>)</span><br><span class="line">    <span class="comment"># model = Lenet5().to(device)</span></span><br><span class="line">    model = ResNet18().to(device)</span><br><span class="line"></span><br><span class="line">    criteon = nn.CrossEntropyLoss().to(device)</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">    print(model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line"></span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> batchidx, (x, label) <span class="keyword">in</span> enumerate(cifar_train):</span><br><span class="line">            <span class="comment"># [b, 3, 32, 32]</span></span><br><span class="line">            <span class="comment"># [b]</span></span><br><span class="line">            x, label = x.to(device), label.to(device)</span><br><span class="line"></span><br><span class="line">            logits = model(x)</span><br><span class="line">            <span class="comment"># logits: [b, 10]</span></span><br><span class="line">            <span class="comment"># label:  [b]</span></span><br><span class="line">            <span class="comment"># loss: tensor scalar</span></span><br><span class="line">            loss = criteon(logits, label)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># backprop</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        print(epoch, <span class="string">'loss:'</span>, loss.item())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        model.eval()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># test</span></span><br><span class="line">            total_correct = <span class="number">0</span></span><br><span class="line">            total_num = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> x, label <span class="keyword">in</span> cifar_test:</span><br><span class="line">                <span class="comment"># [b, 3, 32, 32]</span></span><br><span class="line">                <span class="comment"># [b]</span></span><br><span class="line">                x, label = x.to(device), label.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># [b, 10]</span></span><br><span class="line">                logits = model(x)</span><br><span class="line">                <span class="comment"># [b]</span></span><br><span class="line">                pred = logits.argmax(dim=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># [b] vs [b] =&gt; scalar tensor</span></span><br><span class="line">                correct = torch.eq(pred, label).float().sum().item()</span><br><span class="line">                total_correct += correct</span><br><span class="line">                total_num += x.size(<span class="number">0</span>)</span><br><span class="line">                <span class="comment"># print(correct)</span></span><br><span class="line"></span><br><span class="line">            acc = total_correct / total_num</span><br><span class="line">            print(epoch, <span class="string">'acc:'</span>, acc)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><ol><li>该代码没有完整实现ResNet-18结构，只实现了两residual Block。后面我自己会补上</li><li>初学ResNet，这个代码还是很OK的。<h4 id="实现代码2"><a href="#实现代码2" class="headerlink" title="实现代码2"></a>实现代码2</h4><blockquote><p>注：这段代码摘录于CSDN，由作者所说，acc = 95.170%，是完整实现ResNet-18，且封装性优于上述代码，参考价值很高</p></blockquote><h5 id="Pytorch上搭建ResNet-18："><a href="#Pytorch上搭建ResNet-18：" class="headerlink" title="Pytorch上搭建ResNet-18："></a>Pytorch上搭建ResNet-18：</h5><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''ResNet-18 Image classfication for cifar-10 with PyTorch </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Author 'Sun-qian'.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inchannel, outchannel, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(ResidualBlock, self).__init__()</span><br><span class="line">        self.left = nn.Sequential(</span><br><span class="line">            nn.Conv2d(inchannel, outchannel, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outchannel),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(outchannel, outchannel, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outchannel)</span><br><span class="line">        )</span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> inchannel != outchannel:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(inchannel, outchannel, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(outchannel)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.left(x)</span><br><span class="line">        out += self.shortcut(x)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ResidualBlock, num_classes=<span class="number">10</span>)</span>:</span></span><br><span class="line">        super(ResNet, self).__init__()</span><br><span class="line">        self.inchannel = <span class="number">64</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line">        self.layer1 = self.make_layer(ResidualBlock, <span class="number">64</span>,  <span class="number">2</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.layer2 = self.make_layer(ResidualBlock, <span class="number">128</span>, <span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self.make_layer(ResidualBlock, <span class="number">256</span>, <span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self.make_layer(ResidualBlock, <span class="number">512</span>, <span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_layer</span><span class="params">(self, block, channels, num_blocks, stride)</span>:</span></span><br><span class="line">        strides = [stride] + [<span class="number">1</span>] * (num_blocks - <span class="number">1</span>)   <span class="comment">#strides=[1,1]</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> stride <span class="keyword">in</span> strides:</span><br><span class="line">            layers.append(block(self.inchannel, channels, stride))</span><br><span class="line">            self.inchannel = channels</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.layer1(out)</span><br><span class="line">        out = self.layer2(out)</span><br><span class="line">        out = self.layer3(out)</span><br><span class="line">        out = self.layer4(out)</span><br><span class="line">        out = F.avg_pool2d(out, <span class="number">4</span>)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet18</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ResNet(ResidualBlock)</span><br></pre></td></tr></table></figure><h5 id="Pytorch上训练："><a href="#Pytorch上训练：" class="headerlink" title="Pytorch上训练："></a>Pytorch上训练：</h5><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> resnet <span class="keyword">import</span> ResNet18</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义是否使用GPU</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数设置,使得我们能够手动输入命令行参数，就是让风格变得和Linux命令行差不多</span></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'PyTorch CIFAR10 Training'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--outf'</span>, default=<span class="string">'./model/'</span>, help=<span class="string">'folder to output images and model checkpoints'</span>) <span class="comment">#输出结果保存路径</span></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数设置</span></span><br><span class="line">EPOCH = <span class="number">135</span>   <span class="comment">#遍历数据集次数</span></span><br><span class="line">pre_epoch = <span class="number">0</span>  <span class="comment"># 定义已经遍历数据集的次数</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span>      <span class="comment">#批处理尺寸(batch_size)</span></span><br><span class="line">LR = <span class="number">0.01</span>        <span class="comment">#学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集并预处理</span></span><br><span class="line">transform_train = transforms.Compose([</span><br><span class="line">    transforms.RandomCrop(<span class="number">32</span>, padding=<span class="number">4</span>),  <span class="comment">#先四周填充0，在吧图像随机裁剪成32*32</span></span><br><span class="line">    transforms.RandomHorizontalFlip(),  <span class="comment">#图像一半的概率翻转，一半的概率不翻转</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>)), <span class="comment">#R,G,B每层的归一化用到的均值和方差</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">transform_test = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform_train) <span class="comment">#训练数据集</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)   <span class="comment">#生成一个个batch进行批训练，组成batch的时候顺序打乱取</span></span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform_test)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">100</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Cifar-10的标签</span></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>, <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型定义-ResNet</span></span><br><span class="line">net = ResNet18().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化方式</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment">#损失函数为交叉熵，多用于多分类问题</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=LR, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">5e-4</span>) <span class="comment">#优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(args.outf):</span><br><span class="line">os.makedirs(args.outf)</span><br><span class="line">    best_acc = <span class="number">85</span>  <span class="comment">#2 初始化best test accuracy</span></span><br><span class="line">    print(<span class="string">"Start Training, Resnet-18!"</span>)  <span class="comment"># 定义遍历数据集的次数</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"acc.txt"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">"log.txt"</span>, <span class="string">"w"</span>)<span class="keyword">as</span> f2:</span><br><span class="line">            <span class="keyword">for</span> epoch <span class="keyword">in</span> range(pre_epoch, EPOCH):</span><br><span class="line">                print(<span class="string">'\nEpoch: %d'</span> % (epoch + <span class="number">1</span>))</span><br><span class="line">                net.train()</span><br><span class="line">                sum_loss = <span class="number">0.0</span></span><br><span class="line">                correct = <span class="number">0.0</span></span><br><span class="line">                total = <span class="number">0.0</span></span><br><span class="line">                <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">                    <span class="comment"># 准备数据</span></span><br><span class="line">                    length = len(trainloader)</span><br><span class="line">                    inputs, labels = data</span><br><span class="line">                    inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">                    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># forward + backward</span></span><br><span class="line">                    outputs = net(inputs)</span><br><span class="line">                    loss = criterion(outputs, labels)</span><br><span class="line">                    loss.backward()</span><br><span class="line">                    optimizer.step()</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 每训练1个batch打印一次loss和准确率</span></span><br><span class="line">                    sum_loss += loss.item()</span><br><span class="line">                    _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">                    total += labels.size(<span class="number">0</span>)</span><br><span class="line">                    correct += predicted.eq(labels.data).cpu().sum()</span><br><span class="line">                    print(<span class="string">'[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% '</span></span><br><span class="line">                          % (epoch + <span class="number">1</span>, (i + <span class="number">1</span> + epoch * length), sum_loss / (i + <span class="number">1</span>), <span class="number">100.</span> * correct / total))</span><br><span class="line">                    f2.write(<span class="string">'%03d  %05d |Loss: %.03f | Acc: %.3f%% '</span></span><br><span class="line">                          % (epoch + <span class="number">1</span>, (i + <span class="number">1</span> + epoch * length), sum_loss / (i + <span class="number">1</span>), <span class="number">100.</span> * correct / total))</span><br><span class="line">                    f2.write(<span class="string">'\n'</span>)</span><br><span class="line">                    f2.flush()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 每训练完一个epoch测试一下准确率</span></span><br><span class="line">                print(<span class="string">"Waiting Test!"</span>)</span><br><span class="line">                <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                    correct = <span class="number">0</span></span><br><span class="line">                    total = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">                        net.eval()</span><br><span class="line">                        images, labels = data</span><br><span class="line">                        images, labels = images.to(device), labels.to(device)</span><br><span class="line">                        outputs = net(images)</span><br><span class="line">                        <span class="comment"># 取得分最高的那个类 (outputs.data的索引号)</span></span><br><span class="line">                        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">                        total += labels.size(<span class="number">0</span>)</span><br><span class="line">                        correct += (predicted == labels).sum()</span><br><span class="line">                    print(<span class="string">'测试分类准确率为：%.3f%%'</span> % (<span class="number">100</span> * correct / total))</span><br><span class="line">                    acc = <span class="number">100.</span> * correct / total</span><br><span class="line">                    <span class="comment"># 将每次测试结果实时写入acc.txt文件中</span></span><br><span class="line">                    print(<span class="string">'Saving model......'</span>)</span><br><span class="line">                    torch.save(net.state_dict(), <span class="string">'%s/net_%03d.pth'</span> % (args.outf, epoch + <span class="number">1</span>))</span><br><span class="line">                    f.write(<span class="string">"EPOCH=%03d,Accuracy= %.3f%%"</span> % (epoch + <span class="number">1</span>, acc))</span><br><span class="line">                    f.write(<span class="string">'\n'</span>)</span><br><span class="line">                    f.flush()</span><br><span class="line">                    <span class="comment"># 记录最佳测试分类准确率并写入best_acc.txt文件中</span></span><br><span class="line">                    <span class="keyword">if</span> acc &gt; best_acc:</span><br><span class="line">                        f3 = open(<span class="string">"best_acc.txt"</span>, <span class="string">"w"</span>)</span><br><span class="line">                        f3.write(<span class="string">"EPOCH=%d,best_acc= %.3f%%"</span> % (epoch + <span class="number">1</span>, acc))</span><br><span class="line">                        f3.close()</span><br><span class="line">                        best_acc = acc</span><br><span class="line">            print(<span class="string">"Training Finished, TotalEPOCH=%d"</span> % EPOCH)</span><br></pre></td></tr></table></figure><h5 id="实现效果"><a href="#实现效果" class="headerlink" title="实现效果"></a>实现效果</h5><img src="https://img-blog.csdn.net/20180426220936286?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1bnFpYW5kZTg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></li></ol><blockquote><p>注：该图像是作者将数据下载到.txt文件，然后再matlab中进行生成</p></blockquote><h5 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h5><ol><li>将数据导入.txt文件，用matlab处理<blockquote><p>确实是好方法，visdom用起来也会很方便</p></blockquote></li><li>定义GPU是否使用的写法</li><li>ResNet-18模块封装性很好，完全符合结构<h5 id="困惑"><a href="#困惑" class="headerlink" title="困惑"></a>困惑</h5>在make_layer那里最后一句的return nn.Sequential(<em>layers)中的</em>layers是什么意思呢，上面加*是什么意思呢？<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3></li><li><a href="https://blog.csdn.net/sunqiande88/article/details/80100891">https://blog.csdn.net/sunqiande88/article/details/80100891</a></li><li><a href="http://www.jeepxie.net/article/601129.html">http://www.jeepxie.net/article/601129.html</a></li><li><a href="https://www.jianshu.com/p/e58437f39f65">https://www.jianshu.com/p/e58437f39f65</a></li><li><a href="https://www.zhihu.com/question/53224378/answer/159102095">https://www.zhihu.com/question/53224378/answer/159102095</a></li><li>Cifar-10：<a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Deep Residual Learning for Image Recognition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ResNet是何凯明（微软亚洲AI研究院工作）提出的残差神经网络，曾经在Kaggle等平台上获得多次大奖。&lt;/p&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-6 双极型晶体三管工作原理</title>
    <link href="http://yoursite.com/2020/02/26/ch4-6%20%E5%8F%8C%E6%9E%81%E5%9E%8B%E6%99%B6%E4%BD%93%E4%B8%89%E7%AE%A1%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2020/02/26/ch4-6%20%E5%8F%8C%E6%9E%81%E5%9E%8B%E6%99%B6%E4%BD%93%E4%B8%89%E7%AE%A1%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/</id>
    <published>2020-02-26T04:44:16.054Z</published>
    <updated>2020-02-26T08:37:13.978Z</updated>
    
    <content type="html"><![CDATA[<h4 id="晶体管的结构以及符号"><a href="#晶体管的结构以及符号" class="headerlink" title="晶体管的结构以及符号"></a>晶体管的结构以及符号</h4><ol><li><p>结构：发射区（e，emitter）集电区（c，collector），基区（b，basic）</p><blockquote><p>发射区重掺杂，基区轻掺杂，集电区面积大</p></blockquote></li><li><p>符号：箭头：从P区指向N区</p><a id="more"></a><h4 id="晶体管工作原理"><a href="#晶体管工作原理" class="headerlink" title="晶体管工作原理"></a>晶体管工作原理</h4><p>看图比较好理解，这里我不放图了，简单叙述一下工作过程</p><blockquote><p>在晶体管在发射结正偏，集电结反偏的情况下：</p></blockquote></li><li><p>发射结电子因为发射结正偏，扩散运动——&gt;基区，其中少部分电子和基结的空穴复合</p></li><li><p>由于基区轻掺杂，扩散运动过来的自由电子只有少部分在基区和空穴复合</p></li><li><p>由于集电区反偏，形成强大电场，产生漂移运动，基区大部分电子漂移到集电区</p></li><li><p>因为集电区反偏，所以基区和集电区的少子相互漂移，形成反向饱和电流（很小，和温度关系很大，与之前PN结的情况是一样的）</p><blockquote><p>在晶体三极管中在基区的作用下，把电子从发射区几乎全部传到了集电区，这是放大功能的体现（<strong><em>这哪里体现了放大？？睁眼说瞎</em></strong>）</p></blockquote><h4 id="比例系数-beta-alpha"><a href="#比例系数-beta-alpha" class="headerlink" title="比例系数 $\beta \alpha$"></a>比例系数 $\beta \alpha$</h4></li><li><p>$\overline{\beta}$:共发射极直流电流放大系数。忽略集电区和基区的空穴和自由电子的漂移运动产生的电流（因为电流很小嘛）所以可以近似看作：<br>$$\overline{\beta}=\cfrac{I_C}{I_B}$$</p><blockquote><p>当$I_B=0$时，$I_C$中仍有电流，我们称为穿透电流（是之前忽略的一小部分来的）<br>$$\overline{\beta}=\cfrac{I_C}{I_B}$$<br>$$I_C=\overline{\beta}I_B$$<br>$$I_E=(1+\overline{\beta})I_B$$<br>上式公式牢记</p></blockquote></li><li><p>$\overline{\alpha}$:共基极直流电流放大系数。同样忽略集电区和基区的空穴和自由电子的漂移运动产生的电流，所以可以近似看作：<br>$$\overline{\alpha}=\cfrac{I_C}{I_E}$$</p></li><li><p>$\overline{\alpha}$和$\overline{\beta}$的关系<br>$$\overline{\beta}=\cfrac{\overline{\alpha}}{1-\overline{\alpha}}$$<br>$$\overline{\alpha}=\cfrac{\overline{\beta}}{1+\overline{\beta}}$$</p></li></ol><p>2020.2.26</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;晶体管的结构以及符号&quot;&gt;&lt;a href=&quot;#晶体管的结构以及符号&quot; class=&quot;headerlink&quot; title=&quot;晶体管的结构以及符号&quot;&gt;&lt;/a&gt;晶体管的结构以及符号&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;结构：发射区（e，emitter）集电区（c，collector），基区（b，basic）&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;发射区重掺杂，基区轻掺杂，集电区面积大&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;符号：箭头：从P区指向N区&lt;/p&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-5 稳压管以及其它二极管</title>
    <link href="http://yoursite.com/2020/02/26/ch4-5%20%E7%A8%B3%E5%8E%8B%E7%AE%A1%E4%BB%A5%E5%8F%8A%E5%85%B6%E5%AE%83%E4%BA%8C%E6%9E%81%E7%AE%A1/"/>
    <id>http://yoursite.com/2020/02/26/ch4-5%20%E7%A8%B3%E5%8E%8B%E7%AE%A1%E4%BB%A5%E5%8F%8A%E5%85%B6%E5%AE%83%E4%BA%8C%E6%9E%81%E7%AE%A1/</id>
    <published>2020-02-26T04:43:24.185Z</published>
    <updated>2020-02-26T06:17:46.060Z</updated>
    
    <content type="html"><![CDATA[<h4 id="稳压二极管"><a href="#稳压二极管" class="headerlink" title="稳压二极管"></a>稳压二极管</h4><ol><li>符号：在二级管的符号上加一个小弯</li><li>稳定电压（$U_Z$）：工作在反向击穿状态时的稳定电压即稳定电压</li><li>额定功耗（$P_Z$）：使用时功耗不超过这个标准，这个功耗由管子本身的属性决定</li><li>稳压电流（$I_Z$）：稳压二极管工作时候有最大电流和最小电流，在这个范围区间的就是稳压电流，小于该值，管子失去稳压作用，大于此值，管子会烧坏<a id="more"></a>$$I_Z=\cfrac{U_Z}{I_Z}$$</li><li>动态电阻（$r_Z$）：一般在几欧姆到十几欧姆<br>$$r_Z=\cfrac{\Delta U_Z}{\Delta I_Z}$$</li><li>温度系数：稳定电压较高（十几伏）一般为雪崩击穿，稳定电压在6伏以下，一般为齐纳击穿，所以稳定电压一般在（5–7V）的稳压管兼有两种击穿，温度稳定性比较好</li><li>有温度补偿的稳压管：将两稳压管相互对接即可，利用一个工作在正向导通（负温度系数），一个工作在反向导通（正温度系数）即可相互补偿，改善温度稳定性。<br>$$U=\pm(U_Z+U_D)$$<blockquote><p>其中$U_D$是正向导通电压</p></blockquote></li><li>如果计算带有稳压二极管的电路，首先要判断稳压二极管能否被击穿，方法是先假设稳压管断开，看 $U_O$ 是否大于 $U_Z$ .<blockquote><p>具体题目翻一下MOOC，这里不记录了</p></blockquote></li><li>稳压电路限流电阻R的选择：就是要保证电流在稳压电流区工作，公式比较长不写了，遇到具体问题再翻书补充</li><li>由稳压二极管构成的限幅电路（单向，双向）：和二极管构成的限幅电路是相似的，但是由于正向导通特性，单向限幅的时候，负端会有0.7V的电压（硅管）<h4 id="其他二极管"><a href="#其他二极管" class="headerlink" title="其他二极管"></a>其他二极管</h4></li><li>变容二极管：利用了二极管的电容特性，具体在高频时候用的比较多，遇到在做深入了解</li><li>光电二极管：被光线照射后产生一个电流</li><li>发光二极管：……</li><li>肖特基二极管：结构是金属和N型材料掺杂，特点是速度快，常用在高频，遇到再深入了解</li></ol><p>2020.2.26</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;稳压二极管&quot;&gt;&lt;a href=&quot;#稳压二极管&quot; class=&quot;headerlink&quot; title=&quot;稳压二极管&quot;&gt;&lt;/a&gt;稳压二极管&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;符号：在二级管的符号上加一个小弯&lt;/li&gt;
&lt;li&gt;稳定电压（$U_Z$）：工作在反向击穿状态时的稳定电压即稳定电压&lt;/li&gt;
&lt;li&gt;额定功耗（$P_Z$）：使用时功耗不超过这个标准，这个功耗由管子本身的属性决定&lt;/li&gt;
&lt;li&gt;稳压电流（$I_Z$）：稳压二极管工作时候有最大电流和最小电流，在这个范围区间的就是稳压电流，小于该值，管子失去稳压作用，大于此值，管子会烧坏
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-4 晶体二极管应用</title>
    <link href="http://yoursite.com/2020/02/25/ch4-4%20%E6%99%B6%E4%BD%93%E4%BA%8C%E6%9E%81%E7%AE%A1%E5%BA%94%E7%94%A8/"/>
    <id>http://yoursite.com/2020/02/25/ch4-4%20%E6%99%B6%E4%BD%93%E4%BA%8C%E6%9E%81%E7%AE%A1%E5%BA%94%E7%94%A8/</id>
    <published>2020-02-25T13:56:18.113Z</published>
    <updated>2020-02-26T04:40:54.035Z</updated>
    
    <content type="html"><![CDATA[<h4 id="整流电路"><a href="#整流电路" class="headerlink" title="整流电路"></a>整流电路</h4><blockquote><p>这里最好有电路图，但是懒得加了，咱们就脑子里回忆吧，哈哈哈</p></blockquote><a id="more"></a><ol><li>二极管加负载<blockquote><p>利用二极管单向导电性，如果输入电压大于0.7V，就输出电压=输入电压，如果输入电压小于0.7V，则二极管截止，输出电压为0.</p></blockquote></li><li>整流桥电路(又称绝对值电路)<br>$$U_o=|U_i|$$<h4 id="限幅电路"><a href="#限幅电路" class="headerlink" title="限幅电路"></a>限幅电路</h4></li><li>上限幅电路</li><li>双向限幅电路<blockquote><p>当输入电压大于二极管和电压源，Uo即二极管管压降和电压源压降之和，当输入电压小于二极管和电压源，$U_o=U_i$</p></blockquote></li></ol><p>2020.2.25</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;整流电路&quot;&gt;&lt;a href=&quot;#整流电路&quot; class=&quot;headerlink&quot; title=&quot;整流电路&quot;&gt;&lt;/a&gt;整流电路&lt;/h4&gt;&lt;blockquote&gt;
&lt;p&gt;这里最好有电路图，但是懒得加了，咱们就脑子里回忆吧，哈哈哈&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-3 晶体二极管的特性及参数</title>
    <link href="http://yoursite.com/2020/02/25/ch4-3%20%E6%99%B6%E4%BD%93%E4%BA%8C%E6%9E%81%E7%AE%A1%E7%9A%84%E7%89%B9%E6%80%A7%E5%8F%8A%E5%8F%82%E6%95%B0/"/>
    <id>http://yoursite.com/2020/02/25/ch4-3%20%E6%99%B6%E4%BD%93%E4%BA%8C%E6%9E%81%E7%AE%A1%E7%9A%84%E7%89%B9%E6%80%A7%E5%8F%8A%E5%8F%82%E6%95%B0/</id>
    <published>2020-02-25T13:55:27.788Z</published>
    <updated>2020-02-25T14:38:58.286Z</updated>
    
    <content type="html"><![CDATA[<h4 id="二极管的伏安特性–指数特性"><a href="#二极管的伏安特性–指数特性" class="headerlink" title="二极管的伏安特性–指数特性"></a>二极管的伏安特性–指数特性</h4><p>$$i_D=I_S(e^\cfrac{U_D}{U_T}-1)$$</p><blockquote><p>注：其中 $I_S$ 为反向饱和电流，$U_T$为热电压，$U_D$为导通电压，$i_D$为正向导通电流</p></blockquote><a id="more"></a><ol><li>正向特性：<blockquote><p>死区电压（门限电压）：正向加电压的过程中，一开始电流很小，然后突然指数级的增加。在电流很小的区间，电压及是死区电压。</p></blockquote></li></ol><blockquote><p>死区电压：室温（27°C）下，硅管：0.5-0.7V，锗管：0.1-0.3V</p></blockquote><p>管压降：工业上–硅管：0.7V，锗管–0.3V<br>2. 反向特性:<br>电流很小，硅管：一般小于0.1uA，锗管小于几十微安</p><h4 id="二极管参数"><a href="#二极管参数" class="headerlink" title="二极管参数"></a>二极管参数</h4><ol><li>直流电阻<br>$$R_D=\cfrac{U_D}{I_D}$$<blockquote><p>正向电阻越小，反向电阻越大，二极管的导电性能越好</p></blockquote></li><li>交流电阻<br>$$r_D=\cfrac{\Delta U_D}{\Delta i_D}$$<blockquote><p>$r_D &lt;&lt; R_D$</p></blockquote></li><li>温度特性<blockquote><p>温度T增大:<br>反向饱和电流增大，死区电压减小<br>雪崩击穿电压增大（温度增加）<br>齐纳击穿电压减小（温度减小）</p></blockquote></li><li>最大整流电流<blockquote><p>二极管允许通过的最大正向平均电流</p></blockquote></li><li>最大反向工作电压</li><li>反向电流<blockquote><p>越小越好，通常反向电流与温度密切相关</p></blockquote></li><li>最高工作频率<br>超过该工作频率，二极管单向导电性能变坏<h4 id="二级管电路模型"><a href="#二级管电路模型" class="headerlink" title="二级管电路模型"></a>二级管电路模型</h4>通常看做，P端正，N端负，硅管的管压降为0.7V，锗管的管压降为0.3V即可。</li></ol><p>2020.2.25</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;二极管的伏安特性–指数特性&quot;&gt;&lt;a href=&quot;#二极管的伏安特性–指数特性&quot; class=&quot;headerlink&quot; title=&quot;二极管的伏安特性–指数特性&quot;&gt;&lt;/a&gt;二极管的伏安特性–指数特性&lt;/h4&gt;&lt;p&gt;$$i_D=I_S(e^\cfrac{U_D}{U_T}-1)$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注：其中 $I_S$ 为反向饱和电流，$U_T$为热电压，$U_D$为导通电压，$i_D$为正向导通电流&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-2 P--N结</title>
    <link href="http://yoursite.com/2020/02/24/ch4-2%20P--N%E7%BB%93/"/>
    <id>http://yoursite.com/2020/02/24/ch4-2%20P--N%E7%BB%93/</id>
    <published>2020-02-24T08:51:42.623Z</published>
    <updated>2020-02-25T13:52:47.678Z</updated>
    
    <content type="html"><![CDATA[<h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><ol><li>PN结：P型半导体和N型半导体交接面处会形成一个有特殊物理性质的薄层，称为PN结<a id="more"></a></li><li>PN结的形成：由于两边载流子浓度差————&gt;多子扩散（N型半导体–自由电子，P型半导体–空穴）————&gt;交接处复合，显露出正负原子————形成内电场————&gt;阻碍多子扩散，利于少子漂移————&gt;动态平衡，形成PN结</li><li>中间形成电场的区域称为<strong><em>耗尽区或势垒区或阻挡区</em></strong>，在掺杂浓度不对称的PN结中，掺杂浓度大的地方延伸小，掺杂浓度小的地方，延伸大（因为两边正负电荷数量相等）<h4 id="PN结的导电特性"><a href="#PN结的导电特性" class="headerlink" title="PN结的导电特性"></a>PN结的导电特性</h4></li><li>正向偏置：外部添加P正，N负的电场，与内电场相互抵消，耗尽层变窄，则扩散作用加强，漂移运动减弱，（也就是说N结处的自由电子向浓度低的P结移动，空穴从P向N移动），因此形成正向电流（从P——&gt;N）<blockquote><p>注：电流方向是正电荷移动方向，与负电荷移动方向相反，因为在定义电流方向的时候，还没有发现电子……</p></blockquote></li><li>反向偏置：外部添加N正，P负的电场，产生与内部电场方向相同的电场，则耗尽区变宽，扩散运动减弱，漂移运动加强，反向电流很小（少子提供电流）</li><li>可以认为是（<strong><em>正偏—导通，反偏—截止</em></strong>）<h4 id="PN结的击穿特性"><a href="#PN结的击穿特性" class="headerlink" title="PN结的击穿特性"></a>PN结的击穿特性</h4></li><li>当反向电压超过反向截止电压的时候，反向电流会急剧增大，有两种机理</li><li>雪崩击穿：加反向电压，耗尽区变大，少子漂移时，被加速，动能过大，撞击共价键，使自由电子挣脱束缚，然后蝴蝶效应，一起被加速，到处撞击……反向电流变大</li><li>齐纳击穿：在重掺杂的PN结中，耗尽区很窄，所以较小的反向电压会产生很大的电场，一定的强度就可以将共价键中的价电子直接拉出，产生大量自由电子和空穴对，使反向电流急剧增大。<blockquote><p>一般而言，对于硅材料的PN结，截止电压 &gt; 7V为雪崩击穿，截止电压 &lt; 5V为齐纳击穿；截止电压在 5 ~ 7 V时，两种击穿都有。</p></blockquote></li></ol><blockquote><p>只要限制击穿后的电流，击穿并不损坏 PN 结（可逆性）</p></blockquote><h4 id="PN结的电容特性"><a href="#PN结的电容特性" class="headerlink" title="PN结的电容特性"></a>PN结的电容特性</h4><ol><li>PN结有电容特性，由势垒电容和扩散电容两部分组成</li><li>势垒电容：高阻的耗尽区，与平板电容器相似，正偏电压越大，容值越小<br>$$C_T=\cfrac{\epsilon S}{d}$$<blockquote><p>d 是耗尽区长度，S是耗尽区横截面积</p></blockquote></li><li>扩散电容：正向偏置的PN结，由于多子扩散，会有电容特性<br>$$C_D=\cfrac{\tau I}{U_T}$$<blockquote><p>$C_T$ 和 $C_D$ 都随外加电压的变化而变化，且都非线性，低频时可以忽略，高频时需要考虑……</p></blockquote></li></ol><p>2020.2.24</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;PN结：P型半导体和N型半导体交接面处会形成一个有特殊物理性质的薄层，称为PN结
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-1 半导体物理基础</title>
    <link href="http://yoursite.com/2020/02/24/ch4-1%20%E5%8D%8A%E5%AF%BC%E4%BD%93%E7%89%A9%E7%90%86%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2020/02/24/ch4-1%20%E5%8D%8A%E5%AF%BC%E4%BD%93%E7%89%A9%E7%90%86%E5%9F%BA%E7%A1%80/</id>
    <published>2020-02-24T06:15:33.978Z</published>
    <updated>2020-02-25T13:52:45.505Z</updated>
    
    <content type="html"><![CDATA[<h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><ol><li>物体分为：导体，半导体，绝缘体。其中半导体的导电能力随着温度，光照，掺杂等外界因素所改变（价带，禁带，导带）<a id="more"></a></li><li>本征半导体：纯净的硅和锗单晶体是本征半导体，硅和锗的最外层电子都是4个，则4个最外层电子又称作价电子（物理，化学性质主要取决于最外层电子）。</li><li>本征激发：价电子获得足够大的能力（即外界因素干扰），挣脱共价键的束缚，成为自由电子，产生了自由电子以及空穴，本征激发产生的空穴和自由电子的数目相等。</li><li>载流子：带负电的自由电子和带正电的空穴，都可以导电，统称为载流子。</li><li>复合：是激发的逆过程，即自由电子填入空穴，释放能量，从而消失一对载流子的过程</li><li>硅的温度温度性比锗要好，所以集成电路多用硅（因为硅的电子挣脱共价键的束缚需要的能力大于锗，硅1.21eV，锗0.78eV）<h4 id="N型半导体和P型半导体"><a href="#N型半导体和P型半导体" class="headerlink" title="N型半导体和P型半导体"></a>N型半导体和P型半导体</h4>由于本征半导体的导电能力比较弱，掺杂一些元素的原子可以提高半导体的导电能力，杂质半导体分为N型半导体和P型半导体。</li><li>N型半导体：在本征半导体中掺入5价原子，即N型半导体。提供自由电子（称为<strong><em>施主电离</em></strong>），其中多数载流子—自由电子，少数载流子—空穴，半导体仍然保持中性（自由电子为负，原子即表示正性，抵消）</li><li>多子：多数载流子，少子–少数载流子</li><li>少子浓度和温度关系很大，多子与温度关系不大</li><li>P型半导体：在本征半导体中掺入3价原子，即P型半导体。提供空穴（称为<strong><em>受主电离</em></strong>），其中多数载流子—空穴，少数载流子—自由电子，半导体仍然保持中性（空穴显正性，原子显负，抵消）<h4 id="漂移电流和扩散电流"><a href="#漂移电流和扩散电流" class="headerlink" title="漂移电流和扩散电流"></a>漂移电流和扩散电流</h4></li><li>半导体电流：即自由电子形成的电流加上空穴形成的电流。</li><li>漂移电流：在电场的作用下，自由电子逆电场方向漂移，空穴顺着电场方向漂移，这样产生的电流称为漂移电流（受到<strong><em>载流子浓度，迁移率，电场强度</em></strong>的影响）</li><li>迁移率：单位电场的作用下，自由电子的移动速度比空穴快</li><li>扩散电流：半导体中浓度不均匀分布的时候，载流子会从高浓度区域向低浓度区域扩散，从而形成扩散电流，（受到<strong><em>载流子的浓度差或者叫浓度梯度</em></strong>的影响）</li></ol><p>2020.2.24</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;物体分为：导体，半导体，绝缘体。其中半导体的导电能力随着温度，光照，掺杂等外界因素所改变（价带，禁带，导带）
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch2-6 微分器</title>
    <link href="http://yoursite.com/2020/02/20/ch2-6%20%E5%BE%AE%E5%88%86%E5%99%A8/"/>
    <id>http://yoursite.com/2020/02/20/ch2-6%20%E5%BE%AE%E5%88%86%E5%99%A8/</id>
    <published>2020-02-20T11:27:34.939Z</published>
    <updated>2020-02-21T14:08:34.108Z</updated>
    
    <content type="html"><![CDATA[<h3 id="微分器"><a href="#微分器" class="headerlink" title="微分器"></a>微分器</h3><blockquote><p>目标：$Uo(t)=k\cfrac{dui(t)}{dt}$</p></blockquote><blockquote><p>即输出与输入的微分成正比，微分是积分的逆运算。</p></blockquote><a id="more"></a><h4 id="微分器电路"><a href="#微分器电路" class="headerlink" title="微分器电路"></a>微分器电路</h4><p>(A)时域分析</p><blockquote><p>由于微分和积分是互逆的关系，所以把反相输入端的电容和反馈网络中电阻的位置互换即是，微分器电路，分析与积分器相似：<br>$$U_o(t)=-U_R(t)=-i_R(t)R$$<br>$$i_R(t)=i_c(t)=C\cfrac{dUc(t)}{dt}$$<br>$$Uo(t)=-RC\cfrac{dUi(t)}{dt}=-\tau \cfrac{dUi(t)}{dt}$$<br>(B)频域分析<br>$$Au(j\omega)=\cfrac{Uo(j\omega)}{Ui(j\omega)}=-j\omega RC=-j\omega\tau$$<br>因为是反相输入端，抵消一个负号，所以是 j，超前90度</p></blockquote><ol><li>相位超前90度</li><li>增益的模 $|Au(j\omega)|=\omega RC$</li><li>有个结论：频率越高增益越大，且有一个90度的超前相移。<h4 id="微分器的高频增益"><a href="#微分器的高频增益" class="headerlink" title="微分器的高频增益"></a>微分器的高频增益</h4><blockquote><p>由于电容是通高频，阻低频，所以，若干反相输入端有高频信号（高频噪声）输入，电容相当于是短路，高频信号会被反相放大，影响正常输出信号。</p></blockquote><h5 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h5></li><li>在反相端电容的前面加一个小电阻$R_2$，要远远小于反馈网络中的负载电阻Rf阻值：<br>$$A_u(j\omega)=\cfrac{U_o(j\omega)}{U_i(j\omega)}=-\cfrac{R}{R_2+\cfrac{1}{j\omega C}}=-\cfrac{j\omega RC}{1+j\omega R_2C}$$<blockquote><p>当$R_2$非常小的时候，可以忽略，就相当于$-j\omega RC$</p></blockquote></li><li>因为高频噪声的原因，所以微分器通常在工程中会被积分器所取代，通过解微分方程就可以，举课上面的例子：<br>$$\cfrac{d^2U_o(t)}{dt^2}+10\cfrac{dU_o(t)}{dt}+2U_o(t)=Ui(t)$$<br>$$\cfrac{dU_o(t)}{dt}=\int[u_i(t)-10\cfrac{dU_o(t)}{dt}-2U_o(t)]dt$$<br>$$U_o(t)=\iint U_i(t)dt-2\iint U_o(t)dt-10\int U_o(t)dt$$<h4 id="微分器实验"><a href="#微分器实验" class="headerlink" title="微分器实验"></a>微分器实验</h4></li><li>三角波方波变化<blockquote><p>微分嘛，三角波下降的时候，斜率不变且小于0，即 $\cfrac{U_i(t)}{dt}&lt;0$,和前面负号抵消，所以是输出高电平，同理，三角波上升时，斜率不变且大于0，即 $\cfrac{U_i(t)}{dt}&gt;0$,前面加一个负号，所以输出低电平，最后输出的是方波。</p></blockquote></li><li>方波变化尖脉冲<blockquote><p>输入方波为高低电平的时候，输入电平没有变化，斜率为0，高低电平直接是突变，斜率变化很大，所以有个尖脉冲。</p></blockquote></li></ol><p>2020.2.21</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;微分器&quot;&gt;&lt;a href=&quot;#微分器&quot; class=&quot;headerlink&quot; title=&quot;微分器&quot;&gt;&lt;/a&gt;微分器&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;目标：$Uo(t)=k\cfrac{dui(t)}{dt}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;即输出与输入的微分成正比，微分是积分的逆运算。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch2-5 积分器</title>
    <link href="http://yoursite.com/2020/02/20/ch2-5%20%E7%A7%AF%E5%88%86%E5%99%A8/"/>
    <id>http://yoursite.com/2020/02/20/ch2-5%20%E7%A7%AF%E5%88%86%E5%99%A8/</id>
    <published>2020-02-20T11:26:55.734Z</published>
    <updated>2020-02-21T14:11:44.501Z</updated>
    
    <content type="html"><![CDATA[<h3 id="积分器"><a href="#积分器" class="headerlink" title="积分器"></a>积分器</h3><blockquote><p>目标：$Uo(t)=k\int Ui(t)dt$</p></blockquote><p>即输出与输入的积分成正比</p><a id="more"></a><h4 id="反相积分电路"><a href="#反相积分电路" class="headerlink" title="反相积分电路"></a>反相积分电路</h4><blockquote><p>就是将反相比例放大器中的 Rf 换成电容</p></blockquote><p>(A)时域分析<br>$$Uo(t)=-Uc(t)=-\cfrac{Q}{C}=-\cfrac{1}{C}\int ic(t)dt$$<br>$$=-\cfrac{1}{C}\int \cfrac{Ui(t)}{R}dt=-\cfrac{1}{RC}\int Ui(t)dt=-\cfrac{1}{\tau}\int Ui(t)dt$$<br>其中 $\tau$ = RC 是积分常数</p><p>(B)频域分析</p><blockquote><p>这里和反相比例放大器是一个分析方法<br>$$Au(j\omega)=\cfrac{Uo(j\omega)}{Ui(j\omega)}=-\cfrac{1}{j\omega RC}=-\cfrac{1}{j\omega \tau}$$<br>因为这里有一个$\cfrac{1}{j}=-j$ ,又是反相输入端，最外面那个负号抵消，所以：</p></blockquote><ol><li>相位滞后90度，</li><li>增益的模：$|Au(j\omega)|=\cfrac{1}{\omega RC}$</li><li>这里有个结论：频率越高，衰减越大(<strong><em>这里是真的不明白为什么了？？</em></strong>)<h4 id="差分积分器电路"><a href="#差分积分器电路" class="headerlink" title="差分积分器电路"></a>差分积分器电路</h4><blockquote><p>这里和简单减法器是相似，把 Rf 和下面对应的电阻换成电容即是差分积分器啦<br>$$Uo(j\omega)=\cfrac{1}{j\omega RC}[Ui1(j\omega)-Ui2(j\omega)]$$<br>$$Uo(j\omega)=\cfrac{1}{RC}\int[Ui1(t)-Ui2(t)]dt$$<br>上面一个是频域形式，一个是时域形式啦</p></blockquote><h4 id="同相积分电路"><a href="#同相积分电路" class="headerlink" title="同相积分电路"></a>同相积分电路</h4><blockquote><p>就是将反相输入端接地，Ui2 = 0<br>$$Uo(j\omega)=\cfrac{1}{j\omega RC}Ui1(j\omega)$$<br>$$Uo(j\omega)=\cfrac{1}{RC}\int Ui1(t)dt$$</p></blockquote><h4 id="积分器设计"><a href="#积分器设计" class="headerlink" title="积分器设计"></a>积分器设计</h4><blockquote><p>这里注意一下设计积分器首先要确定时间常数 $\tau$ ,又因为电容的可选类型比较少，电阻相对要多的多，所以，先确定电容的值，在确定电阻的阻值。</p></blockquote></li></ol><blockquote><p>电容单位 F 换算：1F = 1 * 10^6 $\mu$F(微法)_…………_1 $\mu$F = 1 * 10^3 nF_…………1 $\mu$F = 1 * 10^6 pF</p></blockquote><h4 id="单级积分器电路"><a href="#单级积分器电路" class="headerlink" title="单级积分器电路"></a>单级积分器电路</h4><blockquote><p>由于电容是通交流隔直流的，所以只在负反馈网络加电容，会使直流无法反馈，造成积分器不正常工作，或有噪声的情况，（<strong><em>这里加入的电阻要保证R &gt;&gt;&gt; 10</em>Ri</strong>）,不然对增益会很大的影响。</p></blockquote><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><ol><li>方波转换成三角波<blockquote><p>反相输入端输入方波，当输入为高电平，输入端高电位，输出端拉低，反馈回去，电位下降；同理反相端输入为低电平，输出端拉高电位上升，反馈回去。输出端就是三角波了。</p></blockquote></li><li>正弦波和余弦波互相转换（<strong><em>因为相位互相差90度</em></strong>）</li></ol><p>2020.2.20</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;积分器&quot;&gt;&lt;a href=&quot;#积分器&quot; class=&quot;headerlink&quot; title=&quot;积分器&quot;&gt;&lt;/a&gt;积分器&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;目标：$Uo(t)=k\int Ui(t)dt$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;即输出与输入的积分成正比&lt;/p&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>生不逢时—曾国藩</title>
    <link href="http://yoursite.com/2020/02/20/%E7%94%9F%E4%B8%8D%E9%80%A2%E6%97%B6%E2%80%94%E6%9B%BE%E5%9B%BD%E8%97%A9/"/>
    <id>http://yoursite.com/2020/02/20/%E7%94%9F%E4%B8%8D%E9%80%A2%E6%97%B6%E2%80%94%E6%9B%BE%E5%9B%BD%E8%97%A9/</id>
    <published>2020-02-20T07:37:10.015Z</published>
    <updated>2020-02-21T14:12:19.008Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>寒假时间转瞬即逝，为数不多的收获之一是在受到白岩松的启发后，将《曾国藩》全书（一共上中下三册）看完，略有启发，这里记录下来。</p><a id="more"></a><h3 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h3><p>本书讲诉曾国藩年轻气盛之时，历经回乡守孝，出山创建湘军，官场不顺，三败石达开，至攻克江宁（九弟-曾国荃），平定太平天国，整饬两江，被迫与捻军作战不利，到处理天津教案名声败北，最后开始新办洋务，送幼童出国学习技术，最后在矛盾与怅然中离去。</p><h4 id="时代所限的“晚清名臣”"><a href="#时代所限的“晚清名臣”" class="headerlink" title="时代所限的“晚清名臣”"></a>时代所限的“晚清名臣”</h4><p>曾国藩在晚年多次提及天命，用了比喻，与康福提到，自己和皇上都是棋盘中的棋子，那谁是执子人呢，曾国藩答：天命。这里的天命是什么，是时代命运，清朝末年，妇人垂帘，皇子孱弱，国运衰微，到了什么地步呢，堂堂江宁，汉唐繁盛至极，现今竟然城内屋檐下，饿殍，乞丐随处可见，比起太平天国时期，竟然是比之远远不如，皇室中，恭亲王和西宫太后内斗，内忧外患，西方列强制度先进，技术先进，压迫清政府。用李鸿章的话是“三千年之未有大变局”。</p><p>曾国藩从小受到程朱理学理学，孔孟熏陶，心中把忠于皇室作为立人为本的标杆，自创湘军，希望也是帮助君主，使天下太平，百姓过上好日子，心只是可惜，天命不从，若是汉唐鼎盛时代，曾国藩一定可有所为，流芳百世，可惜时代变化，恰恰是忠于皇室，忠于国运衰微的皇室让他走上了一条错误的道路。</p><p>曾国藩在晚年的时候心病严重，一心希望世间太平，让人民过上好日子，可事与愿违，街头百姓无饭可吃，山头绿林强盗与官府打为一片，官僚腐败彻底，细细一想，自己用了十几年的生涯创立湘军，最后攻下江宁，平定太平天国，又是为了什么呢。书中说曾国藩晚年在与郭嵩焘的交谈中，郭嵩焘大力批评曾国藩居于忠于皇室的“小节”，而不顾民族和中华命运的“大节”，不知道是不是真的有这次交谈，但是相信若有机会与曾国藩对话，这也是作者心中想表达出来的意思吧。</p><p>曾国藩也只是时代轮换下的一个可怜人了。</p><p>李鸿章是曾国藩的弟子，在书中，曾国藩知道自己时日不多，在晚年与所有健在的人，一一作了告别，包括他最得意的弟子——李鸿章，此后书中再无李鸿章出现，我感兴趣的是李鸿章是怎么想的呢，众人熟知的是李鸿章也是晚清四大名臣之一，被称作是“中国的俾斯麦”，在曾国藩去世之后，本书已经结束，但历史没有，李鸿章一直作为晚清的执掌大臣，签署了各个辱国条约，李鸿章又是怎样的心理变化呢，还有军事天才，左宗棠，他在面临外忧内患的情况下又做了什么，，思考了什么呢，历史的趣味大概就在这里了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;寒假时间转瞬即逝，为数不多的收获之一是在受到白岩松的启发后，将《曾国藩》全书（一共上中下三册）看完，略有启发，这里记录下来。&lt;/p&gt;
    
    </summary>
    
    
      <category term="生活感想" scheme="http://yoursite.com/categories/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%83%B3/"/>
    
    
  </entry>
  
  <entry>
    <title>ch2-4 相减器</title>
    <link href="http://yoursite.com/2020/02/19/ch2-4%20%E7%9B%B8%E5%87%8F%E5%99%A8/"/>
    <id>http://yoursite.com/2020/02/19/ch2-4%20%E7%9B%B8%E5%87%8F%E5%99%A8/</id>
    <published>2020-02-19T13:46:17.617Z</published>
    <updated>2020-02-21T14:11:43.415Z</updated>
    
    <content type="html"><![CDATA[<h3 id="相减器"><a href="#相减器" class="headerlink" title="相减器"></a>相减器</h3><p>目标：使输入的两个信号相减输出<br>$$Uo = a Ui1 - b Ui2$$</p><blockquote><p>那么令同相端为被减数（Ui1），反相端为减数（Ui2），即可</p></blockquote><a id="more"></a><h4 id="a-b-1"><a href="#a-b-1" class="headerlink" title="$a = b + 1$ :"></a>$a = b + 1$ :</h4><blockquote><p>将Ui1和Ui2分别接地，可得到同相比例和反相比例放大器的 Uo，然后叠加原理，相加即是输出电压</p></blockquote><p>推导一下：</p><blockquote><p>令Ui2 接地,则为同相比例放大器<br>$$Uo = (1 + \cfrac{Rf}{R2})Ui1$$<br>令Ui1接地，则为反相比例放大器<br>$$Uo = -\cfrac{Rf}{R2}Ui2$$<br>用叠加定理合起来就是<br>$$Uo = (1 + \cfrac{Rf}{R2})Ui1 -\cfrac{Rf}{R2}Ui2 $$</p></blockquote><h4 id="a-lt-b-1"><a href="#a-lt-b-1" class="headerlink" title="a &lt; b + 1"></a>a &lt; b + 1</h4><blockquote><p>要减小a，减小同相输入端的电压（分压即可）</p></blockquote><p>在U+处接上一个电阻，这个电阻接地即可<br>$$Uo = (1 + \cfrac{Rf}{R2})(\cfrac{R4}{R1+R4})Ui1 -\cfrac{Rf}{R2}Ui2 $$</p><h4 id="a-gt-b-1"><a href="#a-gt-b-1" class="headerlink" title="a &gt; b + 1"></a>a &gt; b + 1</h4><blockquote><p>同理增加a，减小Ui2的放大倍数就可以了，Rf不变，增加分母上的值即可，即，在R2上面并联一个电阻即可<br>$$Uo = (1+\cfrac{Rf}{R2+R})Ui1-\cfrac{Rf}{R2}Ui2$$<br>由于虚断的原因，在反相比例放大器的情况下，R两端电压都是0，所以R不起作用</p></blockquote><h4 id="a-b-k"><a href="#a-b-k" class="headerlink" title="a = b = k"></a>a = b = k</h4><blockquote><p>常用的模型是使R2 = R1，Rf = R3 ，就可以满足 a = b = k</p></blockquote><p>则：<br>$$Uo = \cfrac{R3}{R2}(Ui1-Ui2) = \cfrac{Rf}{R2}(Ui1-Ui2)$$</p><h4 id="相减器设计实例"><a href="#相减器设计实例" class="headerlink" title="相减器设计实例"></a>相减器设计实例</h4><ol><li>实现：Uo = 5 ( Ui1 - Ui2 )<blockquote><p>放大倍数相同，说明Rf = R4，R1 = R2，……</p></blockquote></li><li>实现：Uo = 5 Ui1 - 8 Ui2<blockquote><p>写出 a &lt; b 的情况（公式）</p></blockquote></li><li>实现：Uo = 8 Ui1 - 5 Ui2</li></ol><h4 id="相减器的应用"><a href="#相减器的应用" class="headerlink" title="相减器的应用"></a>相减器的应用</h4><ol><li>直流电平移位 <blockquote><p>案例里面是将信号接到反相输入端，同相输入端接直流信号，当然这个就反相放大了。同相放大的话就是，将信号接入同相输入端即可</p></blockquote></li><li>抑制共模干扰<blockquote><p>共模干扰——百度百科：共模干扰指的是干扰电压在信号线及其信号地线上的幅度相同。(<strong><em>额？还是觉得哪里不明白</em></strong>)  </p></blockquote></li></ol><blockquote><p>这里孙老师是举了例子，是把两个相同信号源的输入信号放入相减器中间，使两个相同的noise相减，抵消后输入仪器(<strong><em>妙极了</em></strong>)</p></blockquote><h4 id="简单相减器存在的问题"><a href="#简单相减器存在的问题" class="headerlink" title="简单相减器存在的问题"></a>简单相减器存在的问题</h4><ol><li>如果要求两个输入信号的放大倍数相同的话，那么要求上下对应的电阻阻值相同——————&gt;调节增益困难，要同步调节上下两个电阻</li><li>输入电阻偏小(注意哦，不是放大器内部的输入电阻，是同反相输入端上的电阻)，对两个信号源的影响不同(<strong><em>这个是为什么呢？</em></strong>)</li></ol><h4 id="仪表放大器"><a href="#仪表放大器" class="headerlink" title="仪表放大器"></a>仪表放大器</h4><blockquote><p>你好，放大器一书中把仪表放大器归于功能放大器当中哦~</p></blockquote><blockquote><p>结构就是在简单的相减器的前面增加两个同相比例放大器</p></blockquote><h6 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h6><ol><li>两个同相比例放大器由于放大器内部输入电阻阻值趋于无穷大，所以对信号输入起到了很好的隔绝作用。</li><li>通过调节两个同相比例放大器中间的电阻阻值可以很方便的改变仪表放大器的增益，为啥呢，也比较简单，我推导一下：<blockquote><p>首先由于虚断，同相比例放大器的两个信号输入端无电流(阻值太大嘛~),由于虚端，两个信号输入端的电压相同，即信号相同，则：<br>$$Uo1-Uo2=ix(R+R+Rx)=\cfrac{Ui1-Ui2}{Rx}(2R+Rx)$$<br>其中Rx是两个同相比例放大器之间的电压，Ui1和Ui2是后半部分简单减法器的输入信号端<br>$$Uo=\cfrac{R4}{R3}(Uo1-Uo2)=\cfrac{R4}{R3}(1+\cfrac{2R}{Rx}(Ui1-Ui2)$$<br>$$Au=\cfrac{Uo}{Ui1-Ui2}=\cfrac{R4}{R3}(1+\cfrac{2R}{Rx})$$<br>所以增益可以由Rx改变啦</p></blockquote></li></ol><p>2020.2.20</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;相减器&quot;&gt;&lt;a href=&quot;#相减器&quot; class=&quot;headerlink&quot; title=&quot;相减器&quot;&gt;&lt;/a&gt;相减器&lt;/h3&gt;&lt;p&gt;目标：使输入的两个信号相减输出&lt;br&gt;$$Uo = a Ui1 - b Ui2$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;那么令同相端为被减数（Ui1），反相端为减数（Ui2），即可&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch2-3 相加器</title>
    <link href="http://yoursite.com/2020/02/19/ch2-3%20%E7%9B%B8%E5%8A%A0%E5%99%A8/"/>
    <id>http://yoursite.com/2020/02/19/ch2-3%20%E7%9B%B8%E5%8A%A0%E5%99%A8/</id>
    <published>2020-02-19T13:46:17.612Z</published>
    <updated>2020-02-21T14:11:42.356Z</updated>
    
    <content type="html"><![CDATA[<h3 id="由运放构成的相加器"><a href="#由运放构成的相加器" class="headerlink" title="由运放构成的相加器"></a>由运放构成的相加器</h3><h4 id="传统电阻分压"><a href="#传统电阻分压" class="headerlink" title="传统电阻分压"></a>传统电阻分压</h4><blockquote><p>缺点：1. 信号会衰减，无法放大；2. 加上负载的话，分压系数会变化；3. 两端初始的信号源也会互相干扰（可能有电场等相互干扰吧）————&gt;加上运算放大器解决问题</p></blockquote><a id="more"></a><h4 id="同相加法器"><a href="#同相加法器" class="headerlink" title="同相加法器"></a>同相加法器</h4><blockquote><p>运放输入电阻趋近于无穷大，输出电阻趋近于0，起到隔离放大的作用（<strong><em>这里的隔离是指的隔离什么呢？？是输入信号的噪声吗？？？</em></strong>）</p></blockquote><blockquote><p>首先这里是在同相比例放大器的基础上，在输入端加入两个分开的电源以及负载（这里也要记住在脑子里），负载分别是R1和R2，输入电压为U+<br>$$U+=\cfrac{R2}{R1+R2}Ui1+\cfrac{R1}{R1+R2}Ui2$$<br>这个公式怎么推导的，这里要记住，对Ui1和Ui2分开考虑，当对Ui1考虑时，就把Ui2接地，当对Ui2考虑时就把Ui1接地，分别求出Ui1和Ui2对U+的影响，然后叠加起来即为U+处的电压<br>$$Uo = (1+\cfrac{Rf}{R})U+$$<br>$$Uo = (1+\cfrac{Rf}{R})(\cfrac{R2}{R1+R2}Ui1+\cfrac{R1}{R1+R2}Ui2)$$<br>若R1 = R2<br>$$Uo = \cfrac{1}{2}(1+\cfrac{Rf}{R})(Ui1 + Ui2)$$<br>缺点：两信号源之间依然会互相影响（接在一起肯定会互相影响）</p></blockquote><h4 id="反相相加器"><a href="#反相相加器" class="headerlink" title="反相相加器"></a>反相相加器</h4><blockquote><p>这里是在反相比例放大器的基础上，将反相输入端接上分别的信号源以及负载(<strong><em>图请记在脑子里，请记得反相输入端是虚地</em></strong>)</p></blockquote><blockquote><p>根据叠加原理，可得：<br>$$Uo = -(\cfrac{Rf}{R1}Ui1+\cfrac{Rf}{R2}Ui2+\cfrac{Rf}{R3}Ui3)$$<br>当R1 = R2 = R3 时<br>$$Uo = -\cfrac{Rf}{R}(Ui1+Ui2+Ui3)$$<br>优点：因为反相端虚地，各电流值由该支路信号源和电阻独立决定，各个信号源之间就不会影响。(<strong><em>这里我就奇怪了，不管是虚地还是啥的，两个连接上的信号源对这个节点就没有影响了？？？</em></strong>)</p></blockquote><h4 id="反相加法器的设计"><a href="#反相加法器的设计" class="headerlink" title="反相加法器的设计"></a>反相加法器的设计</h4><p>(<strong><em>记牢反相比例，同相比例放大器的闭环增益公式很重要</em></strong>)<br>$$Auf = -\cfrac{Rf}{R}$$</p><ol><li>实现：Uo = -6( Ui1 + Ui2 )———要求闭环输入电阻Rif大于等于30k<blockquote><p>注：脑子里算算就行了</p></blockquote></li><li>实现：Uo = - 5<em>Ui1 - 8</em>Ui2 - 3*Ui3<blockquote><p>由于反馈网络负载是固定的，所以先确定Rf，剩下各自算算就行，脑子里过一下</p></blockquote><h4 id="同相加法器的设计"><a href="#同相加法器的设计" class="headerlink" title="同相加法器的设计"></a>同相加法器的设计</h4>(<strong><em>记牢反相比例，同相比例放大器的闭环增益公式很重要</em></strong>)<br>$$Auf = (1 + \cfrac{Rf}{R})$$<br>对于同相加法器，对信号的放大倍数还取决于R1 ，R2<br>$$Uo = (1+\cfrac{Rf}{R})(\cfrac{R2}{R1+R2}Ui1+\cfrac{R1}{R1+R2}Ui2)$$</li><li>实现：Uo = 9(Ui1 + Ui2)<blockquote><p>对两个信号放大倍数相同，有公式可知，由于增益不变，放大倍数也不变，所以R1 = R2<br>$$Uo = \cfrac{1}{2}(1+\cfrac{Rf}{R})(Ui1 + Ui2)$$<br>剩下参数自己设一下都行</p></blockquote></li><li>实现：Uo = 2 Ui1 + 5 Ui2<br>同相加法器，增益不变嘛，所以放大倍数由R1 ，R2（信号源上的负载决定），所以 R1和R2求比值，剩下参数自己选择代入即可</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;由运放构成的相加器&quot;&gt;&lt;a href=&quot;#由运放构成的相加器&quot; class=&quot;headerlink&quot; title=&quot;由运放构成的相加器&quot;&gt;&lt;/a&gt;由运放构成的相加器&lt;/h3&gt;&lt;h4 id=&quot;传统电阻分压&quot;&gt;&lt;a href=&quot;#传统电阻分压&quot; class=&quot;headerlink&quot; title=&quot;传统电阻分压&quot;&gt;&lt;/a&gt;传统电阻分压&lt;/h4&gt;&lt;blockquote&gt;
&lt;p&gt;缺点：1. 信号会衰减，无法放大；2. 加上负载的话，分压系数会变化；3. 两端初始的信号源也会互相干扰（可能有电场等相互干扰吧）————&amp;gt;加上运算放大器解决问题&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch2-2 同相以及反相比例放大器</title>
    <link href="http://yoursite.com/2020/02/18/ch2-2%20%E5%90%8C%E7%9B%B8%E4%BB%A5%E5%8F%8A%E5%8F%8D%E7%9B%B8%E6%AF%94%E4%BE%8B%E6%94%BE%E5%A4%A7%E5%99%A8/"/>
    <id>http://yoursite.com/2020/02/18/ch2-2%20%E5%90%8C%E7%9B%B8%E4%BB%A5%E5%8F%8A%E5%8F%8D%E7%9B%B8%E6%AF%94%E4%BE%8B%E6%94%BE%E5%A4%A7%E5%99%A8/</id>
    <published>2020-02-18T12:24:40.334Z</published>
    <updated>2020-02-21T14:11:41.162Z</updated>
    
    <content type="html"><![CDATA[<h4 id="同相比例放大器"><a href="#同相比例放大器" class="headerlink" title="同相比例放大器"></a>同相比例放大器</h4><blockquote><p>根据理想运放传输特性，当增益Auo很大时，需要保证运放差模输入电压趋近于0，同相端输入，反相端要进行一个负反馈，使得反相端输入电压随同相端输入电压保持动态平衡，满足运放差模输入电压趋近于0。</p></blockquote><a id="more"></a><blockquote><p>那么为了保证运放差模输入电压趋近于0，同相端电压和反相端电压要相等（<strong><em>虚短路</em></strong>），反相端电压又等于输出端电压的分压（<strong><em>没有图，公式记在脑子里</em></strong>）带入闭环增益（输出电压比上输入的运放差模输入电压），约掉输出电压得到：<br>$$A=\cfrac{Uo}{Ui}=1+\cfrac{R2}{R1}$$<br>注意这里R2是反馈网络的负载，R1是反相输入端口的负载，要注意是，这里反相输入端口也是接地了的。(<strong><em>图记在脑子里啊</em></strong> )</p></blockquote><h4 id="反相比例放大器"><a href="#反相比例放大器" class="headerlink" title="反相比例放大器"></a>反相比例放大器</h4><blockquote><p>和上面相似，同相端接地，反相端输入端。不同的是反馈端接到反相输入端，因为同相端接地，为0，反相端增加，输出电压减小，反馈给反相端口，就趋近于0.(那么这里反相输入端就可以近似看做是地，叫做<strong><em>虚地</em></strong>)</p></blockquote><p>这里上课讲一个分压比，得到反相输入端的电压和运放差模输入电压以及输出电压的关系(<strong><em>这里不理解？？</em></strong>)</p><blockquote><p>由所推出来的式子得到<br>$$Uo=-\cfrac{R2}{R1}Ui$$<br>$$A = \cfrac{Uo}{Ui} = -\cfrac{R2}{R1}$$</p></blockquote><h4 id="反相比例放大器的输入电阻"><a href="#反相比例放大器的输入电阻" class="headerlink" title="反相比例放大器的输入电阻"></a>反相比例放大器的输入电阻</h4><blockquote><p>同相比例放大器的输入电阻是无穷大,反相比例放大器不是这个情况。需要考虑R2（反馈网络上的负载）对输入端的影响，通过密勒原理（<strong><em>不明白，感觉是电路欠下的债？？</em></strong>）在输入端等效出密勒等效电阻，得出密勒等效电阻趋于0（Auo在分母，趋于无穷大，结果趋于0），得出输入电阻趋于R1.</p></blockquote><h4 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h4><p>共模输入：大小和相位基本相同，叫做共模信号</p><p>虚短路：$Ui+ = Ui-\not={0}$</p><p>虚断路：$Ui+ = Ui-=0$</p><p>2020.2.18</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;同相比例放大器&quot;&gt;&lt;a href=&quot;#同相比例放大器&quot; class=&quot;headerlink&quot; title=&quot;同相比例放大器&quot;&gt;&lt;/a&gt;同相比例放大器&lt;/h4&gt;&lt;blockquote&gt;
&lt;p&gt;根据理想运放传输特性，当增益Auo很大时，需要保证运放差模输入电压趋近于0，同相端输入，反相端要进行一个负反馈，使得反相端输入电压随同相端输入电压保持动态平衡，满足运放差模输入电压趋近于0。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch2-1 运放基础</title>
    <link href="http://yoursite.com/2020/02/18/ch2-1%20%E8%BF%90%E6%94%BE%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2020/02/18/ch2-1%20%E8%BF%90%E6%94%BE%E5%9F%BA%E7%A1%80/</id>
    <published>2020-02-18T12:24:40.330Z</published>
    <updated>2020-02-21T14:11:39.654Z</updated>
    
    <content type="html"><![CDATA[<h3 id="集成运算放大器"><a href="#集成运算放大器" class="headerlink" title="集成运算放大器"></a>集成运算放大器</h3><h4 id="符号："><a href="#符号：" class="headerlink" title="符号："></a>符号：</h4><ol><li>三角表示，上下端口–电源</li><li>同相输入端：输入输出信号相位相同</li><li>反相输入端：反之，相位相反</li><li>输出信号和两个输入信号之差成正比：<a id="more"></a><h3 id="模型："><a href="#模型：" class="headerlink" title="模型："></a>模型：</h3></li></ol><h4 id="受控源模型"><a href="#受控源模型" class="headerlink" title="受控源模型"></a>受控源模型</h4><ol><li>上式中的Auo–运放开环放大倍数（开环增益）</li><li>运放差模输入电压：输入端电压之差<h4 id="理想受控源模型"><a href="#理想受控源模型" class="headerlink" title="理想受控源模型"></a>理想受控源模型</h4></li><li>虚断路：输入电阻无限接近于0，输入部分电流也就趋近于0，所以说输入部分“虚断路”</li><li>理想运放条件：输入电阻趋近无穷大（保证全部分压给运放），输入偏流趋近0，输出电阻趋近0，开环增益趋近无穷大，上限频率趋近无穷大……<h3 id="传输特性"><a href="#传输特性" class="headerlink" title="传输特性"></a>传输特性</h3><h4 id="集成运放电压传输特性"><a href="#集成运放电压传输特性" class="headerlink" title="集成运放电压传输特性"></a>集成运放电压传输特性</h4></li><li>由于电源电压有限，所以输出电压随输入电压会有限制，输出电压和输入电压变化关系分成线性区（输入输出电压成线性关系）和限幅区。<h4 id="理想集成运放电压传输特性"><a href="#理想集成运放电压传输特性" class="headerlink" title="理想集成运放电压传输特性"></a>理想集成运放电压传输特性</h4></li><li>因为Auo–&gt;无穷大，所以线性放大区———&gt;0，则同相端电压和反相端电压相等，输出电阻为0（没有电阻，只有导线，自然可视为“虚短路”），开环运放是无法作为线性放大器（<strong><em>开环指的是输出只受输入控制，没有反馈系统</em></strong>）</li></ol><hr><p>差模输入电压这里应该也是可以不为0，只是因为限幅，这个和趋近于0，所产生的输出电压几乎没有区别。</p><hr><h4 id="反相电压传输特性"><a href="#反相电压传输特性" class="headerlink" title="反相电压传输特性"></a>反相电压传输特性</h4><p>只是Auo &lt; 0 。传输特性图像水平翻转。</p><p>2020.2.18</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;集成运算放大器&quot;&gt;&lt;a href=&quot;#集成运算放大器&quot; class=&quot;headerlink&quot; title=&quot;集成运算放大器&quot;&gt;&lt;/a&gt;集成运算放大器&lt;/h3&gt;&lt;h4 id=&quot;符号：&quot;&gt;&lt;a href=&quot;#符号：&quot; class=&quot;headerlink&quot; title=&quot;符号：&quot;&gt;&lt;/a&gt;符号：&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;三角表示，上下端口–电源&lt;/li&gt;
&lt;li&gt;同相输入端：输入输出信号相位相同&lt;/li&gt;
&lt;li&gt;反相输入端：反之，相位相反&lt;/li&gt;
&lt;li&gt;输出信号和两个输入信号之差成正比：
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch1-2 放大器主要指标</title>
    <link href="http://yoursite.com/2020/02/17/ch1-2%20%E6%94%BE%E5%A4%A7%E5%99%A8%E4%B8%BB%E8%A6%81%E6%8C%87%E6%A0%87/"/>
    <id>http://yoursite.com/2020/02/17/ch1-2%20%E6%94%BE%E5%A4%A7%E5%99%A8%E4%B8%BB%E8%A6%81%E6%8C%87%E6%A0%87/</id>
    <published>2020-02-17T12:18:27.025Z</published>
    <updated>2020-02-21T14:11:37.670Z</updated>
    
    <content type="html"><![CDATA[<h3 id="什么是放大器？"><a href="#什么是放大器？" class="headerlink" title="什么是放大器？"></a>什么是放大器？</h3><ol><li>概念：功率放大倍数 大于 1 （变压器不是放大器）</li><li>同向放大器：输入输出信号相位相同</li><li>反向放大器：输入输出信号相位相反</li><li>放大器有两正负两极，作用同上，称为同向输入端和反向输入端<a id="more"></a><h4 id="放大器性能指标"><a href="#放大器性能指标" class="headerlink" title="放大器性能指标"></a>放大器性能指标</h4></li><li>可等效为一个有源二端口网络（一个输入端–接信号源，一个输出端–接负载）</li><li>放大器结构是两个电阻（一个输入电阻一个输出电阻），再加上一个受控源（放大倍数）</li><li>放大器参数就是上面的放大倍数A，加上输入输出电阻阻值，以及频率响应与带宽（<strong><em>？？？</em></strong>），总谐波失真系数（非线性失真系数）THD（<strong><em>？？？</em></strong>）</li><li>放大倍数A：（也称增益）输入量和输出量的比值，电压，电流增益（<strong><em>无量纲</em></strong> ），互阻，互导增益</li><li>输入电阻：Ri决定了放大器从电压源处分到的电压值。</li><li>输出电阻：对负载而言，放大器相当于信号源，R0是信号源内阻，则其阻值表达了其的带载能力。</li><li>频率特性:放大器中存在电容（<strong><em>放大器前面是输入输出电阻加上受控源，现在又有电容，所以到底放大器内部是什么结构？？</em></strong>）频率大，容抗小，反之，频率小，容抗就大，那么容抗会对增益有影响（<strong><em>怎么影响是没有说的</em></strong>）</li><li>放大器的截止频率和通频带：频率分低频区，中频区，高频区。频率过高或过低，增益绝对值都会下降。截止频率为0.707增益，高频区为上限截止频率，低频区为下限截止频率。两个截止频率之间为通频带。（记图像）</li><li>失真：相位失真（各波形相位不同，则叠加波形会错位，），频率失真（两个频率不同的波形叠加放大，因为频率不同，所以两个波形增益不一样，那放大后波形就会失真）<blockquote><p>关于相位失真，好奇这个相位怎么样才能移位呢？？？</p></blockquote></li><li>非线性失真系数（THD）：放大器本身是非线性器件晶体管组成，所以，本身输出会产生非线性失真。有个非线性失真系数，不写了。</li><li>放大器的输出动态范围：就是避免放大器的非线性失真，不失真的电压有个范围，最大值叫做放大器的输出动态范围。</li><li>放大器传输特性：随着输入电压的变化，输出电压线性变化叫做线性输入范围，非线性临界点叫做非线性限幅。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;什么是放大器？&quot;&gt;&lt;a href=&quot;#什么是放大器？&quot; class=&quot;headerlink&quot; title=&quot;什么是放大器？&quot;&gt;&lt;/a&gt;什么是放大器？&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;概念：功率放大倍数 大于 1 （变压器不是放大器）&lt;/li&gt;
&lt;li&gt;同向放大器：输入输出信号相位相同&lt;/li&gt;
&lt;li&gt;反向放大器：输入输出信号相位相反&lt;/li&gt;
&lt;li&gt;放大器有两正负两极，作用同上，称为同向输入端和反向输入端
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch1-1 导论</title>
    <link href="http://yoursite.com/2020/02/17/ch1-1%20%E5%AF%BC%E8%AE%BA/"/>
    <id>http://yoursite.com/2020/02/17/ch1-1%20%E5%AF%BC%E8%AE%BA/</id>
    <published>2020-02-17T12:18:27.021Z</published>
    <updated>2020-02-21T14:11:35.592Z</updated>
    
    <content type="html"><![CDATA[<h3 id="为什么学？"><a href="#为什么学？" class="headerlink" title="为什么学？"></a>为什么学？</h3><h4 id="模电："><a href="#模电：" class="headerlink" title="模电："></a>模电：</h4><ol><li>发展-&gt;高频/数电/微电子/电力电子</li><li>可直接解决工程问题：（温度……——&gt;传感器——&gt;电信号） /  数字信号——&gt;模拟电路——&gt;模拟信号</li><li>时间和幅度连续变化<a id="more"></a><h3 id="学什么？"><a href="#学什么？" class="headerlink" title="学什么？"></a>学什么？</h3><h4 id="电子器件"><a href="#电子器件" class="headerlink" title="电子器件"></a>电子器件</h4></li><li>电阻：耗能元件 / 电容：储能元件（有损耗） /电感 变压器 / 受控源 /非线性器件……<h4 id="物理非线性器件的探索："><a href="#物理非线性器件的探索：" class="headerlink" title="物理非线性器件的探索："></a>物理非线性器件的探索：</h4></li><li>1904年–伦敦大学–弗莱明–发明电子管（电流电压非线性/有结构脆弱等缺点）</li><li>1947年–贝尔实验室–威廉，肖克利等三位科学家发明–晶体三极管（半导体器件）</li><li>1952年–英国皇家雷达研究所–提出”集成电路”概念</li><li>1958年–美国德州仪器公司–Jack S.Kilby–发明第一片集成电路</li><li>现代–系统集成（SOC）–&gt;集成度高<h4 id="课程"><a href="#课程" class="headerlink" title="课程"></a>课程</h4></li><li>器件：晶体管，场效应管</li><li>放大器：顾名思义，放大小信号 <strong><em>且不失真</em></strong></li><li>滤波器：给输入信号去除噪声</li><li>振荡器：上电即可发出脉冲（如三角波，方波，正弦波……如555振荡器）</li><li>电源：输入交流——&gt;输出稳定直流电<h4 id="模电难点-amp-解决方案"><a href="#模电难点-amp-解决方案" class="headerlink" title="模电难点&amp;解决方案"></a>模电难点&amp;解决方案</h4></li><li>敏感性：器件非线性，RC数值，温度，频率（<strong><em>这里的频率指的是什么频率？只是输入信号吗？</em></strong>），噪声/干扰，电源电压……</li><li>解决方案：1.提高器件性能 2.引入负反馈<blockquote><p>针对负反馈，这里咱多写两句，这里和何凯明同学提出的残差网络（Resnet）很相似（其实何凯明同学也是借鉴了电路中的负反馈）只是神经网络中，他是将输出通过负反馈与前者相比较，取loss的优质带入下一层网络，而这里是将放大器倍数增大，增益只取决于F（反馈）这很有意思，但还有一些不太明白：<strong><em>这里F有些抽象，不明白这是怎么工作的，是什么器件来反馈 ?</em></strong></p></blockquote></li></ol><h4 id="重要思想……（贯彻全书？）"><a href="#重要思想……（贯彻全书？）" class="headerlink" title="重要思想……（贯彻全书？）"></a>重要思想……（贯彻全书？）</h4><p>$\cfrac{A}{1+AF}$ $\thickapprox$ $\cfrac{1}{F}$</p><p> 相减和高倍放大器集成==&gt;运算放大器（operational Amplifier）<br> 所以：运放+反馈 = …………电路</p><p>2020.2.17</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;为什么学？&quot;&gt;&lt;a href=&quot;#为什么学？&quot; class=&quot;headerlink&quot; title=&quot;为什么学？&quot;&gt;&lt;/a&gt;为什么学？&lt;/h3&gt;&lt;h4 id=&quot;模电：&quot;&gt;&lt;a href=&quot;#模电：&quot; class=&quot;headerlink&quot; title=&quot;模电：&quot;&gt;&lt;/a&gt;模电：&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;发展-&amp;gt;高频/数电/微电子/电力电子&lt;/li&gt;
&lt;li&gt;可直接解决工程问题：（温度……——&amp;gt;传感器——&amp;gt;电信号） /  数字信号——&amp;gt;模拟电路——&amp;gt;模拟信号&lt;/li&gt;
&lt;li&gt;时间和幅度连续变化
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>Git远程管理Github仓库&amp;遇到的问题</title>
    <link href="http://yoursite.com/2020/02/16/Git%E8%BF%9C%E7%A8%8B%E7%AE%A1%E7%90%86Github%E4%BB%93%E5%BA%93&amp;%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2020/02/16/Git%E8%BF%9C%E7%A8%8B%E7%AE%A1%E7%90%86Github%E4%BB%93%E5%BA%93&amp;%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98/</id>
    <published>2020-02-16T12:43:29.958Z</published>
    <updated>2020-02-21T14:17:02.534Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>一直在网络上看到很多从业好多年的后端开发人员推荐使用Git来进行代码的管理，这次在学习中简单总结一下。</p><a id="more"></a><h3 id="git上传代码流程"><a href="#git上传代码流程" class="headerlink" title="git上传代码流程"></a>git上传代码流程</h3><ol><li>$ git clone<br>将github上download处的https地址复制，在电脑本地创建git.文件夹。</li><li>$ git add.<br>将本地文件保存到暂存区</li><li>$ git commit -m<br>文件提交到本地仓库</li><li>$ git push<br>将文件上传到远程仓库</li><li>$ git pull<br>拉取/同步远程仓库的代码到本地，<br>每次开始开发和提交前都要记得pull一下，以免提交发生冲突。<h3 id="git上传遇到的问题"><a href="#git上传遇到的问题" class="headerlink" title="git上传遇到的问题"></a>git上传遇到的问题</h3><h4 id="1-The-remote-end-hung-up-unexpectedly"><a href="#1-The-remote-end-hung-up-unexpectedly" class="headerlink" title="1.The remote end hung up unexpectedly"></a>1.The remote end hung up unexpectedly</h4>这个挺头疼的，在Stack Overflow上面找了几种方法，有人说的网络的问题，有人说是暂缓区不足，有人说是网络的问题，也有说关掉防火墙和杀毒软件会OK，还去调节参数。。。因为时间的原因，我暂时还没有试过每一种方法，目前尝试的几种方法是不成功的。git push完后会等待很长时间然后报错，/呜呜呜。。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;一直在网络上看到很多从业好多年的后端开发人员推荐使用Git来进行代码的管理，这次在学习中简单总结一下。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>icarus 美化主题的坑</title>
    <link href="http://yoursite.com/2020/02/15/icarus%20%E7%BE%8E%E5%8C%96%E4%B8%BB%E9%A2%98%E7%9A%84%E5%9D%91/"/>
    <id>http://yoursite.com/2020/02/15/icarus%20%E7%BE%8E%E5%8C%96%E4%B8%BB%E9%A2%98%E7%9A%84%E5%9D%91/</id>
    <published>2020-02-15T15:06:21.072Z</published>
    <updated>2020-02-21T14:14:11.026Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>很羡慕别人做的很精美，简约大方的博客，所以自己也开始捣鼓，在Google和百度上找方法，但有些方法在自己这里不成功，下面我自己总结一下这些坑。</p><a id="more"></a><h3 id="1-鼠标点击爱心效果"><a href="#1-鼠标点击爱心效果" class="headerlink" title="1.鼠标点击爱心效果"></a>1.鼠标点击爱心效果</h3><p>看了几篇文章，都说是在/themes/icarus/sourse/js/src中添加click.js文件，然后复制代码，实际上没有最后的src文件，在/themes/icarus/sourse/js里添加即可。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;很羡慕别人做的很精美，简约大方的博客，所以自己也开始捣鼓，在Google和百度上找方法，但有些方法在自己这里不成功，下面我自己总结一下这些坑。&lt;/p&gt;
    
    </summary>
    
    
      <category term="hexo" scheme="http://yoursite.com/categories/hexo/"/>
    
    
  </entry>
  
  <entry>
    <title>hexo 操作注意</title>
    <link href="http://yoursite.com/2020/02/15/hexo%20%E6%93%8D%E4%BD%9C%E6%B3%A8%E6%84%8F/"/>
    <id>http://yoursite.com/2020/02/15/hexo%20%E6%93%8D%E4%BD%9C%E6%B3%A8%E6%84%8F/</id>
    <published>2020-02-15T09:37:01.435Z</published>
    <updated>2020-02-21T14:14:04.261Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>本文针对自己使用hexo中遇到的问题进行总结</p><h3 id="1-instal安装超时，ERROR"><a href="#1-instal安装超时，ERROR" class="headerlink" title="1 instal安装超时，ERROR"></a>1 instal安装超时，ERROR</h3><p>这个用国内的淘宝镜像源进行安装，在安装过cnpm基础之上，每次将命令中的npm修改成cnpm即可。</p><a id="more"></a><h3 id="2-hexo-d-ERROR"><a href="#2-hexo-d-ERROR" class="headerlink" title="2 hexo d ERROR"></a>2 hexo d ERROR</h3><p>hexo d命令报错 ERROR Deployer not found：git<br>这个是因为没有安装hexo-deployer-git插件，安装即可，命令如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><h3 id="3-hexo-常用指令"><a href="#3-hexo-常用指令" class="headerlink" title="3 hexo 常用指令"></a>3 hexo 常用指令</h3><p>hexo g ：generate 重新生成改动文件<br>hexo clean ：清除缓存文件和已经生成的静态文件（在更换主题时需要）<br>hexo s：server （也可以理解start）启动本地服务器<br>hexo d：deploy 部署网站（上传远程仓库）</p><h3 id="4-hexo-d-ERROR-Timeout"><a href="#4-hexo-d-ERROR-Timeout" class="headerlink" title="4 hexo d ERROR Timeout"></a>4 hexo d ERROR Timeout</h3><p>上传失败，发现是网络问题，发现如果科学上网的话是没有出现过ERROR Timeout的情况</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;本文针对自己使用hexo中遇到的问题进行总结&lt;/p&gt;
&lt;h3 id=&quot;1-instal安装超时，ERROR&quot;&gt;&lt;a href=&quot;#1-instal安装超时，ERROR&quot; class=&quot;headerlink&quot; title=&quot;1 instal安装超时，ERROR&quot;&gt;&lt;/a&gt;1 instal安装超时，ERROR&lt;/h3&gt;&lt;p&gt;这个用国内的淘宝镜像源进行安装，在安装过cnpm基础之上，每次将命令中的npm修改成cnpm即可。&lt;/p&gt;
    
    </summary>
    
    
      <category term="hexo" scheme="http://yoursite.com/categories/hexo/"/>
    
    
  </entry>
  
  <entry>
    <title>CNN经典网络Lenet5</title>
    <link href="http://yoursite.com/2020/02/14/CNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9CLenet5/"/>
    <id>http://yoursite.com/2020/02/14/CNN%E7%BB%8F%E5%85%B8%E7%BD%91%E7%BB%9CLenet5/</id>
    <published>2020-02-14T13:10:29.088Z</published>
    <updated>2020-02-27T03:54:24.583Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在跟着老师学习写第一个CNN（卷积神经网络）的时候，很多地方感到困惑，对此，我决定把卷积神经网络中的经典案例进行简单的分析，这个是1998年LeCun所提出的LeNet 5，是卷积神经网络的鼻祖，也是卷积神经网络的“Hello，world”。</p><h3 id="网络分析"><a href="#网络分析" class="headerlink" title="网络分析"></a>网络分析</h3><p><img src="https://img-blog.csdn.net/20171018154341615?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaGFwcHlvcmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="LeNet5"></p><p>Lenet 5 一共有2个卷积层，2个池化层，3个全连接层。</p><a id="more"></a><h4 id="LeNet-5第一层：卷积层C1"><a href="#LeNet-5第一层：卷积层C1" class="headerlink" title="LeNet-5第一层：卷积层C1"></a>LeNet-5第一层：卷积层C1</h4><p>C1层是卷积层，形成6个特征图谱。卷积的输入区域大小是5x5，每个特征图谱内参数共享，即每个特征图谱内只使用一个共同卷积核，卷积核有5x5个连接参数加上1个偏置共26个参数。卷积区域每次滑动一个像素，这样卷积层形成的每个特征图谱大小是(32-5)/1+1=28x28。C1层共有26x6=156个训练参数，有(5x5+1)x28x28x6=122304个连接。C1层的连接结构如下所示。</p><p><img src="https://img-blog.csdn.net/20171018154917808?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaGFwcHlvcmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><h4 id="LeNet-5第二层：池化层S2"><a href="#LeNet-5第二层：池化层S2" class="headerlink" title="LeNet-5第二层：池化层S2"></a>LeNet-5第二层：池化层S2</h4><p>S2层是一个下采样层（为什么是下采样？利用图像局部相关性的原理，对图像进行子抽样，可以减少数据处理量同时保留有用信息）。C1层的6个28x28的特征图谱分别进行以2x2为单位的下抽样得到6个14x14（（28-2）/2+1）的图。每个特征图谱使用一个下抽样核。5x14x14x6=5880个连接。S2层的网络连接结构如下图</p><p><img src="https://img-blog.csdn.net/20171018155110551?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaGFwcHlvcmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><h4 id="LeNet-5第三层：卷积层C3"><a href="#LeNet-5第三层：卷积层C3" class="headerlink" title="LeNet-5第三层：卷积层C3"></a>LeNet-5第三层：卷积层C3</h4><p>C3层是一个卷积层，卷积和和C1相同，不同的是C3的每个节点与S2中的多个图相连。C3层有16个10x10（14-5+1）的图，每个图与S2层的连接的方式如下表 所示。C3与S2中前3个图相连的卷积结构见下图.这种不对称的组合连接的方式有利于提取多种组合特征。该层有(5x5x3+1)x6 + (5x5x4 + 1) x 3 + (5x5x4 +1)x6 + (5x5x6+1)x1 = 1516个训练参数，共有1516x10x10=151600个连接。</p><p><img src="https://img-blog.csdn.net/20171018155242912?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaGFwcHlvcmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><h4 id="LeNet-5第四层：池化层S4"><a href="#LeNet-5第四层：池化层S4" class="headerlink" title="LeNet-5第四层：池化层S4"></a>LeNet-5第四层：池化层S4</h4><p>S4是一个下采样层。C3层的16个10x10的图分别进行以2x2为单位的下抽样得到16个5x5的图。5x5x5x16=2000个连接。连接的方式与S2层类似，如下所示。<br><img src="https://img-blog.csdn.net/20171018155446634?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaGFwcHlvcmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><h4 id="LeNet-5第五层：全连接层C5"><a href="#LeNet-5第五层：全连接层C5" class="headerlink" title="LeNet-5第五层：全连接层C5"></a>LeNet-5第五层：全连接层C5</h4><p>C5层是一个全连接层。由于S4层的16个图的大小为5x5，与卷积核的大小相同，所以卷积后形成的图的大小为1x1。这里形成120个卷积结果。每个都与上一层的16个图相连。所以共有(5x5x16+1)x120 = 48120个参数，同样有48120个连接。C5层的网络结构如下所示。<br><img src="https://img-blog.csdn.net/20171018155558086?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaGFwcHlvcmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><h4 id="LeNet-5第六层：全连接层F6"><a href="#LeNet-5第六层：全连接层F6" class="headerlink" title="LeNet-5第六层：全连接层F6"></a>LeNet-5第六层：全连接层F6</h4><p>F6层是全连接层。F6层有84个节点，对应于一个7x12的比特图，该层的训练参数和连接数都是(120 + 1)x84=10164.<br><img src="https://img-blog.csdn.net/20171018155923735?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaGFwcHlvcmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><h4 id="LeNet-5第七层：全连接层Output"><a href="#LeNet-5第七层：全连接层Output" class="headerlink" title="LeNet-5第七层：全连接层Output"></a>LeNet-5第七层：全连接层Output</h4><p>略<br><img src="https://img-blog.csdn.net/20171018160214082?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaGFwcHlvcmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt=""></p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>1.<a href="https://blog.csdn.net/happyorg/article/details/78274066">https://blog.csdn.net/happyorg/article/details/78274066</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;在跟着老师学习写第一个CNN（卷积神经网络）的时候，很多地方感到困惑，对此，我决定把卷积神经网络中的经典案例进行简单的分析，这个是1998年LeCun所提出的LeNet 5，是卷积神经网络的鼻祖，也是卷积神经网络的“Hello，world”。&lt;/p&gt;
&lt;h3 id=&quot;网络分析&quot;&gt;&lt;a href=&quot;#网络分析&quot; class=&quot;headerlink&quot; title=&quot;网络分析&quot;&gt;&lt;/a&gt;网络分析&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://img-blog.csdn.net/20171018154341615?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvaGFwcHlvcmc=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;LeNet5&quot;&gt;&lt;/p&gt;
&lt;p&gt;Lenet 5 一共有2个卷积层，2个池化层，3个全连接层。&lt;/p&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
  </entry>
  
</feed>
