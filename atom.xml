<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>仰望星空</title>
  
  <subtitle>keep learning</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-03-15T12:41:06.063Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>王子晰</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>visdom--Pytorch可视化工具</title>
    <link href="http://yoursite.com/2020/03/15/visdom--Pytorch%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/"/>
    <id>http://yoursite.com/2020/03/15/visdom--Pytorch%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7/</id>
    <published>2020-03-15T10:23:34.488Z</published>
    <updated>2020-03-15T12:41:06.063Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>从安装visdom到我现在写这篇博客已经有至少半个月了吧，实际上使用visdom的次数可能也就2，3次，上次使用visdom是学习MNSIT的时候，这次学习迁移学习-自定义数据集操作的时候，再一次用到，我一下子也想不起来，但是数据可视化十分重要，包括数据和模型的调试，科研论文的撰写，这都是离不开的，所以这里就做一个简单的总结</p><a id="more"></a><h3 id="什么是可视化工具"><a href="#什么是可视化工具" class="headerlink" title="什么是可视化工具"></a>什么是可视化工具</h3><p>这里的可视化就是将，data中的数据，比如说image，或者学习参数，test_acc等用图表等形式显示出来。</p><p>现在TensorFlow用的是TensorBoard，对应的pytorch有一个TensorBoardX，和visdom，两个都是可视化工具，但是有所不同，TensorBoardX在绘制图像等时会产生文件夹，如果数据很大，就会存储大量的文件，导致运行很卡；另外，在代码方面TensorBoardX不能直接支持tensor变量，需要再代码中将其转换成numpy数据类型，绘制图像帧数在30s更新一次。</p><p>相比较而言，visdom不会产生文件夹，而且可以直接支持Tensor变量，当然最后绘制图像都是使用的numpy数据类型，但是visdom在内部进行了Tensor——&gt;numpy数据类型的一个转换，因此在代码里可以省去变量类型转换的步骤，最重要的是visdom的数据更新速度是5s更新一次数据点。</p><h3 id="visdom的安装"><a href="#visdom的安装" class="headerlink" title="visdom的安装"></a>visdom的安装</h3><p>这里主要参考网易云课堂上龙龙老师的讲解，但是实际安装时遇到了一些别的困难，导致，没有办法像视频演示一样安装，网上查询资料说是可能因为pip 安装源来自于墙外网站，所以部分下载文件被过滤(删除)了，然后我科学上网以后，下载完毕还是一样的缺失文件，在网络上大部分是Linux的操作，对于Windows的解决方法如下：</p><blockquote><p>在CSDN上下载一个static文件(淘宝1.2)，里面有别人下载好的json等文件，将原来文件中static替换掉即可成功运行。</p></blockquote><h3 id="visdom的使用"><a href="#visdom的使用" class="headerlink" title="visdom的使用"></a>visdom的使用</h3><p>打开cmd或者git bash(终端)，输入：</p><blockquote><p>python -m visdom.server</p></blockquote><p><strong><em>注：上面窗口不要关掉，后台继续运行</em></strong><br>可以得到一个浏览器地址，将该地址在浏览器中打开即可</p><h3 id="visdom语法"><a href="#visdom语法" class="headerlink" title="visdom语法"></a>visdom语法</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> visdom <span class="keyword">import</span> Visdom</span><br><span class="line">viz = visdom()</span><br><span class="line"></span><br><span class="line">viz.line([<span class="number">0.</span>], [<span class="number">0.</span>], win=<span class="string">'train_loss'</span>, opts=dict(title=<span class="string">'train loss'</span>)) <span class="comment">#新建一个窗口(图像)，初始化变量，第一个[0.]是Y，第二个[0.]是X，其中win是Window的缩写，title是图像的名字</span></span><br><span class="line"></span><br><span class="line">viz.line([[<span class="number">0.0</span>, <span class="number">0.0</span>]], [<span class="number">0.</span>], win=<span class="string">'test'</span>, opts=dict(title=<span class="string">'test loss&amp;acc.'</span>, legend=[<span class="string">'loss'</span>, <span class="string">'acc.'</span>]))</span><br><span class="line"><span class="comment">#新建一个窗口(图像)，两个变量，第一个[0.0，0.0]代表两个Y变量，第二个[0.]代表X</span></span><br><span class="line"></span><br><span class="line">viz.line([[test_loss, correct / len(test_loader.dataset)]],</span><br><span class="line">[global_step], win=<span class="string">'test'</span>, update=<span class="string">'append'</span>)</span><br><span class="line"><span class="comment">#初始化完毕，就带入变量，其中update='append'指的是数据实时更新</span></span><br><span class="line"></span><br><span class="line">viz.images(data.view(<span class="number">-1</span>, <span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>), win=<span class="string">'x'</span>)</span><br><span class="line"><span class="comment">#载入MNSIT的图片</span></span><br><span class="line"></span><br><span class="line">viz.text(str(pred.detach().cpu().numpy()), win=<span class="string">'pred'</span>,</span><br><span class="line">opts=dict(title=<span class="string">'pred'</span>))</span><br><span class="line"><span class="comment">#载入MNSIT的predict数字，最终网络根据图片预测出来的数字。</span></span><br></pre></td></tr></table></figure><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>上面代码中列举了几个最简单的，绘制曲线，载入图片，text文案的方法，以后如果又忘记了，就有可以复习的资料了，加油~</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;从安装visdom到我现在写这篇博客已经有至少半个月了吧，实际上使用visdom的次数可能也就2，3次，上次使用visdom是学习MNSIT的时候，这次学习迁移学习-自定义数据集操作的时候，再一次用到，我一下子也想不起来，但是数据可视化十分重要，包括数据和模型的调试，科研论文的撰写，这都是离不开的，所以这里就做一个简单的总结&lt;/p&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>如何在AD18新建工程</title>
    <link href="http://yoursite.com/2020/03/13/%E5%A6%82%E4%BD%95%E5%9C%A8AD18%E6%96%B0%E5%BB%BA%E5%B7%A5%E7%A8%8B/"/>
    <id>http://yoursite.com/2020/03/13/%E5%A6%82%E4%BD%95%E5%9C%A8AD18%E6%96%B0%E5%BB%BA%E5%B7%A5%E7%A8%8B/</id>
    <published>2020-03-13T14:33:08.833Z</published>
    <updated>2020-03-13T14:52:09.161Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在2020年一月份学的AD18，过了两个月重新上手画板子，真的是忘的一干二净了，雕刻机的操作步骤也忘记了，方便雕刻，好像板子要反着画？？到时候是又要请教春哥和钱哥了，先来总结一下AD18新建工程的步骤。</p><a id="more"></a><h3 id="新建步骤"><a href="#新建步骤" class="headerlink" title="新建步骤"></a>新建步骤</h3><ol><li>先在需要的地方新建文件夹，重命名</li><li>打开AD18，点击文件，新的，项目，(使用默认模式Default)选择好新建路径，创建，ok</li><li>然后右击左栏产生的new Project，分别创建新的原理图，PCB，元件库，封装库即可</li></ol><p>2020.3.13</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;在2020年一月份学的AD18，过了两个月重新上手画板子，真的是忘的一干二净了，雕刻机的操作步骤也忘记了，方便雕刻，好像板子要反着画？？到时候是又要请教春哥和钱哥了，先来总结一下AD18新建工程的步骤。&lt;/p&gt;
    
    </summary>
    
    
      <category term="NXP智能车" scheme="http://yoursite.com/categories/NXP%E6%99%BA%E8%83%BD%E8%BD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>ch5-6 共基极放大器以及三种组态对比</title>
    <link href="http://yoursite.com/2020/03/13/ch5-6%20%E5%85%B1%E5%9F%BA%E6%9E%81%E6%94%BE%E5%A4%A7%E5%99%A8%E4%BB%A5%E5%8F%8A%E4%B8%89%E7%A7%8D%E7%BB%84%E6%80%81%E5%AF%B9%E6%AF%94/"/>
    <id>http://yoursite.com/2020/03/13/ch5-6%20%E5%85%B1%E5%9F%BA%E6%9E%81%E6%94%BE%E5%A4%A7%E5%99%A8%E4%BB%A5%E5%8F%8A%E4%B8%89%E7%A7%8D%E7%BB%84%E6%80%81%E5%AF%B9%E6%AF%94/</id>
    <published>2020-03-13T09:42:10.552Z</published>
    <updated>2020-03-13T13:24:45.439Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch5-5 共集电极放大电路分析</title>
    <link href="http://yoursite.com/2020/03/13/ch5-5%20%E5%85%B1%E9%9B%86%E7%94%B5%E6%9E%81%E6%94%BE%E5%A4%A7%E7%94%B5%E8%B7%AF%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2020/03/13/ch5-5%20%E5%85%B1%E9%9B%86%E7%94%B5%E6%9E%81%E6%94%BE%E5%A4%A7%E7%94%B5%E8%B7%AF%E5%88%86%E6%9E%90/</id>
    <published>2020-03-13T09:12:59.628Z</published>
    <updated>2020-03-13T13:24:42.511Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>就是输入电阻大，输出电阻小，适合放置在其它的部分是电路分析，公式有需要，后期再补上。</p><a id="more"></a><p>2020.3.13</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;就是输入电阻大，输出电阻小，适合放置在其它的部分是电路分析，公式有需要，后期再补上。&lt;/p&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch5-4 带电流负反馈的共射放大器性能分析</title>
    <link href="http://yoursite.com/2020/03/13/ch5-4%20%E5%B8%A6%E7%94%B5%E6%B5%81%E8%B4%9F%E5%8F%8D%E9%A6%88%E7%9A%84%E5%85%B1%E5%B0%84%E6%94%BE%E5%A4%A7%E5%99%A8%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2020/03/13/ch5-4%20%E5%B8%A6%E7%94%B5%E6%B5%81%E8%B4%9F%E5%8F%8D%E9%A6%88%E7%9A%84%E5%85%B1%E5%B0%84%E6%94%BE%E5%A4%A7%E5%99%A8%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/</id>
    <published>2020-03-13T08:29:14.916Z</published>
    <updated>2020-03-13T09:12:29.922Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>在前面共射放大电路的基础上，将共射极电阻的旁路电容去电或者再加一个电阻，分析一下这个对电路的交流参数有什么影响。</p><a id="more"></a><h3 id="对交流参数的影响"><a href="#对交流参数的影响" class="headerlink" title="对交流参数的影响"></a>对交流参数的影响</h3><h4 id="A-u-对电压放大倍数的影响"><a href="#A-u-对电压放大倍数的影响" class="headerlink" title="$A_u$对电压放大倍数的影响"></a>$A_u$对电压放大倍数的影响</h4><p>$$A_u=\cfrac{U_o}{U_i}=-\cfrac{\beta I_b(R_C||R_L)}{[Rbe+(1+\beta)R_E]I_b}=-\cfrac{\beta I_bR_L`}{Rbe+(1+\beta)R_E}$$</p><blockquote><p>if满足<br>$$(1+\beta)R_E&gt;&gt;Rbe,则A_u\approx-\cfrac{R_L`}{R_E}$$</p></blockquote><h4 id="R-i-输入电阻"><a href="#R-i-输入电阻" class="headerlink" title="$R_i$输入电阻"></a>$R_i$输入电阻</h4><p>$$R_i=Rb1 || Rb2||[Rbe+(1+\beta)R_E]$$</p><h4 id="R-o-输出电阻"><a href="#R-o-输出电阻" class="headerlink" title="$R_o$输出电阻"></a>$R_o$输出电阻</h4><p>没变化，因为$R_o`&gt;&gt;R_C$,所以$R_o\approx R_C$</p><p>2020.3.13</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;在前面共射放大电路的基础上，将共射极电阻的旁路电容去电或者再加一个电阻，分析一下这个对电路的交流参数有什么影响。&lt;/p&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch5-3 共射放大器性能分析</title>
    <link href="http://yoursite.com/2020/03/13/ch5-3%20%E5%85%B1%E5%B0%84%E6%94%BE%E5%A4%A7%E5%99%A8%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2020/03/13/ch5-3%20%E5%85%B1%E5%B0%84%E6%94%BE%E5%A4%A7%E5%99%A8%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90/</id>
    <published>2020-03-13T07:15:58.642Z</published>
    <updated>2020-03-13T08:28:31.416Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前晚上做练习的时候(就是黄老师慕课堂上网课)，有的题目实在看不明白，也不知道同学们都是咋写出来的？所以没有提交，这回mooc上面看过网课了，才明白，这里总结一下</p><a id="more"></a><h3 id="分析步骤"><a href="#分析步骤" class="headerlink" title="分析步骤"></a>分析步骤</h3><p>这个分析方法的全称：共射极放大器的交流等效电路分析法</p><ol><li>画出直流通路，估算直流静态参数<blockquote><p>画直流通路就是，电容开路，交流电源短路，直流静态参数：$I_B I_C Ube Uce$</p></blockquote></li><li>在直流通路的基础上画出交流通路，估算交流参数<blockquote><p>画交流通路，就是电容短路，直流电源接地，为什么接地呢，因为对于交流信号而言，直流信号是平的(不考虑噪声，理论上是这样的)，交流分量就是0，所以和接地没有区别，所以就一起接地啦。交流参数：$A_u A_i R_i R_o Aus$</p></blockquote><h3 id="交流参数说明"><a href="#交流参数说明" class="headerlink" title="交流参数说明"></a>交流参数说明</h3></li><li>$A_u$ :电压放大倍数，怎么求记在脑子里</li><li>$A_i$:电流放大倍数，由于前后电阻都很大，所以可以看做微变等效模型前后电流之比，近似$\beta$</li><li>$R_i$:输入电阻,输入的几个电阻并联就可以了<blockquote><p>这里说明一下由于一般来说，Rb&gt;&gt;Rbe,所以，Rbe近似Ri，在微变等效模型中$Rbe=R`+(1+\beta)\cfrac{26mV}{I_CmA}$,所以$\beta$ 越大，输入电阻越大</p></blockquote></li><li>$R_o$:输出电阻，负载电阻不算，往里看，剩下电阻并联即可，因为Rce比较大，所以输出电阻$R_o近似=R_C$</li><li>$Aus$:源电压放大倍数，就是输出电压和信号源电压的比值，注意信号源电压和输入电压的区别是，输入电压在信号源上的内阻分压了。分压求解即可</li></ol><p>2020.3.13</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;之前晚上做练习的时候(就是黄老师慕课堂上网课)，有的题目实在看不明白，也不知道同学们都是咋写出来的？所以没有提交，这回mooc上面看过网课了，才明白，这里总结一下&lt;/p&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>电解电容与滤波电容</title>
    <link href="http://yoursite.com/2020/03/12/%E7%94%B5%E8%A7%A3%E7%94%B5%E5%AE%B9%E4%B8%8E%E6%BB%A4%E6%B3%A2%E7%94%B5%E5%AE%B9/"/>
    <id>http://yoursite.com/2020/03/12/%E7%94%B5%E8%A7%A3%E7%94%B5%E5%AE%B9%E4%B8%8E%E6%BB%A4%E6%B3%A2%E7%94%B5%E5%AE%B9/</id>
    <published>2020-03-12T15:07:33.833Z</published>
    <updated>2020-03-13T06:04:55.922Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>今天在研究电源部分的降压模块，其中的线性电源模块(TPS768xx)输入端接了两个电容，一个普通电解电容，一个是滤波电容，那么有两个问题产生了：为什么要接两个电容滤波呢，一个不够么？电解电容和滤波电容的区别？</p><a id="more"></a><h3 id="电解电容和滤波电容的区别以及滤波工作原理"><a href="#电解电容和滤波电容的区别以及滤波工作原理" class="headerlink" title="电解电容和滤波电容的区别以及滤波工作原理"></a>电解电容和滤波电容的区别以及滤波工作原理</h3><p>首先，电解电容和滤波电容一样也具有滤波的作用，不过他们的用处和材料有所不同，同为滤波，工作原理很简单，电容是起到一个充电再放电的过程，电容作为电能的储存容器和储水容器一样，当有水流入的时候就把阀门打开，存储水量，到存储上限了以后再放出来，这其中把水放入和将水放出都需要一定的时间。因此在通过电容以后，电压是无法突变的！因此电容实际上是对电压做了一次平滑处理。</p><h3 id="为什么接两个电容滤波呢"><a href="#为什么接两个电容滤波呢" class="headerlink" title="为什么接两个电容滤波呢"></a>为什么接两个电容滤波呢</h3><p>原因很简单，因为不同容值的电容对不同频率的纹波(噪声)进行滤波，一般0.1uf是高频滤波，可以提高芯片的高频性能，10uf是一般滤波。一般都是一个大容量的电容和小容量的电容并联接在输入端口，可以起到一般滤波加上高频滤波的作用。</p><blockquote><p>在百度百科上查阅有搭配是大电容(1000uf),小电容(0.01uf)，在用TPS786xx时(线性电源降压模块)，数据手册上建议是大电容(10uf)，小电容(0.1uf)</p></blockquote><h3 id="为什么会有高频噪声呢"><a href="#为什么会有高频噪声呢" class="headerlink" title="为什么会有高频噪声呢"></a>为什么会有高频噪声呢</h3><ol><li>当刚开始上电的时候会有负载瞬态响应，电流很大，响应很大，会产生高频噪声</li><li>此外，在高功耗电路上，尤其是电源模块，其中的功率线(<strong><em>有大电流通过的线，不是信号线</em></strong>)上，由于输出功率的变化，会导致电源电压不稳定，从而产生自激现象，产生部分高频噪声</li></ol><p>2020.3.13</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;今天在研究电源部分的降压模块，其中的线性电源模块(TPS768xx)输入端接了两个电容，一个普通电解电容，一个是滤波电容，那么有两个问题产生了：为什么要接两个电容滤波呢，一个不够么？电解电容和滤波电容的区别？&lt;/p&gt;
    
    </summary>
    
    
      <category term="NXP智能车" scheme="http://yoursite.com/categories/NXP%E6%99%BA%E8%83%BD%E8%BD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>电机选型以及驱动电路</title>
    <link href="http://yoursite.com/2020/03/12/%E7%94%B5%E6%9C%BA%E9%80%89%E5%9E%8B%E4%BB%A5%E5%8F%8A%E9%A9%B1%E5%8A%A8%E7%94%B5%E8%B7%AF/"/>
    <id>http://yoursite.com/2020/03/12/%E7%94%B5%E6%9C%BA%E9%80%89%E5%9E%8B%E4%BB%A5%E5%8F%8A%E9%A9%B1%E5%8A%A8%E7%94%B5%E8%B7%AF/</id>
    <published>2020-03-11T17:16:43.983Z</published>
    <updated>2020-03-11T17:56:08.458Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>智能车制作新手上路，现在开始准备画整车的电路板，电路板一般分成4个模块：电源部分，驱动部分，主控部分，信号部分，这篇文章简单总结一下电机的选择和对应驱动电路的选择</p><a id="more"></a><h4 id="电机"><a href="#电机" class="headerlink" title="电机"></a>电机</h4><h5 id="电机分类："><a href="#电机分类：" class="headerlink" title="电机分类："></a>电机分类：</h5><p>无刷电机和有刷电机，相对而言，无刷电机更好，它没有损耗，而且支持静音，那么有刷电机就会有损耗，寿命有限，且会有声音。无刷电机的缺点是比较贵，少则几百，多则上千，那么有刷电机价格则从十几到几百不等。</p><blockquote><p>其中无刷电机又分为带霍尔元件和不带霍尔元件两种，霍尔元件的作用是测速，不带霍尔元件测速的话需要测量电机产生的反电动势，通常电机的转速都在几万转每秒，所以智能车转速需求较小，采用带霍尔测速好，对于无人机等需要高转速的应用，高转速会产生较大的反电动势，更容易测量，所以适合不带霍尔元件的无刷电机</p></blockquote><h5 id="减速箱"><a href="#减速箱" class="headerlink" title="减速箱"></a>减速箱</h5><p>由于电机每秒转速通常上万，智能车电机不需要这么大的转速，所以一般会在电机上加减速箱，同时减速箱也有增加扭矩的功能</p><h5 id="扭矩的测量"><a href="#扭矩的测量" class="headerlink" title="扭矩的测量"></a>扭矩的测量</h5><p>通常测量方法是测量电机的抖转电流和空载电流(主要是测抖转电流)，空载电流是电机不带负载的工作电流，抖转电流是电机被动停转时的工作电流，那么抖转电流越大，说明电机的带载能力越强</p><blockquote><p>测量方法是给电机加上轮子，通电开始转动，然后将轮子捏住，使其不转动，然后测量电机的电流</p></blockquote><h4 id="驱动电路"><a href="#驱动电路" class="headerlink" title="驱动电路"></a>驱动电路</h4><p>理论上，有刷电机的两端接上单片机的两个引脚，一端接3.3V，一端接0V，电机就可以动了，但是实际上不可能，因为单片机提供电流太小了，通常单片机的引脚电流不超过20mA，而小电机的驱动电流都多达160mA左右，所以需要再单片机和电机之间添加驱动电路</p><h5 id="H桥电路和集成芯片"><a href="#H桥电路和集成芯片" class="headerlink" title="H桥电路和集成芯片"></a>H桥电路和集成芯片</h5><p>最常见的驱动电路就是H桥电路了，由二极管组成(不贴图了，感兴趣且忘记了的话，自己再网上再搜一搜)，很麻烦，而且可以加载十几A的电流，我们小车用不到这么多，所以可以使用集成芯片，在芯片的周围会添加一些电容和电阻，所有方案可以在淘宝上参考产品级别的模块原理图或者是智能车的技术手册</p><blockquote><p>这里有几个小技巧：</p></blockquote><ol><li>在单片机的引脚输入端上加300Ω(这个阻值是经验值)的电阻，目的是保护电路。</li><li>在单片机的两个引脚输入端分别下拉电阻，也是保证在每有接到单片机引脚高电平的时候，一直强制要求两个输入端保持低电平</li><li>在电机的两个输入端口接上电容，起到滤波的作用</li></ol><p>2020.3.12</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;p&gt;智能车制作新手上路，现在开始准备画整车的电路板，电路板一般分成4个模块：电源部分，驱动部分，主控部分，信号部分，这篇文章简单总结一下电机的选择和对应驱动电路的选择&lt;/p&gt;
    
    </summary>
    
    
      <category term="NXP智能车" scheme="http://yoursite.com/categories/NXP%E6%99%BA%E8%83%BD%E8%BD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>ch5-2 放大器的偏置电路和直流工作点的判断</title>
    <link href="http://yoursite.com/2020/03/11/ch5-2%20%E6%94%BE%E5%A4%A7%E5%99%A8%E7%9A%84%E5%81%8F%E7%BD%AE%E7%94%B5%E8%B7%AF%E5%92%8C%E7%9B%B4%E6%B5%81%E5%B7%A5%E4%BD%9C%E7%82%B9%E7%9A%84%E5%88%A4%E6%96%AD/"/>
    <id>http://yoursite.com/2020/03/11/ch5-2%20%E6%94%BE%E5%A4%A7%E5%99%A8%E7%9A%84%E5%81%8F%E7%BD%AE%E7%94%B5%E8%B7%AF%E5%92%8C%E7%9B%B4%E6%B5%81%E5%B7%A5%E4%BD%9C%E7%82%B9%E7%9A%84%E5%88%A4%E6%96%AD/</id>
    <published>2020-03-11T09:18:42.227Z</published>
    <updated>2020-03-13T07:15:12.404Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>晶体管在放大应用的时候，要求外电路将晶体管偏置在放大区(若是NPN管就是发射结正偏，集电结反偏)，使得信号在放大的时候不产生线性失真(静态工作点太靠前的话工作在非线性区，就会失真)</p><a id="more"></a><h4 id="对偏置电路的要求"><a href="#对偏置电路的要求" class="headerlink" title="对偏置电路的要求"></a>对偏置电路的要求</h4><ol><li>在更换过管子或者温度变化之后，偏置的静态工作点也要力求稳定，即保持Icq和Uceq稳定</li><li>电路形式尽量简单，如采用单一电源，尽可能少使用电阻等</li><li>对信号和直流能量的损耗要尽量小，如减小信号中的分压分流损耗等<h4 id="常见偏置电路"><a href="#常见偏置电路" class="headerlink" title="常见偏置电路"></a>常见偏置电路</h4><h5 id="固定偏流电路"><a href="#固定偏流电路" class="headerlink" title="固定偏流电路"></a>固定偏流电路</h5>没有图，自己看PDF回顾(就是一个电源，集电极($R_B$)和基极($R_C$)各添加一个电阻)，其中：<br>$$I_C=\beta I_B$$<br>$$Uce=Ucc-I_C*R_C$$<blockquote><p>由于温度变化或者更换管子引起$\beta$和Iceo的变化，该电路不稳定</p></blockquote><h5 id="电流负反馈型偏置电路"><a href="#电流负反馈型偏置电路" class="headerlink" title="电流负反馈型偏置电路"></a>电流负反馈型偏置电路</h5>在上面电路的基础上，在管子的发射极串联电阻即可，形成负反馈效果,可以形成自我调节<blockquote><p>原理：Ic变大——&gt;Ie变大——&gt;Ue变大——&gt;Ube减小——&gt;Ib减小——&gt;Ic减小</p></blockquote></li></ol><p>$R_E$的阻值选取，自己看PDF</p><h4 id="分压式偏置电路"><a href="#分压式偏置电路" class="headerlink" title="分压式偏置电路"></a>分压式偏置电路</h4><p>就是在基极的上下两侧都加上电阻，起到分压的作用，保证基极电位$U_B$固定，这样$I_C$就通过Ube对$U_E$直接产生影响，增强了Ube对$I_C$的控制</p><p>2020.3.13</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;p&gt;晶体管在放大应用的时候，要求外电路将晶体管偏置在放大区(若是NPN管就是发射结正偏，集电结反偏)，使得信号在放大的时候不产生线性失真(静态工作点太靠前的话工作在非线性区，就会失真)&lt;/p&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch5-1 典型放大电路结构特点-三种组态放大器电路</title>
    <link href="http://yoursite.com/2020/03/11/ch5-1%20%E5%85%B8%E5%9E%8B%E6%94%BE%E5%A4%A7%E7%94%B5%E8%B7%AF%E7%BB%93%E6%9E%84%E7%89%B9%E7%82%B9-%E4%B8%89%E7%A7%8D%E7%BB%84%E6%80%81%E6%94%BE%E5%A4%A7%E5%99%A8%E7%94%B5%E8%B7%AF/"/>
    <id>http://yoursite.com/2020/03/11/ch5-1%20%E5%85%B8%E5%9E%8B%E6%94%BE%E5%A4%A7%E7%94%B5%E8%B7%AF%E7%BB%93%E6%9E%84%E7%89%B9%E7%82%B9-%E4%B8%89%E7%A7%8D%E7%BB%84%E6%80%81%E6%94%BE%E5%A4%A7%E5%99%A8%E7%94%B5%E8%B7%AF/</id>
    <published>2020-03-11T08:15:16.522Z</published>
    <updated>2020-03-11T09:17:47.776Z</updated>
    
    <content type="html"><![CDATA[<h4 id="基本放大器组成原则"><a href="#基本放大器组成原则" class="headerlink" title="基本放大器组成原则"></a>基本放大器组成原则</h4><p>基本放大器通常是指的由一个晶体管或场效应管构成的单级放大电路</p><a id="more"></a><h5 id="放大条件："><a href="#放大条件：" class="headerlink" title="放大条件："></a>放大条件：</h5><ol><li>有控制元件：晶体管或者场效应管</li><li>有外加电源提供能量</li><li>偏置在放大区</li><li>待放大信号一定加在发射结(或者栅源结)，不能加到集电结(或者漏极)</li><li>信号可从集电极或者发射极输出，不可以从基极(或栅极)输出</li><li>要有负载，将变化电流转换成电压<blockquote><p>为什么一定将待放大信号加在发射结或集电结：因为栅源电压或者Ube对输出电流的影响最大，也就是跨导比较大，所以将待放大信号加在发射结或集电结。</p></blockquote><h4 id="晶体管放大电路结构"><a href="#晶体管放大电路结构" class="headerlink" title="晶体管放大电路结构"></a>晶体管放大电路结构</h4>分成三类：共发射极，共集电极，共基极三种，其中最常见的就是共发射极电路</li></ol><p><img src="http://www.dzkfw.com.cn/jichu/UploadFiles_6678/201810/20181020175014109.gif" alt=""></p><blockquote><p>阻容耦合共发射极电路</p></blockquote><p>放大过程：Ube变大——&gt;ib变大——&gt;ic=$\beta i_b$变大——&gt;Uo变大</p><blockquote><p>其中耦合电容对直流开路，使信号源和负载不影响工作点</p></blockquote><p>2020.3.11</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;基本放大器组成原则&quot;&gt;&lt;a href=&quot;#基本放大器组成原则&quot; class=&quot;headerlink&quot; title=&quot;基本放大器组成原则&quot;&gt;&lt;/a&gt;基本放大器组成原则&lt;/h4&gt;&lt;p&gt;基本放大器通常是指的由一个晶体管或场效应管构成的单级放大电路&lt;/p&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-11 晶体管和场效应管低频小信号模型(等效电路)</title>
    <link href="http://yoursite.com/2020/03/10/ch4-11%20%E6%99%B6%E4%BD%93%E7%AE%A1%E5%92%8C%E5%9C%BA%E6%95%88%E5%BA%94%E7%AE%A1%E4%BD%8E%E9%A2%91%E5%B0%8F%E4%BF%A1%E5%8F%B7%E6%A8%A1%E5%9E%8B(%E7%AD%89%E6%95%88%E7%94%B5%E8%B7%AF)/"/>
    <id>http://yoursite.com/2020/03/10/ch4-11%20%E6%99%B6%E4%BD%93%E7%AE%A1%E5%92%8C%E5%9C%BA%E6%95%88%E5%BA%94%E7%AE%A1%E4%BD%8E%E9%A2%91%E5%B0%8F%E4%BF%A1%E5%8F%B7%E6%A8%A1%E5%9E%8B(%E7%AD%89%E6%95%88%E7%94%B5%E8%B7%AF)/</id>
    <published>2020-03-10T10:02:20.972Z</published>
    <updated>2020-03-11T09:17:59.549Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>晶体三极管模型在静态工作点附近可以近似看做线性变化，其中变化的部分看做交流分量，线性部分看做直流分量。(本章很多地方听的不明白，后期学习再补回来)</p><a id="more"></a><h4 id="流控等效模型"><a href="#流控等效模型" class="headerlink" title="流控等效模型"></a>流控等效模型</h4><ol><li>Ube变化引起ib的变化体现在体现在管子的输入电阻<blockquote><p>这里的公式下标自己觉得有点复杂，没看懂，晚上看书补充   </p></blockquote></li><li>ib对ic的控制作用体现在受控源 $i_c=\beta i_b$</li><li>Uce对ic的影响体现在输出电阻 $rce=\cfrac{dUce}{di_c}$<blockquote><p>图片PDF找一下，或者书上应该有<br>……</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;p&gt;晶体三极管模型在静态工作点附近可以近似看做线性变化，其中变化的部分看做交流分量，线性部分看做直流分量。(本章很多地方听的不明白，后期学习再补回来)&lt;/p&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-10 MOS场效应管的工作原理和特性参数</title>
    <link href="http://yoursite.com/2020/02/28/ch4-10%20MOS%E5%9C%BA%E6%95%88%E5%BA%94%E7%AE%A1%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%92%8C%E7%89%B9%E6%80%A7%E5%8F%82%E6%95%B0/"/>
    <id>http://yoursite.com/2020/02/28/ch4-10%20MOS%E5%9C%BA%E6%95%88%E5%BA%94%E7%AE%A1%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%92%8C%E7%89%B9%E6%80%A7%E5%8F%82%E6%95%B0/</id>
    <published>2020-02-28T08:27:34.810Z</published>
    <updated>2020-03-10T08:33:50.825Z</updated>
    
    <content type="html"><![CDATA[<h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><ol><li>绝缘栅场效应管由金属-氧化物-半导体构成(Metal-Oxide-Semiconductor),简称MOSFET(MOS管)<a id="more"></a></li><li>结构：栅极有二氧化硅(绝缘材料)和下方衬底隔开，所以栅极电流为0，</li><li>MOS管类型和符号：<br><img src="https://img-blog.csdn.net/20180302103833471" alt=""><blockquote><p>注:MOS管分为增强型和耗尽型，右侧竖线表示导电沟道，平时增强型MOS管导电沟道断开，而耗尽型平时导电沟道是接通的。其中箭头表示PN结的方向.N沟道表示导电沟道为N型，P沟道同上。</p></blockquote><h4 id="导通原理"><a href="#导通原理" class="headerlink" title="导通原理"></a>导通原理</h4><h5 id="栅源电压Ugs对导电沟道的作用"><a href="#栅源电压Ugs对导电沟道的作用" class="headerlink" title="栅源电压Ugs对导电沟道的作用"></a>栅源电压Ugs对导电沟道的作用</h5>通常衬底和栅极连接，通过施加栅源电压，产生纵向的电子和空穴的移动，当形成导电沟道时的栅源电压称为开启电压，通过对栅源电压的控制可以使沟道均匀的变宽窄<h5 id="漏源电压Uds对导电沟道的控制作用"><a href="#漏源电压Uds对导电沟道的控制作用" class="headerlink" title="漏源电压Uds对导电沟道的控制作用"></a>漏源电压Uds对导电沟道的控制作用</h5>当栅源电压大于开启电压的时候(导电沟道形成)，改变漏源电压可以使沟道不均匀变窄，直到预夹断为止(预夹断还是在电场的作用下有电流产生)<h4 id="增强型MOS管伏安特性曲线"><a href="#增强型MOS管伏安特性曲线" class="headerlink" title="增强型MOS管伏安特性曲线"></a>增强型MOS管伏安特性曲线</h4>由于栅极无电流，所以无输入特性曲线<h5 id="输出特性曲线"><a href="#输出特性曲线" class="headerlink" title="输出特性曲线"></a>输出特性曲线</h5>$$I_D=f(Uds)$$<br>和结型场效应管相似，图的话看书吧<h5 id="转移特性"><a href="#转移特性" class="headerlink" title="转移特性"></a>转移特性</h5>$$I_D=f(Ugs)$$<br>i和Ugs成平方性的关系<blockquote><p>公式比较长，这里不写了，复习的话看书吧</p></blockquote></li></ol><p>平方率关系方程中，和沟道长宽，电子迁移速率，以及单位面积栅极电容有关系<br>(<strong><em>和电容有什么关系呢?</em></strong>)<br>Ugs越大，Rds越小(Ugs越大，沟道越宽，阻值自然就小了)</p><h4 id="耗尽型MOS管伏安特性曲线"><a href="#耗尽型MOS管伏安特性曲线" class="headerlink" title="耗尽型MOS管伏安特性曲线"></a>耗尽型MOS管伏安特性曲线</h4><p>由于耗尽型MOS管本身就有导电沟道，所以，有反向电压截止，其他区别不大，复习看书吧<br>(输出特性+转移特性)</p><p>2020.3.10</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;绝缘栅场效应管由金属-氧化物-半导体构成(Metal-Oxide-Semiconductor),简称MOSFET(MOS管)
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-9 结型场效应管的工作原理和特性参数</title>
    <link href="http://yoursite.com/2020/02/28/ch4-9%20%E7%BB%93%E5%9E%8B%E5%9C%BA%E6%95%88%E5%BA%94%E7%AE%A1%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%92%8C%E7%89%B9%E6%80%A7%E5%8F%82%E6%95%B0/"/>
    <id>http://yoursite.com/2020/02/28/ch4-9%20%E7%BB%93%E5%9E%8B%E5%9C%BA%E6%95%88%E5%BA%94%E7%AE%A1%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%92%8C%E7%89%B9%E6%80%A7%E5%8F%82%E6%95%B0/</id>
    <published>2020-02-28T08:26:22.009Z</published>
    <updated>2020-03-10T08:33:54.664Z</updated>
    
    <content type="html"><![CDATA[<h3 id="JFET基本概念"><a href="#JFET基本概念" class="headerlink" title="JFET基本概念"></a>JFET基本概念</h3><ol><li>结型场效应管(Junction field effect tube —— JFET)，栅极(Grid)，漏极(Drain)，源极(Source)</li><li>结构：长方体两侧连接栅极，中间沟道连接漏极和源极<a id="more"></a></li><li>N沟道JFET：中间的沟道材料为N型材料，两侧是P型材料。符号上是G——&gt;S，箭头代表P——&gt;N(材料)</li><li>P沟道JFET：中间的沟道材料为P型材料，两侧是N型材料。符号上是G&lt;——S，箭头代表P——&gt;N(材料)<h4 id="JFET基本控制原理"><a href="#JFET基本控制原理" class="headerlink" title="JFET基本控制原理"></a>JFET基本控制原理</h4>$$I_D=\cfrac{Uds}{Rds}$$<br>$$Rds=\rho \cfrac{L}{S}$$<blockquote><p>沟道越长，阻力越大，横截面积越大，阻力越小。</p></blockquote></li></ol><p>PN结反偏，栅极无电流，栅极电压控制导电沟道宽度，漏极电流为多子漂移电流。比如D+，S-，电子从S——&gt;D，就是从D——&gt;S的电流了。</p><h4 id="JFET的工作原理"><a href="#JFET的工作原理" class="headerlink" title="JFET的工作原理"></a>JFET的工作原理</h4><h5 id="栅源电压对导电沟道的控制"><a href="#栅源电压对导电沟道的控制" class="headerlink" title="栅源电压对导电沟道的控制"></a>栅源电压对导电沟道的控制</h5><p>栅源电压控制中间沟道的宽窄，当沟道宽度为0时，称为( <strong><em>夹断电压</em></strong> )</p><blockquote><p>比如说，栅源电压反相增大，PN结变宽，那沟道就变窄了</p></blockquote><h5 id="漏源电压对导电沟道的控制"><a href="#漏源电压对导电沟道的控制" class="headerlink" title="漏源电压对导电沟道的控制"></a>漏源电压对导电沟道的控制</h5><p>当漏源电压增大会改变沟道靠近正端的宽窄(因为电压高的话，反偏程度更大一点)，当沟道在漏端被夹断的时候称为( <strong><em>预夹断</em></strong> )</p><blockquote><p>预夹断之后还是可以导电，因为沟道还导通大部分，在最后一小部分，尽管被夹断，但是由于电场的作用电子还是会流通。</p></blockquote><h3 id="JFET的特性曲线"><a href="#JFET的特性曲线" class="headerlink" title="JFET的特性曲线"></a>JFET的特性曲线</h3><p>以共源极为例，由于PN结反偏，所以栅极输入端无电流，无输入特性曲线</p><h4 id="输出特性"><a href="#输出特性" class="headerlink" title="输出特性"></a>输出特性</h4><p>$$Id=F(Uds)$$</p><p><img src="https://www.dgzj.com/uploads/allimg/180320/1ZJ423L-1.jpg" alt=""></p><blockquote><p>nice啊，这张图看的很清楚</p></blockquote><h5 id="恒流区"><a href="#恒流区" class="headerlink" title="恒流区"></a>恒流区</h5><ol><li>条件：<br>$$Ugs&gt;Ugsoff$$<br>$$Uds&gt;=Ugs - Ugsoff$$<blockquote><p>就是使沟道处在预夹断状态和但没有完全夹断</p></blockquote></li><li>特点：<br>$Uds$增大时，$i_D$也会变大一点，因为长度变小，阻值变小，所以电流变大一点，因为增大很小所以可以看做恒流。<h5 id="可变电阻区"><a href="#可变电阻区" class="headerlink" title="可变电阻区"></a>可变电阻区</h5></li><li>条件：<br>$$Ugs&gt;Ugsoff$$<br>$$Uds&lt;Ugs - Ugsoff$$</li><li>特点<blockquote><p>处于预夹断状态之前，Ugs增大，电流id会近似线性增大，变化的斜率受到栅源电压的控制，等效为漏极和源极直接接了一个电阻</p></blockquote></li></ol><blockquote><p>实际上这个电阻是沟道的阻值</p></blockquote><h5 id="截止区"><a href="#截止区" class="headerlink" title="截止区"></a>截止区</h5><ol><li>条件：<br>$$Ugs&lt;=Ugsoff$$ <blockquote><p>就是使沟道完全夹断</p></blockquote></li><li>特点：<blockquote><p>沟道夹断，漏极和源极之间断开</p></blockquote><h5 id="击穿区"><a href="#击穿区" class="headerlink" title="击穿区"></a>击穿区</h5>$Uds$大到一定程度就会击穿，实际中要避免这个情况<h4 id="转移特性"><a href="#转移特性" class="headerlink" title="转移特性"></a>转移特性</h4>$$Id=F(Ugs)$$</li></ol><blockquote><p>和上图相似，两者成平方性的关系</p></blockquote><p>2020.3.6</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;JFET基本概念&quot;&gt;&lt;a href=&quot;#JFET基本概念&quot; class=&quot;headerlink&quot; title=&quot;JFET基本概念&quot;&gt;&lt;/a&gt;JFET基本概念&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;结型场效应管(Junction field effect tube —— JFET)，栅极(Grid)，漏极(Drain)，源极(Source)&lt;/li&gt;
&lt;li&gt;结构：长方体两侧连接栅极，中间沟道连接漏极和源极
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-8 双极型晶体管极限参数和工作状态判别举例</title>
    <link href="http://yoursite.com/2020/02/27/ch4-8%20%E5%8F%8C%E6%9E%81%E5%9E%8B%E6%99%B6%E4%BD%93%E7%AE%A1%E6%9E%81%E9%99%90%E5%8F%82%E6%95%B0%E5%92%8C%E5%B7%A5%E4%BD%9C%E7%8A%B6%E6%80%81%E5%88%A4%E5%88%AB%E4%B8%BE%E4%BE%8B/"/>
    <id>http://yoursite.com/2020/02/27/ch4-8%20%E5%8F%8C%E6%9E%81%E5%9E%8B%E6%99%B6%E4%BD%93%E7%AE%A1%E6%9E%81%E9%99%90%E5%8F%82%E6%95%B0%E5%92%8C%E5%B7%A5%E4%BD%9C%E7%8A%B6%E6%80%81%E5%88%A4%E5%88%AB%E4%B8%BE%E4%BE%8B/</id>
    <published>2020-02-27T07:50:32.149Z</published>
    <updated>2020-02-28T08:25:27.993Z</updated>
    
    <content type="html"><![CDATA[<h4 id="晶体管的极限参数"><a href="#晶体管的极限参数" class="headerlink" title="晶体管的极限参数"></a>晶体管的极限参数</h4><ol><li>击穿电压：因为有三个极，所以有三个反向击穿电压。</li><li>集电极最大允许电流$Icm$：随着$I_C$的增大，$\beta$会减小，当$\beta_0=\cfrac{2}{3}\beta$时候的电流称作$Icm$(最大允许电流)，集电极电流超过此值的时候，放大倍数会减小(因为$\beta$明显减小)<a id="more"></a></li><li>集电极最大允许耗散功率$Pcm$:当管子超过该功率的时候，会性能下降或者被烧坏<br>$$P_C=I_C*Uce$$</li><li>在上述的三个限制条件下，即是晶体三极管的安全工作区，使用管子的时候，尽量在此安全工作区下使用<h4 id="温度对晶体管参数的影响"><a href="#温度对晶体管参数的影响" class="headerlink" title="温度对晶体管参数的影响"></a>温度对晶体管参数的影响</h4>温度升高，$Ube$减小，$Icbo$增大，$\beta$增大，在输出特性曲线表示为，温度升高，曲线上移的间隔变大<h4 id="典型题目"><a href="#典型题目" class="headerlink" title="典型题目"></a>典型题目</h4></li><li>判断晶体管工作状态<blockquote><p>看两极是正偏还是反偏</p></blockquote></li><li>晶体管在放大状态下，判别管型和电机<blockquote><p>规律：<strong>NPN管：电流从e极流出，从b,c极流入；PNP管：电流从e极流入，从b,c极流出</strong></p></blockquote></li><li>根据晶体管的电位判别电极，管型和材料<blockquote><p>规律：<strong>e结电压为0.7V–硅管，0.3V–锗管；c极电位最高，e极电位最低，则为NPN管；e极电位最高，c极电位最低，则为PNP管；</strong></p></blockquote></li></ol><p>2020.2.28</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;晶体管的极限参数&quot;&gt;&lt;a href=&quot;#晶体管的极限参数&quot; class=&quot;headerlink&quot; title=&quot;晶体管的极限参数&quot;&gt;&lt;/a&gt;晶体管的极限参数&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;击穿电压：因为有三个极，所以有三个反向击穿电压。&lt;/li&gt;
&lt;li&gt;集电极最大允许电流$Icm$：随着$I_C$的增大，$\beta$会减小，当$\beta_0=\cfrac{2}{3}\beta$时候的电流称作$Icm$(最大允许电流)，集电极电流超过此值的时候，放大倍数会减小(因为$\beta$明显减小)
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-7 双极型晶体管特性曲线</title>
    <link href="http://yoursite.com/2020/02/27/ch4-7%20%E5%8F%8C%E6%9E%81%E5%9E%8B%E6%99%B6%E4%BD%93%E7%AE%A1%E7%89%B9%E6%80%A7%E6%9B%B2%E7%BA%BF/"/>
    <id>http://yoursite.com/2020/02/27/ch4-7%20%E5%8F%8C%E6%9E%81%E5%9E%8B%E6%99%B6%E4%BD%93%E7%AE%A1%E7%89%B9%E6%80%A7%E6%9B%B2%E7%BA%BF/</id>
    <published>2020-02-27T07:49:34.709Z</published>
    <updated>2020-02-28T06:30:34.428Z</updated>
    
    <content type="html"><![CDATA[<h4 id="共发射极输出特性曲线–输出电流和输出电压的关系"><a href="#共发射极输出特性曲线–输出电流和输出电压的关系" class="headerlink" title="共发射极输出特性曲线–输出电流和输出电压的关系"></a>共发射极输出特性曲线–输出电流和输出电压的关系</h4><a id="more"></a><p><img src="https://player.slidesplayer.com/101/17390235/slides/slide_2.jpg" alt=""></p><blockquote><p>如上图所示，输出特性曲线分成，放大区，截止区，饱和区。</p></blockquote><h5 id="放大区"><a href="#放大区" class="headerlink" title="放大区"></a>放大区</h5><blockquote><p>条件：e结正偏( $I_B&gt;0$ )，c结反偏（ $Uce&gt;=Ube$ ）</p></blockquote><p>特点：</p><ol><li>由图可见$I_B$对$I_C$有很强的控制力，当 $I_B$ 变化一点的时候，$I_C$就会有比较大的变化量,因此我们定义了共发射极交流电流放大倍数：<br>$$\beta=\cfrac{\Delta I_C}{\Delta I_B}$$<blockquote><p>当$Uce$为常数时候，上式成立。在特性曲线上反映是两条不同的$I_B$之间的间隔</p></blockquote></li><li>$Uce$ 对 $I_C$ 的影响很小，<strong><em>$I_C$略有上升，末尾不是平的，还是向上倾斜的。</em></strong><blockquote><p>原因：基区宽度调制效应，当 $Uce$ 变大后，集电区PN结的内电场变宽，所以基区变窄，基区复合的电流会少一点，但是由于 $I_B$ 不变，复合的电流很少</p></blockquote><h5 id="饱和区"><a href="#饱和区" class="headerlink" title="饱和区"></a>饱和区</h5><blockquote><p>条件：e结正偏，c结正偏，( Uce&lt; Ube) 即临界饱和线的左侧</p></blockquote></li></ol><p>特点：</p><ol><li>$I_C$不受到$I_B$的控制</li><li>$Uce$一定而$I_B$增大时，$I_C$基本不变，因此$\beta$趋近于0</li><li>$I_B$一定时，$I_C$的数值比放大时小<h5 id="截止区"><a href="#截止区" class="headerlink" title="截止区"></a>截止区</h5><blockquote><p>条件：e结和c结均处于反偏</p></blockquote></li></ol><p>特点：</p><ol><li>三个电极上的电流均为反向电流，即极间开路<blockquote><p>当$I_B=0$时，$I_C=Iceo$，小功率管$Iceo$很小，可以视$I_C=0$，当大功率管时，$Iceo$很大，则必须保证e结反偏</p></blockquote><h4 id="共发射极输入特性曲线–输入电流和输入电压的关系"><a href="#共发射极输入特性曲线–输入电流和输入电压的关系" class="headerlink" title="共发射极输入特性曲线–输入电流和输入电压的关系"></a>共发射极输入特性曲线–输入电流和输入电压的关系</h4><blockquote><p>随着$Uce$的增大，$I_B$也增大，类似二极管特性曲线，当$Ube&lt;0$时，晶体管截止，超过某值，e结也会反向击穿。</p></blockquote></li></ol><blockquote><p>同样有死区电压，硅管：$Ube:$ 0.6–0.7V,锗管$Ube:$ 0.1–0.3V</p></blockquote><h4 id="转移特性（输出电流-I-C-和输入电压-Ube-的关系）"><a href="#转移特性（输出电流-I-C-和输入电压-Ube-的关系）" class="headerlink" title="转移特性（输出电流$I_C$和输入电压$Ube$的关系）"></a>转移特性（输出电流$I_C$和输入电压$Ube$的关系）</h4><p><img src="https://player.slidesplayer.com/101/17390235/slides/slide_8.jpg" alt=""></p><p>2020.2.28</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;共发射极输出特性曲线–输出电流和输出电压的关系&quot;&gt;&lt;a href=&quot;#共发射极输出特性曲线–输出电流和输出电压的关系&quot; class=&quot;headerlink&quot; title=&quot;共发射极输出特性曲线–输出电流和输出电压的关系&quot;&gt;&lt;/a&gt;共发射极输出特性曲线–输出电流和输出电压的关系&lt;/h4&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ResNet-18</title>
    <link href="http://yoursite.com/2020/02/27/ResNet-18/"/>
    <id>http://yoursite.com/2020/02/27/ResNet-18/</id>
    <published>2020-02-27T03:54:50.026Z</published>
    <updated>2020-02-27T07:33:34.937Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p><strong>Deep Residual Learning for Image Recognition</strong></p><p>ResNet是何凯明（微软亚洲AI研究院工作）提出的残差神经网络，曾经在Kaggle等平台上获得多次大奖。</p><a id="more"></a><h3 id="为什么提出ResNet"><a href="#为什么提出ResNet" class="headerlink" title="为什么提出ResNet"></a>为什么提出ResNet</h3><p>众所周知，随着神经网络的发展，深度越大，网络的表达性能就越好，可实际训练的时候，随着深度的加大，网络出现了梯度弥散（也有叫梯度消失等）的情况。</p><blockquote><p>比如说在原始的网络当中，输入变量每经过一层就通过一次sigmoid激活函数，由于sigmoid函数只在0附近梯度变化明显，远离0附近梯度变化趋近于0，因此随着网络的深化，梯度变化趋于0，相当于线性恒等映射，深化的网络是做了无用功。</p></blockquote><p>为了解决该问题，人们想了一些办法，比如说改变激活函数使用relu，Leaky—relu，或者Batch Normalization等，但是不能从根本上解决问题，因此何凯明提出了ResNet（残差神经网络）</p><p>其他参考</p><blockquote><p>虽然通过Batch Normalization或者正则初始化等能够训练了，但是又会出现另一个问题，就是<strong>退化问题</strong>，网络层数增加，但是在训练集上的准确率却饱和甚至下降了。<br>退化问题说明了深度网络不能很简单地被很好地优化</p></blockquote><h3 id="ResNet是什么？怎么解决梯度弥散以及退化问题？"><a href="#ResNet是什么？怎么解决梯度弥散以及退化问题？" class="headerlink" title="ResNet是什么？怎么解决梯度弥散以及退化问题？"></a>ResNet是什么？怎么解决梯度弥散以及退化问题？</h3><h4 id="两种mapping"><a href="#两种mapping" class="headerlink" title="两种mapping"></a>两种mapping</h4><p>何凯明在ResNet中提出两种mapping：</p><ol><li>identity mapping(处理图像中也称叫feature map)：就是本身，即下图中的x</li><li>residual mapping：指的公式中的$F(x)$ <h4 id="残差函数"><a href="#残差函数" class="headerlink" title="残差函数"></a>残差函数</h4>ResNet中通过学习残差函数来解决问题，学习差值比学习梯度变化要容易的多,摘录知乎解释：<blockquote><p>F是求和前网络映射，H是从输入到求和后的网络映射。比如把5映射到5.1，那么引入残差前是F’(5)=5.1，引入残差后是H(5)=5.1, H(5)=F(5)+5, F(5)=0.1。这里的F’和F都表示网络参数映射，引入残差后的映射对输出的变化更敏感。比如s输出从5.1变到5.2，映射F’的输出增加了1/51=2%，而对于残差结构输出从5.1到5.2，映射F是从0.1到0.2，增加了100%。明显后者输出变化对权重的调整作用更大，所以效果更好。残差的思想都是去掉相同的主体部分，从而突出微小的变化，看到残差网络我第一反应就是差分放大器…（博主：哈哈，刚开始接触ResNet的时候我还没学到差分放大器）</p></blockquote></li></ol><p>上述中的$H(x)=F(x)+x$，$F(x)$为残差函数，如果$F(x)=0$，则为恒等映射，这样设计网络可以保证随着深度的加大，不论怎么训练，至少层数更深的网络训练的效果不会比层数浅的网络效果差，网络会一直处于最优状态(理论上)，且残差拟合更加容易，学习速度更快。</p><h4 id="Residual-Block"><a href="#Residual-Block" class="headerlink" title="Residual Block"></a>Residual Block</h4><p><img src="https://upload-images.jianshu.io/upload_images/6095626-49ac0caeb5525b93.png" alt=""></p><p>上图为Residual Block，可以看到输入变量x，通过两层网络和x（通过shortcut）进行element-wise add（就是对应元素加到一起，element-wise是对应元素相乘），然后再经过一个relu输出就是一个Residual Block。</p><p><img src="https://upload-images.jianshu.io/upload_images/6095626-287fc59a3cd86488.png" alt=""></p><p>这两种结构常用在ResNet18，ResNet34(左图)，ResNet50/101/152(右图)，其中右图又被称作”bottleneck”</p><h3 id="ResNet-18-Cifar-10"><a href="#ResNet-18-Cifar-10" class="headerlink" title="ResNet-18 Cifar-10"></a>ResNet-18 Cifar-10</h3><h4 id="Cifar-10"><a href="#Cifar-10" class="headerlink" title="Cifar-10"></a>Cifar-10</h4><p>这个数据集包含60000张32*32的彩色图片，这些图片一共被分成10类，有小猫，小狗等……详见：<a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></p><h4 id="ResNet-18网络结构"><a href="#ResNet-18网络结构" class="headerlink" title="ResNet-18网络结构"></a>ResNet-18网络结构</h4><p><img src="https://img-blog.csdn.net/20180426215052446?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1bnFpYW5kZTg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><blockquote><p>上图中虚线表示channel改变，实线表示channel不变</p></blockquote><h4 id="实现代码1"><a href="#实现代码1" class="headerlink" title="实现代码1"></a>实现代码1</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  torch</span><br><span class="line"><span class="keyword">from</span>    torch <span class="keyword">import</span>  nn</span><br><span class="line"><span class="keyword">from</span>    torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span>    torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span>    torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span>    torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span>    torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># from    torchvision.models import resnet18</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlk</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    resnet block</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ch_in, ch_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param ch_in:</span></span><br><span class="line"><span class="string">        :param ch_out:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ResBlk, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(ch_out)</span><br><span class="line">        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(ch_out)</span><br><span class="line"></span><br><span class="line">        self.extra = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> ch_out != ch_in:</span><br><span class="line">            <span class="comment"># [b, ch_in, h, w] =&gt; [b, ch_out, h, w]</span></span><br><span class="line">            self.extra = nn.Sequential(</span><br><span class="line">                nn.Conv2d(ch_in, ch_out, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>),<span class="comment">#既然维度不一样，为什么用卷积而不用别的呢？</span></span><br><span class="line">                nn.BatchNorm2d(ch_out)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param x: [b, ch, h, w]</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        out = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        out = self.bn2(self.conv2(out))</span><br><span class="line">        <span class="comment"># short cut.</span></span><br><span class="line">        <span class="comment"># extra module: [b, ch_in, h, w] =&gt; [b, ch_out, h, w]</span></span><br><span class="line">        <span class="comment"># element-wise add:</span></span><br><span class="line">        out = self.extra(x) + out  </span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet18</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ResNet18, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># followed 4 blocks</span></span><br><span class="line">        <span class="comment"># [b, 64, h, w] =&gt; [b, 128, h ,w]</span></span><br><span class="line">        self.blk1 = ResBlk(<span class="number">16</span>, <span class="number">16</span>)</span><br><span class="line">        <span class="comment"># [b, 128, h, w] =&gt; [b, 256, h, w]</span></span><br><span class="line">        self.blk2 = ResBlk(<span class="number">16</span>, <span class="number">32</span>)</span><br><span class="line">        <span class="comment"># # [b, 256, h, w] =&gt; [b, 512, h, w]</span></span><br><span class="line">        <span class="comment"># self.blk3 = ResBlk(128, 256)</span></span><br><span class="line">        <span class="comment"># # [b, 512, h, w] =&gt; [b, 1024, h, w]</span></span><br><span class="line">        <span class="comment"># self.blk4 = ResBlk(256, 512)</span></span><br><span class="line"></span><br><span class="line">        self.outlayer = nn.Linear(<span class="number">32</span>*<span class="number">32</span>*<span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param x:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [b, 64, h, w] =&gt; [b, 1024, h, w]</span></span><br><span class="line">        x = self.blk1(x)</span><br><span class="line">        x = self.blk2(x)</span><br><span class="line">        <span class="comment"># x = self.blk3(x)</span></span><br><span class="line">        <span class="comment"># x = self.blk4(x)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        x = self.outlayer(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    batchsz = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">    cifar_train = datasets.CIFAR10(<span class="string">'cifar'</span>, <span class="literal">True</span>, transform=transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">    ]), download=<span class="literal">True</span>)</span><br><span class="line">    cifar_train = DataLoader(cifar_train, batch_size=batchsz, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    cifar_test = datasets.CIFAR10(<span class="string">'cifar'</span>, <span class="literal">False</span>, transform=transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">    ]), download=<span class="literal">True</span>)</span><br><span class="line">    cifar_test = DataLoader(cifar_test, batch_size=batchsz, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    x, label = iter(cifar_train).next()</span><br><span class="line">    print(<span class="string">'x:'</span>, x.shape, <span class="string">'label:'</span>, label.shape)</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">'cuda'</span>)</span><br><span class="line">    <span class="comment"># model = Lenet5().to(device)</span></span><br><span class="line">    model = ResNet18().to(device)</span><br><span class="line"></span><br><span class="line">    criteon = nn.CrossEntropyLoss().to(device)</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">    print(model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line"></span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> batchidx, (x, label) <span class="keyword">in</span> enumerate(cifar_train):</span><br><span class="line">            <span class="comment"># [b, 3, 32, 32]</span></span><br><span class="line">            <span class="comment"># [b]</span></span><br><span class="line">            x, label = x.to(device), label.to(device)</span><br><span class="line"></span><br><span class="line">            logits = model(x)</span><br><span class="line">            <span class="comment"># logits: [b, 10]</span></span><br><span class="line">            <span class="comment"># label:  [b]</span></span><br><span class="line">            <span class="comment"># loss: tensor scalar</span></span><br><span class="line">            loss = criteon(logits, label)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># backprop</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        print(epoch, <span class="string">'loss:'</span>, loss.item())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        model.eval()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># test</span></span><br><span class="line">            total_correct = <span class="number">0</span></span><br><span class="line">            total_num = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> x, label <span class="keyword">in</span> cifar_test:</span><br><span class="line">                <span class="comment"># [b, 3, 32, 32]</span></span><br><span class="line">                <span class="comment"># [b]</span></span><br><span class="line">                x, label = x.to(device), label.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># [b, 10]</span></span><br><span class="line">                logits = model(x)</span><br><span class="line">                <span class="comment"># [b]</span></span><br><span class="line">                pred = logits.argmax(dim=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># [b] vs [b] =&gt; scalar tensor</span></span><br><span class="line">                correct = torch.eq(pred, label).float().sum().item()</span><br><span class="line">                total_correct += correct</span><br><span class="line">                total_num += x.size(<span class="number">0</span>)</span><br><span class="line">                <span class="comment"># print(correct)</span></span><br><span class="line"></span><br><span class="line">            acc = total_correct / total_num</span><br><span class="line">            print(epoch, <span class="string">'acc:'</span>, acc)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><ol><li>该代码没有完整实现ResNet-18结构，只实现了两residual Block。后面我自己会补上</li><li>初学ResNet，这个代码还是很OK的。<h4 id="实现代码2"><a href="#实现代码2" class="headerlink" title="实现代码2"></a>实现代码2</h4><blockquote><p>注：这段代码摘录于CSDN，由作者所说，acc = 95.170%，是完整实现ResNet-18，且封装性优于上述代码，参考价值很高</p></blockquote><h5 id="Pytorch上搭建ResNet-18："><a href="#Pytorch上搭建ResNet-18：" class="headerlink" title="Pytorch上搭建ResNet-18："></a>Pytorch上搭建ResNet-18：</h5><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''ResNet-18 Image classfication for cifar-10 with PyTorch </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Author 'Sun-qian'.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inchannel, outchannel, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(ResidualBlock, self).__init__()</span><br><span class="line">        self.left = nn.Sequential(</span><br><span class="line">            nn.Conv2d(inchannel, outchannel, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outchannel),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(outchannel, outchannel, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outchannel)</span><br><span class="line">        )</span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> inchannel != outchannel:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(inchannel, outchannel, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(outchannel)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.left(x)</span><br><span class="line">        out += self.shortcut(x)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ResidualBlock, num_classes=<span class="number">10</span>)</span>:</span></span><br><span class="line">        super(ResNet, self).__init__()</span><br><span class="line">        self.inchannel = <span class="number">64</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line">        self.layer1 = self.make_layer(ResidualBlock, <span class="number">64</span>,  <span class="number">2</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.layer2 = self.make_layer(ResidualBlock, <span class="number">128</span>, <span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self.make_layer(ResidualBlock, <span class="number">256</span>, <span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self.make_layer(ResidualBlock, <span class="number">512</span>, <span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_layer</span><span class="params">(self, block, channels, num_blocks, stride)</span>:</span></span><br><span class="line">        strides = [stride] + [<span class="number">1</span>] * (num_blocks - <span class="number">1</span>)   <span class="comment">#strides=[1,1]</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> stride <span class="keyword">in</span> strides:</span><br><span class="line">            layers.append(block(self.inchannel, channels, stride))</span><br><span class="line">            self.inchannel = channels</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.layer1(out)</span><br><span class="line">        out = self.layer2(out)</span><br><span class="line">        out = self.layer3(out)</span><br><span class="line">        out = self.layer4(out)</span><br><span class="line">        out = F.avg_pool2d(out, <span class="number">4</span>)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet18</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ResNet(ResidualBlock)</span><br></pre></td></tr></table></figure><h5 id="Pytorch上训练："><a href="#Pytorch上训练：" class="headerlink" title="Pytorch上训练："></a>Pytorch上训练：</h5><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> resnet <span class="keyword">import</span> ResNet18</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义是否使用GPU</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数设置,使得我们能够手动输入命令行参数，就是让风格变得和Linux命令行差不多</span></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'PyTorch CIFAR10 Training'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--outf'</span>, default=<span class="string">'./model/'</span>, help=<span class="string">'folder to output images and model checkpoints'</span>) <span class="comment">#输出结果保存路径</span></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数设置</span></span><br><span class="line">EPOCH = <span class="number">135</span>   <span class="comment">#遍历数据集次数</span></span><br><span class="line">pre_epoch = <span class="number">0</span>  <span class="comment"># 定义已经遍历数据集的次数</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span>      <span class="comment">#批处理尺寸(batch_size)</span></span><br><span class="line">LR = <span class="number">0.01</span>        <span class="comment">#学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集并预处理</span></span><br><span class="line">transform_train = transforms.Compose([</span><br><span class="line">    transforms.RandomCrop(<span class="number">32</span>, padding=<span class="number">4</span>),  <span class="comment">#先四周填充0，在吧图像随机裁剪成32*32</span></span><br><span class="line">    transforms.RandomHorizontalFlip(),  <span class="comment">#图像一半的概率翻转，一半的概率不翻转</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>)), <span class="comment">#R,G,B每层的归一化用到的均值和方差</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">transform_test = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform_train) <span class="comment">#训练数据集</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)   <span class="comment">#生成一个个batch进行批训练，组成batch的时候顺序打乱取</span></span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform_test)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">100</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Cifar-10的标签</span></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>, <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型定义-ResNet</span></span><br><span class="line">net = ResNet18().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化方式</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment">#损失函数为交叉熵，多用于多分类问题</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=LR, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">5e-4</span>) <span class="comment">#优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(args.outf):</span><br><span class="line">os.makedirs(args.outf)</span><br><span class="line">    best_acc = <span class="number">85</span>  <span class="comment">#2 初始化best test accuracy</span></span><br><span class="line">    print(<span class="string">"Start Training, Resnet-18!"</span>)  <span class="comment"># 定义遍历数据集的次数</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"acc.txt"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">"log.txt"</span>, <span class="string">"w"</span>)<span class="keyword">as</span> f2:</span><br><span class="line">            <span class="keyword">for</span> epoch <span class="keyword">in</span> range(pre_epoch, EPOCH):</span><br><span class="line">                print(<span class="string">'\nEpoch: %d'</span> % (epoch + <span class="number">1</span>))</span><br><span class="line">                net.train()</span><br><span class="line">                sum_loss = <span class="number">0.0</span></span><br><span class="line">                correct = <span class="number">0.0</span></span><br><span class="line">                total = <span class="number">0.0</span></span><br><span class="line">                <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">                    <span class="comment"># 准备数据</span></span><br><span class="line">                    length = len(trainloader)</span><br><span class="line">                    inputs, labels = data</span><br><span class="line">                    inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">                    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># forward + backward</span></span><br><span class="line">                    outputs = net(inputs)</span><br><span class="line">                    loss = criterion(outputs, labels)</span><br><span class="line">                    loss.backward()</span><br><span class="line">                    optimizer.step()</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 每训练1个batch打印一次loss和准确率</span></span><br><span class="line">                    sum_loss += loss.item()</span><br><span class="line">                    _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">                    total += labels.size(<span class="number">0</span>)</span><br><span class="line">                    correct += predicted.eq(labels.data).cpu().sum()</span><br><span class="line">                    print(<span class="string">'[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% '</span></span><br><span class="line">                          % (epoch + <span class="number">1</span>, (i + <span class="number">1</span> + epoch * length), sum_loss / (i + <span class="number">1</span>), <span class="number">100.</span> * correct / total))</span><br><span class="line">                    f2.write(<span class="string">'%03d  %05d |Loss: %.03f | Acc: %.3f%% '</span></span><br><span class="line">                          % (epoch + <span class="number">1</span>, (i + <span class="number">1</span> + epoch * length), sum_loss / (i + <span class="number">1</span>), <span class="number">100.</span> * correct / total))</span><br><span class="line">                    f2.write(<span class="string">'\n'</span>)</span><br><span class="line">                    f2.flush()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 每训练完一个epoch测试一下准确率</span></span><br><span class="line">                print(<span class="string">"Waiting Test!"</span>)</span><br><span class="line">                <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                    correct = <span class="number">0</span></span><br><span class="line">                    total = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">                        net.eval()</span><br><span class="line">                        images, labels = data</span><br><span class="line">                        images, labels = images.to(device), labels.to(device)</span><br><span class="line">                        outputs = net(images)</span><br><span class="line">                        <span class="comment"># 取得分最高的那个类 (outputs.data的索引号)</span></span><br><span class="line">                        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">                        total += labels.size(<span class="number">0</span>)</span><br><span class="line">                        correct += (predicted == labels).sum()</span><br><span class="line">                    print(<span class="string">'测试分类准确率为：%.3f%%'</span> % (<span class="number">100</span> * correct / total))</span><br><span class="line">                    acc = <span class="number">100.</span> * correct / total</span><br><span class="line">                    <span class="comment"># 将每次测试结果实时写入acc.txt文件中</span></span><br><span class="line">                    print(<span class="string">'Saving model......'</span>)</span><br><span class="line">                    torch.save(net.state_dict(), <span class="string">'%s/net_%03d.pth'</span> % (args.outf, epoch + <span class="number">1</span>))</span><br><span class="line">                    f.write(<span class="string">"EPOCH=%03d,Accuracy= %.3f%%"</span> % (epoch + <span class="number">1</span>, acc))</span><br><span class="line">                    f.write(<span class="string">'\n'</span>)</span><br><span class="line">                    f.flush()</span><br><span class="line">                    <span class="comment"># 记录最佳测试分类准确率并写入best_acc.txt文件中</span></span><br><span class="line">                    <span class="keyword">if</span> acc &gt; best_acc:</span><br><span class="line">                        f3 = open(<span class="string">"best_acc.txt"</span>, <span class="string">"w"</span>)</span><br><span class="line">                        f3.write(<span class="string">"EPOCH=%d,best_acc= %.3f%%"</span> % (epoch + <span class="number">1</span>, acc))</span><br><span class="line">                        f3.close()</span><br><span class="line">                        best_acc = acc</span><br><span class="line">            print(<span class="string">"Training Finished, TotalEPOCH=%d"</span> % EPOCH)</span><br></pre></td></tr></table></figure><h5 id="实现效果"><a href="#实现效果" class="headerlink" title="实现效果"></a>实现效果</h5><img src="https://img-blog.csdn.net/20180426220936286?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1bnFpYW5kZTg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></li></ol><blockquote><p>注：该图像是作者将数据下载到.txt文件，然后再matlab中进行生成</p></blockquote><h5 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h5><ol><li>将数据导入.txt文件，用matlab处理<blockquote><p>确实是好方法，visdom用起来也会很方便</p></blockquote></li><li>定义GPU是否使用的写法</li><li>ResNet-18模块封装性很好，完全符合结构<h5 id="困惑"><a href="#困惑" class="headerlink" title="困惑"></a>困惑</h5>在make_layer那里最后一句的return nn.Sequential(<em>layers)中的</em>layers是什么意思呢，上面加*是什么意思呢？<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3></li><li><a href="https://blog.csdn.net/sunqiande88/article/details/80100891">https://blog.csdn.net/sunqiande88/article/details/80100891</a></li><li><a href="http://www.jeepxie.net/article/601129.html">http://www.jeepxie.net/article/601129.html</a></li><li><a href="https://www.jianshu.com/p/e58437f39f65">https://www.jianshu.com/p/e58437f39f65</a></li><li><a href="https://www.zhihu.com/question/53224378/answer/159102095">https://www.zhihu.com/question/53224378/answer/159102095</a></li><li>Cifar-10：<a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Deep Residual Learning for Image Recognition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ResNet是何凯明（微软亚洲AI研究院工作）提出的残差神经网络，曾经在Kaggle等平台上获得多次大奖。&lt;/p&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-6 双极型晶体三管工作原理</title>
    <link href="http://yoursite.com/2020/02/26/ch4-6%20%E5%8F%8C%E6%9E%81%E5%9E%8B%E6%99%B6%E4%BD%93%E4%B8%89%E7%AE%A1%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2020/02/26/ch4-6%20%E5%8F%8C%E6%9E%81%E5%9E%8B%E6%99%B6%E4%BD%93%E4%B8%89%E7%AE%A1%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/</id>
    <published>2020-02-26T04:44:16.054Z</published>
    <updated>2020-02-26T08:37:13.978Z</updated>
    
    <content type="html"><![CDATA[<h4 id="晶体管的结构以及符号"><a href="#晶体管的结构以及符号" class="headerlink" title="晶体管的结构以及符号"></a>晶体管的结构以及符号</h4><ol><li><p>结构：发射区（e，emitter）集电区（c，collector），基区（b，basic）</p><blockquote><p>发射区重掺杂，基区轻掺杂，集电区面积大</p></blockquote></li><li><p>符号：箭头：从P区指向N区</p><a id="more"></a><h4 id="晶体管工作原理"><a href="#晶体管工作原理" class="headerlink" title="晶体管工作原理"></a>晶体管工作原理</h4><p>看图比较好理解，这里我不放图了，简单叙述一下工作过程</p><blockquote><p>在晶体管在发射结正偏，集电结反偏的情况下：</p></blockquote></li><li><p>发射结电子因为发射结正偏，扩散运动——&gt;基区，其中少部分电子和基结的空穴复合</p></li><li><p>由于基区轻掺杂，扩散运动过来的自由电子只有少部分在基区和空穴复合</p></li><li><p>由于集电区反偏，形成强大电场，产生漂移运动，基区大部分电子漂移到集电区</p></li><li><p>因为集电区反偏，所以基区和集电区的少子相互漂移，形成反向饱和电流（很小，和温度关系很大，与之前PN结的情况是一样的）</p><blockquote><p>在晶体三极管中在基区的作用下，把电子从发射区几乎全部传到了集电区，这是放大功能的体现（<strong><em>这哪里体现了放大？？睁眼说瞎</em></strong>）</p></blockquote><h4 id="比例系数-beta-alpha"><a href="#比例系数-beta-alpha" class="headerlink" title="比例系数 $\beta \alpha$"></a>比例系数 $\beta \alpha$</h4></li><li><p>$\overline{\beta}$:共发射极直流电流放大系数。忽略集电区和基区的空穴和自由电子的漂移运动产生的电流（因为电流很小嘛）所以可以近似看作：<br>$$\overline{\beta}=\cfrac{I_C}{I_B}$$</p><blockquote><p>当$I_B=0$时，$I_C$中仍有电流，我们称为穿透电流（是之前忽略的一小部分来的）<br>$$\overline{\beta}=\cfrac{I_C}{I_B}$$<br>$$I_C=\overline{\beta}I_B$$<br>$$I_E=(1+\overline{\beta})I_B$$<br>上式公式牢记</p></blockquote></li><li><p>$\overline{\alpha}$:共基极直流电流放大系数。同样忽略集电区和基区的空穴和自由电子的漂移运动产生的电流，所以可以近似看作：<br>$$\overline{\alpha}=\cfrac{I_C}{I_E}$$</p></li><li><p>$\overline{\alpha}$和$\overline{\beta}$的关系<br>$$\overline{\beta}=\cfrac{\overline{\alpha}}{1-\overline{\alpha}}$$<br>$$\overline{\alpha}=\cfrac{\overline{\beta}}{1+\overline{\beta}}$$</p></li></ol><p>2020.2.26</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;晶体管的结构以及符号&quot;&gt;&lt;a href=&quot;#晶体管的结构以及符号&quot; class=&quot;headerlink&quot; title=&quot;晶体管的结构以及符号&quot;&gt;&lt;/a&gt;晶体管的结构以及符号&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;结构：发射区（e，emitter）集电区（c，collector），基区（b，basic）&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;发射区重掺杂，基区轻掺杂，集电区面积大&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;符号：箭头：从P区指向N区&lt;/p&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-5 稳压管以及其它二极管</title>
    <link href="http://yoursite.com/2020/02/26/ch4-5%20%E7%A8%B3%E5%8E%8B%E7%AE%A1%E4%BB%A5%E5%8F%8A%E5%85%B6%E5%AE%83%E4%BA%8C%E6%9E%81%E7%AE%A1/"/>
    <id>http://yoursite.com/2020/02/26/ch4-5%20%E7%A8%B3%E5%8E%8B%E7%AE%A1%E4%BB%A5%E5%8F%8A%E5%85%B6%E5%AE%83%E4%BA%8C%E6%9E%81%E7%AE%A1/</id>
    <published>2020-02-26T04:43:24.185Z</published>
    <updated>2020-02-26T06:17:46.060Z</updated>
    
    <content type="html"><![CDATA[<h4 id="稳压二极管"><a href="#稳压二极管" class="headerlink" title="稳压二极管"></a>稳压二极管</h4><ol><li>符号：在二级管的符号上加一个小弯</li><li>稳定电压（$U_Z$）：工作在反向击穿状态时的稳定电压即稳定电压</li><li>额定功耗（$P_Z$）：使用时功耗不超过这个标准，这个功耗由管子本身的属性决定</li><li>稳压电流（$I_Z$）：稳压二极管工作时候有最大电流和最小电流，在这个范围区间的就是稳压电流，小于该值，管子失去稳压作用，大于此值，管子会烧坏<a id="more"></a>$$I_Z=\cfrac{U_Z}{I_Z}$$</li><li>动态电阻（$r_Z$）：一般在几欧姆到十几欧姆<br>$$r_Z=\cfrac{\Delta U_Z}{\Delta I_Z}$$</li><li>温度系数：稳定电压较高（十几伏）一般为雪崩击穿，稳定电压在6伏以下，一般为齐纳击穿，所以稳定电压一般在（5–7V）的稳压管兼有两种击穿，温度稳定性比较好</li><li>有温度补偿的稳压管：将两稳压管相互对接即可，利用一个工作在正向导通（负温度系数），一个工作在反向导通（正温度系数）即可相互补偿，改善温度稳定性。<br>$$U=\pm(U_Z+U_D)$$<blockquote><p>其中$U_D$是正向导通电压</p></blockquote></li><li>如果计算带有稳压二极管的电路，首先要判断稳压二极管能否被击穿，方法是先假设稳压管断开，看 $U_O$ 是否大于 $U_Z$ .<blockquote><p>具体题目翻一下MOOC，这里不记录了</p></blockquote></li><li>稳压电路限流电阻R的选择：就是要保证电流在稳压电流区工作，公式比较长不写了，遇到具体问题再翻书补充</li><li>由稳压二极管构成的限幅电路（单向，双向）：和二极管构成的限幅电路是相似的，但是由于正向导通特性，单向限幅的时候，负端会有0.7V的电压（硅管）<h4 id="其他二极管"><a href="#其他二极管" class="headerlink" title="其他二极管"></a>其他二极管</h4></li><li>变容二极管：利用了二极管的电容特性，具体在高频时候用的比较多，遇到在做深入了解</li><li>光电二极管：被光线照射后产生一个电流</li><li>发光二极管：……</li><li>肖特基二极管：结构是金属和N型材料掺杂，特点是速度快，常用在高频，遇到再深入了解</li></ol><p>2020.2.26</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;稳压二极管&quot;&gt;&lt;a href=&quot;#稳压二极管&quot; class=&quot;headerlink&quot; title=&quot;稳压二极管&quot;&gt;&lt;/a&gt;稳压二极管&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;符号：在二级管的符号上加一个小弯&lt;/li&gt;
&lt;li&gt;稳定电压（$U_Z$）：工作在反向击穿状态时的稳定电压即稳定电压&lt;/li&gt;
&lt;li&gt;额定功耗（$P_Z$）：使用时功耗不超过这个标准，这个功耗由管子本身的属性决定&lt;/li&gt;
&lt;li&gt;稳压电流（$I_Z$）：稳压二极管工作时候有最大电流和最小电流，在这个范围区间的就是稳压电流，小于该值，管子失去稳压作用，大于此值，管子会烧坏
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-4 晶体二极管应用</title>
    <link href="http://yoursite.com/2020/02/25/ch4-4%20%E6%99%B6%E4%BD%93%E4%BA%8C%E6%9E%81%E7%AE%A1%E5%BA%94%E7%94%A8/"/>
    <id>http://yoursite.com/2020/02/25/ch4-4%20%E6%99%B6%E4%BD%93%E4%BA%8C%E6%9E%81%E7%AE%A1%E5%BA%94%E7%94%A8/</id>
    <published>2020-02-25T13:56:18.113Z</published>
    <updated>2020-02-26T04:40:54.035Z</updated>
    
    <content type="html"><![CDATA[<h4 id="整流电路"><a href="#整流电路" class="headerlink" title="整流电路"></a>整流电路</h4><blockquote><p>这里最好有电路图，但是懒得加了，咱们就脑子里回忆吧，哈哈哈</p></blockquote><a id="more"></a><ol><li>二极管加负载<blockquote><p>利用二极管单向导电性，如果输入电压大于0.7V，就输出电压=输入电压，如果输入电压小于0.7V，则二极管截止，输出电压为0.</p></blockquote></li><li>整流桥电路(又称绝对值电路)<br>$$U_o=|U_i|$$<h4 id="限幅电路"><a href="#限幅电路" class="headerlink" title="限幅电路"></a>限幅电路</h4></li><li>上限幅电路</li><li>双向限幅电路<blockquote><p>当输入电压大于二极管和电压源，Uo即二极管管压降和电压源压降之和，当输入电压小于二极管和电压源，$U_o=U_i$</p></blockquote></li></ol><p>2020.2.25</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;整流电路&quot;&gt;&lt;a href=&quot;#整流电路&quot; class=&quot;headerlink&quot; title=&quot;整流电路&quot;&gt;&lt;/a&gt;整流电路&lt;/h4&gt;&lt;blockquote&gt;
&lt;p&gt;这里最好有电路图，但是懒得加了，咱们就脑子里回忆吧，哈哈哈&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-3 晶体二极管的特性及参数</title>
    <link href="http://yoursite.com/2020/02/25/ch4-3%20%E6%99%B6%E4%BD%93%E4%BA%8C%E6%9E%81%E7%AE%A1%E7%9A%84%E7%89%B9%E6%80%A7%E5%8F%8A%E5%8F%82%E6%95%B0/"/>
    <id>http://yoursite.com/2020/02/25/ch4-3%20%E6%99%B6%E4%BD%93%E4%BA%8C%E6%9E%81%E7%AE%A1%E7%9A%84%E7%89%B9%E6%80%A7%E5%8F%8A%E5%8F%82%E6%95%B0/</id>
    <published>2020-02-25T13:55:27.788Z</published>
    <updated>2020-02-25T14:38:58.286Z</updated>
    
    <content type="html"><![CDATA[<h4 id="二极管的伏安特性–指数特性"><a href="#二极管的伏安特性–指数特性" class="headerlink" title="二极管的伏安特性–指数特性"></a>二极管的伏安特性–指数特性</h4><p>$$i_D=I_S(e^\cfrac{U_D}{U_T}-1)$$</p><blockquote><p>注：其中 $I_S$ 为反向饱和电流，$U_T$为热电压，$U_D$为导通电压，$i_D$为正向导通电流</p></blockquote><a id="more"></a><ol><li>正向特性：<blockquote><p>死区电压（门限电压）：正向加电压的过程中，一开始电流很小，然后突然指数级的增加。在电流很小的区间，电压及是死区电压。</p></blockquote></li></ol><blockquote><p>死区电压：室温（27°C）下，硅管：0.5-0.7V，锗管：0.1-0.3V</p></blockquote><p>管压降：工业上–硅管：0.7V，锗管–0.3V<br>2. 反向特性:<br>电流很小，硅管：一般小于0.1uA，锗管小于几十微安</p><h4 id="二极管参数"><a href="#二极管参数" class="headerlink" title="二极管参数"></a>二极管参数</h4><ol><li>直流电阻<br>$$R_D=\cfrac{U_D}{I_D}$$<blockquote><p>正向电阻越小，反向电阻越大，二极管的导电性能越好</p></blockquote></li><li>交流电阻<br>$$r_D=\cfrac{\Delta U_D}{\Delta i_D}$$<blockquote><p>$r_D &lt;&lt; R_D$</p></blockquote></li><li>温度特性<blockquote><p>温度T增大:<br>反向饱和电流增大，死区电压减小<br>雪崩击穿电压增大（温度增加）<br>齐纳击穿电压减小（温度减小）</p></blockquote></li><li>最大整流电流<blockquote><p>二极管允许通过的最大正向平均电流</p></blockquote></li><li>最大反向工作电压</li><li>反向电流<blockquote><p>越小越好，通常反向电流与温度密切相关</p></blockquote></li><li>最高工作频率<br>超过该工作频率，二极管单向导电性能变坏<h4 id="二级管电路模型"><a href="#二级管电路模型" class="headerlink" title="二级管电路模型"></a>二级管电路模型</h4>通常看做，P端正，N端负，硅管的管压降为0.7V，锗管的管压降为0.3V即可。</li></ol><p>2020.2.25</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;二极管的伏安特性–指数特性&quot;&gt;&lt;a href=&quot;#二极管的伏安特性–指数特性&quot; class=&quot;headerlink&quot; title=&quot;二极管的伏安特性–指数特性&quot;&gt;&lt;/a&gt;二极管的伏安特性–指数特性&lt;/h4&gt;&lt;p&gt;$$i_D=I_S(e^\cfrac{U_D}{U_T}-1)$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注：其中 $I_S$ 为反向饱和电流，$U_T$为热电压，$U_D$为导通电压，$i_D$为正向导通电流&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
</feed>
