<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>仰望星空</title>
  
  <subtitle>keep learning</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-03-11T17:56:08.458Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>王子晰</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>电机选型以及驱动电路</title>
    <link href="http://yoursite.com/2020/03/12/%E7%94%B5%E6%9C%BA%E9%80%89%E5%9E%8B%E4%BB%A5%E5%8F%8A%E9%A9%B1%E5%8A%A8%E7%94%B5%E8%B7%AF/"/>
    <id>http://yoursite.com/2020/03/12/%E7%94%B5%E6%9C%BA%E9%80%89%E5%9E%8B%E4%BB%A5%E5%8F%8A%E9%A9%B1%E5%8A%A8%E7%94%B5%E8%B7%AF/</id>
    <published>2020-03-11T17:16:43.983Z</published>
    <updated>2020-03-11T17:56:08.458Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>智能车制作新手上路，现在开始准备画整车的电路板，电路板一般分成4个模块：电源部分，驱动部分，主控部分，信号部分，这篇文章简单总结一下电机的选择和对应驱动电路的选择</p><a id="more"></a><h4 id="电机"><a href="#电机" class="headerlink" title="电机"></a>电机</h4><h5 id="电机分类："><a href="#电机分类：" class="headerlink" title="电机分类："></a>电机分类：</h5><p>无刷电机和有刷电机，相对而言，无刷电机更好，它没有损耗，而且支持静音，那么有刷电机就会有损耗，寿命有限，且会有声音。无刷电机的缺点是比较贵，少则几百，多则上千，那么有刷电机价格则从十几到几百不等。</p><blockquote><p>其中无刷电机又分为带霍尔元件和不带霍尔元件两种，霍尔元件的作用是测速，不带霍尔元件测速的话需要测量电机产生的反电动势，通常电机的转速都在几万转每秒，所以智能车转速需求较小，采用带霍尔测速好，对于无人机等需要高转速的应用，高转速会产生较大的反电动势，更容易测量，所以适合不带霍尔元件的无刷电机</p></blockquote><h5 id="减速箱"><a href="#减速箱" class="headerlink" title="减速箱"></a>减速箱</h5><p>由于电机每秒转速通常上万，智能车电机不需要这么大的转速，所以一般会在电机上加减速箱，同时减速箱也有增加扭矩的功能</p><h5 id="扭矩的测量"><a href="#扭矩的测量" class="headerlink" title="扭矩的测量"></a>扭矩的测量</h5><p>通常测量方法是测量电机的抖转电流和空载电流(主要是测抖转电流)，空载电流是电机不带负载的工作电流，抖转电流是电机被动停转时的工作电流，那么抖转电流越大，说明电机的带载能力越强</p><blockquote><p>测量方法是给电机加上轮子，通电开始转动，然后将轮子捏住，使其不转动，然后测量电机的电流</p></blockquote><h4 id="驱动电路"><a href="#驱动电路" class="headerlink" title="驱动电路"></a>驱动电路</h4><p>理论上，有刷电机的两端接上单片机的两个引脚，一端接3.3V，一端接0V，电机就可以动了，但是实际上不可能，因为单片机提供电流太小了，通常单片机的引脚电流不超过20mA，而小电机的驱动电流都多达160mA左右，所以需要再单片机和电机之间添加驱动电路</p><h5 id="H桥电路和集成芯片"><a href="#H桥电路和集成芯片" class="headerlink" title="H桥电路和集成芯片"></a>H桥电路和集成芯片</h5><p>最常见的驱动电路就是H桥电路了，由二极管组成(不贴图了，感兴趣且忘记了的话，自己再网上再搜一搜)，很麻烦，而且可以加载十几A的电流，我们小车用不到这么多，所以可以使用集成芯片，在芯片的周围会添加一些电容和电阻，所有方案可以在淘宝上参考产品级别的模块原理图或者是智能车的技术手册</p><blockquote><p>这里有几个小技巧：</p></blockquote><ol><li>在单片机的引脚输入端上加300Ω(这个阻值是经验值)的电阻，目的是保护电路。</li><li>在单片机的两个引脚输入端分别下拉电阻，也是保证在每有接到单片机引脚高电平的时候，一直强制要求两个输入端保持低电平</li><li>在电机的两个输入端口接上电容，起到滤波的作用</li></ol><p>2020.3.12</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;p&gt;智能车制作新手上路，现在开始准备画整车的电路板，电路板一般分成4个模块：电源部分，驱动部分，主控部分，信号部分，这篇文章简单总结一下电机的选择和对应驱动电路的选择&lt;/p&gt;
    
    </summary>
    
    
      <category term="NXP智能车" scheme="http://yoursite.com/categories/NXP%E6%99%BA%E8%83%BD%E8%BD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>ch5-2 放大器的偏置电路和直流工作点的判断</title>
    <link href="http://yoursite.com/2020/03/11/ch5-2%20%E6%94%BE%E5%A4%A7%E5%99%A8%E7%9A%84%E5%81%8F%E7%BD%AE%E7%94%B5%E8%B7%AF%E5%92%8C%E7%9B%B4%E6%B5%81%E5%B7%A5%E4%BD%9C%E7%82%B9%E7%9A%84%E5%88%A4%E6%96%AD/"/>
    <id>http://yoursite.com/2020/03/11/ch5-2%20%E6%94%BE%E5%A4%A7%E5%99%A8%E7%9A%84%E5%81%8F%E7%BD%AE%E7%94%B5%E8%B7%AF%E5%92%8C%E7%9B%B4%E6%B5%81%E5%B7%A5%E4%BD%9C%E7%82%B9%E7%9A%84%E5%88%A4%E6%96%AD/</id>
    <published>2020-03-11T09:18:42.227Z</published>
    <updated>2020-03-11T17:17:14.498Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>晶体管在放大应用的时候，要求外电路将晶体管偏置在放大区(若是NPN管就是发射结正偏，集电结反偏)，使得信号在放大的时候不产生线性失真(静态工作点太靠前的话工作在非线性区，就会失真)</p><h4 id="对偏置电路的要求"><a href="#对偏置电路的要求" class="headerlink" title="对偏置电路的要求"></a>对偏置电路的要求</h4><ol><li>在更换过管子或者温度变化之后，偏置的静态工作点也要力求稳定，即保持Icq和Uceq稳定</li><li>电路形式尽量简单，如采用单一电源，尽可能少使用电阻等</li><li>对信号和直流能量的损耗要尽量小，如减小信号中的分压分流损耗等<h4 id="常见偏置电路"><a href="#常见偏置电路" class="headerlink" title="常见偏置电路"></a>常见偏置电路</h4><h5 id="固定偏流电路"><a href="#固定偏流电路" class="headerlink" title="固定偏流电路"></a>固定偏流电路</h5>没有图，自己看PDF回顾(就是一个电源，集电极($R_B$)和基极($R_C$)各添加一个电阻)，其中：<br>$$I_C=\beta I_B$$<br>$$Uce=Ucc-I_C*R_C$$<blockquote><p>由于温度变化或者更换管子引起$\beta$和Iceo的变化，该电路不稳定</p></blockquote><h5 id="电流负反馈型偏置电路"><a href="#电流负反馈型偏置电路" class="headerlink" title="电流负反馈型偏置电路"></a>电流负反馈型偏置电路</h5>在上面电路的基础上，在管子的发射极串联电阻即可，形成负反馈效果,可以形成自我调节<blockquote><p>原理：Ic变大——&gt;Ie变大——&gt;Ue变大——&gt;Ube减小——&gt;Ib减小——&gt;Ic减小</p></blockquote></li></ol><p>$R_E$的阻值选取，自己看PDF</p><h4 id="分压式偏置电路"><a href="#分压式偏置电路" class="headerlink" title="分压式偏置电路"></a>分压式偏置电路</h4>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;p&gt;晶体管在放大应用的时候，要求外电路将晶体管偏置在放大区(若是NPN管就是发射结正偏，集电结反偏)，使得信号在放大的时候不产生线性失真(静态工
      
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch5-1 典型放大电路结构特点-三种组态放大器电路</title>
    <link href="http://yoursite.com/2020/03/11/ch5-1%20%E5%85%B8%E5%9E%8B%E6%94%BE%E5%A4%A7%E7%94%B5%E8%B7%AF%E7%BB%93%E6%9E%84%E7%89%B9%E7%82%B9-%E4%B8%89%E7%A7%8D%E7%BB%84%E6%80%81%E6%94%BE%E5%A4%A7%E5%99%A8%E7%94%B5%E8%B7%AF/"/>
    <id>http://yoursite.com/2020/03/11/ch5-1%20%E5%85%B8%E5%9E%8B%E6%94%BE%E5%A4%A7%E7%94%B5%E8%B7%AF%E7%BB%93%E6%9E%84%E7%89%B9%E7%82%B9-%E4%B8%89%E7%A7%8D%E7%BB%84%E6%80%81%E6%94%BE%E5%A4%A7%E5%99%A8%E7%94%B5%E8%B7%AF/</id>
    <published>2020-03-11T08:15:16.522Z</published>
    <updated>2020-03-11T09:17:47.776Z</updated>
    
    <content type="html"><![CDATA[<h4 id="基本放大器组成原则"><a href="#基本放大器组成原则" class="headerlink" title="基本放大器组成原则"></a>基本放大器组成原则</h4><p>基本放大器通常是指的由一个晶体管或场效应管构成的单级放大电路</p><a id="more"></a><h5 id="放大条件："><a href="#放大条件：" class="headerlink" title="放大条件："></a>放大条件：</h5><ol><li>有控制元件：晶体管或者场效应管</li><li>有外加电源提供能量</li><li>偏置在放大区</li><li>待放大信号一定加在发射结(或者栅源结)，不能加到集电结(或者漏极)</li><li>信号可从集电极或者发射极输出，不可以从基极(或栅极)输出</li><li>要有负载，将变化电流转换成电压<blockquote><p>为什么一定将待放大信号加在发射结或集电结：因为栅源电压或者Ube对输出电流的影响最大，也就是跨导比较大，所以将待放大信号加在发射结或集电结。</p></blockquote><h4 id="晶体管放大电路结构"><a href="#晶体管放大电路结构" class="headerlink" title="晶体管放大电路结构"></a>晶体管放大电路结构</h4>分成三类：共发射极，共集电极，共基极三种，其中最常见的就是共发射极电路</li></ol><p><img src="http://www.dzkfw.com.cn/jichu/UploadFiles_6678/201810/20181020175014109.gif" alt=""></p><blockquote><p>阻容耦合共发射极电路</p></blockquote><p>放大过程：Ube变大——&gt;ib变大——&gt;ic=$\beta i_b$变大——&gt;Uo变大</p><blockquote><p>其中耦合电容对直流开路，使信号源和负载不影响工作点</p></blockquote><p>2020.3.11</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;基本放大器组成原则&quot;&gt;&lt;a href=&quot;#基本放大器组成原则&quot; class=&quot;headerlink&quot; title=&quot;基本放大器组成原则&quot;&gt;&lt;/a&gt;基本放大器组成原则&lt;/h4&gt;&lt;p&gt;基本放大器通常是指的由一个晶体管或场效应管构成的单级放大电路&lt;/p&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-11 晶体管和场效应管低频小信号模型(等效电路)</title>
    <link href="http://yoursite.com/2020/03/10/ch4-11%20%E6%99%B6%E4%BD%93%E7%AE%A1%E5%92%8C%E5%9C%BA%E6%95%88%E5%BA%94%E7%AE%A1%E4%BD%8E%E9%A2%91%E5%B0%8F%E4%BF%A1%E5%8F%B7%E6%A8%A1%E5%9E%8B(%E7%AD%89%E6%95%88%E7%94%B5%E8%B7%AF)/"/>
    <id>http://yoursite.com/2020/03/10/ch4-11%20%E6%99%B6%E4%BD%93%E7%AE%A1%E5%92%8C%E5%9C%BA%E6%95%88%E5%BA%94%E7%AE%A1%E4%BD%8E%E9%A2%91%E5%B0%8F%E4%BF%A1%E5%8F%B7%E6%A8%A1%E5%9E%8B(%E7%AD%89%E6%95%88%E7%94%B5%E8%B7%AF)/</id>
    <published>2020-03-10T10:02:20.972Z</published>
    <updated>2020-03-11T09:17:59.549Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>晶体三极管模型在静态工作点附近可以近似看做线性变化，其中变化的部分看做交流分量，线性部分看做直流分量。(本章很多地方听的不明白，后期学习再补回来)</p><a id="more"></a><h4 id="流控等效模型"><a href="#流控等效模型" class="headerlink" title="流控等效模型"></a>流控等效模型</h4><ol><li>Ube变化引起ib的变化体现在体现在管子的输入电阻<blockquote><p>这里的公式下标自己觉得有点复杂，没看懂，晚上看书补充   </p></blockquote></li><li>ib对ic的控制作用体现在受控源 $i_c=\beta i_b$</li><li>Uce对ic的影响体现在输出电阻 $rce=\cfrac{dUce}{di_c}$<blockquote><p>图片PDF找一下，或者书上应该有<br>……</p></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;p&gt;晶体三极管模型在静态工作点附近可以近似看做线性变化，其中变化的部分看做交流分量，线性部分看做直流分量。(本章很多地方听的不明白，后期学习再补回来)&lt;/p&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-10 MOS场效应管的工作原理和特性参数</title>
    <link href="http://yoursite.com/2020/02/28/ch4-10%20MOS%E5%9C%BA%E6%95%88%E5%BA%94%E7%AE%A1%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%92%8C%E7%89%B9%E6%80%A7%E5%8F%82%E6%95%B0/"/>
    <id>http://yoursite.com/2020/02/28/ch4-10%20MOS%E5%9C%BA%E6%95%88%E5%BA%94%E7%AE%A1%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%92%8C%E7%89%B9%E6%80%A7%E5%8F%82%E6%95%B0/</id>
    <published>2020-02-28T08:27:34.810Z</published>
    <updated>2020-03-10T08:33:50.825Z</updated>
    
    <content type="html"><![CDATA[<h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><ol><li>绝缘栅场效应管由金属-氧化物-半导体构成(Metal-Oxide-Semiconductor),简称MOSFET(MOS管)<a id="more"></a></li><li>结构：栅极有二氧化硅(绝缘材料)和下方衬底隔开，所以栅极电流为0，</li><li>MOS管类型和符号：<br><img src="https://img-blog.csdn.net/20180302103833471" alt=""><blockquote><p>注:MOS管分为增强型和耗尽型，右侧竖线表示导电沟道，平时增强型MOS管导电沟道断开，而耗尽型平时导电沟道是接通的。其中箭头表示PN结的方向.N沟道表示导电沟道为N型，P沟道同上。</p></blockquote><h4 id="导通原理"><a href="#导通原理" class="headerlink" title="导通原理"></a>导通原理</h4><h5 id="栅源电压Ugs对导电沟道的作用"><a href="#栅源电压Ugs对导电沟道的作用" class="headerlink" title="栅源电压Ugs对导电沟道的作用"></a>栅源电压Ugs对导电沟道的作用</h5>通常衬底和栅极连接，通过施加栅源电压，产生纵向的电子和空穴的移动，当形成导电沟道时的栅源电压称为开启电压，通过对栅源电压的控制可以使沟道均匀的变宽窄<h5 id="漏源电压Uds对导电沟道的控制作用"><a href="#漏源电压Uds对导电沟道的控制作用" class="headerlink" title="漏源电压Uds对导电沟道的控制作用"></a>漏源电压Uds对导电沟道的控制作用</h5>当栅源电压大于开启电压的时候(导电沟道形成)，改变漏源电压可以使沟道不均匀变窄，直到预夹断为止(预夹断还是在电场的作用下有电流产生)<h4 id="增强型MOS管伏安特性曲线"><a href="#增强型MOS管伏安特性曲线" class="headerlink" title="增强型MOS管伏安特性曲线"></a>增强型MOS管伏安特性曲线</h4>由于栅极无电流，所以无输入特性曲线<h5 id="输出特性曲线"><a href="#输出特性曲线" class="headerlink" title="输出特性曲线"></a>输出特性曲线</h5>$$I_D=f(Uds)$$<br>和结型场效应管相似，图的话看书吧<h5 id="转移特性"><a href="#转移特性" class="headerlink" title="转移特性"></a>转移特性</h5>$$I_D=f(Ugs)$$<br>i和Ugs成平方性的关系<blockquote><p>公式比较长，这里不写了，复习的话看书吧</p></blockquote></li></ol><p>平方率关系方程中，和沟道长宽，电子迁移速率，以及单位面积栅极电容有关系<br>(<strong><em>和电容有什么关系呢?</em></strong>)<br>Ugs越大，Rds越小(Ugs越大，沟道越宽，阻值自然就小了)</p><h4 id="耗尽型MOS管伏安特性曲线"><a href="#耗尽型MOS管伏安特性曲线" class="headerlink" title="耗尽型MOS管伏安特性曲线"></a>耗尽型MOS管伏安特性曲线</h4><p>由于耗尽型MOS管本身就有导电沟道，所以，有反向电压截止，其他区别不大，复习看书吧<br>(输出特性+转移特性)</p><p>2020.3.10</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;绝缘栅场效应管由金属-氧化物-半导体构成(Metal-Oxide-Semiconductor),简称MOSFET(MOS管)
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-9 结型场效应管的工作原理和特性参数</title>
    <link href="http://yoursite.com/2020/02/28/ch4-9%20%E7%BB%93%E5%9E%8B%E5%9C%BA%E6%95%88%E5%BA%94%E7%AE%A1%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%92%8C%E7%89%B9%E6%80%A7%E5%8F%82%E6%95%B0/"/>
    <id>http://yoursite.com/2020/02/28/ch4-9%20%E7%BB%93%E5%9E%8B%E5%9C%BA%E6%95%88%E5%BA%94%E7%AE%A1%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E5%92%8C%E7%89%B9%E6%80%A7%E5%8F%82%E6%95%B0/</id>
    <published>2020-02-28T08:26:22.009Z</published>
    <updated>2020-03-10T08:33:54.664Z</updated>
    
    <content type="html"><![CDATA[<h3 id="JFET基本概念"><a href="#JFET基本概念" class="headerlink" title="JFET基本概念"></a>JFET基本概念</h3><ol><li>结型场效应管(Junction field effect tube —— JFET)，栅极(Grid)，漏极(Drain)，源极(Source)</li><li>结构：长方体两侧连接栅极，中间沟道连接漏极和源极<a id="more"></a></li><li>N沟道JFET：中间的沟道材料为N型材料，两侧是P型材料。符号上是G——&gt;S，箭头代表P——&gt;N(材料)</li><li>P沟道JFET：中间的沟道材料为P型材料，两侧是N型材料。符号上是G&lt;——S，箭头代表P——&gt;N(材料)<h4 id="JFET基本控制原理"><a href="#JFET基本控制原理" class="headerlink" title="JFET基本控制原理"></a>JFET基本控制原理</h4>$$I_D=\cfrac{Uds}{Rds}$$<br>$$Rds=\rho \cfrac{L}{S}$$<blockquote><p>沟道越长，阻力越大，横截面积越大，阻力越小。</p></blockquote></li></ol><p>PN结反偏，栅极无电流，栅极电压控制导电沟道宽度，漏极电流为多子漂移电流。比如D+，S-，电子从S——&gt;D，就是从D——&gt;S的电流了。</p><h4 id="JFET的工作原理"><a href="#JFET的工作原理" class="headerlink" title="JFET的工作原理"></a>JFET的工作原理</h4><h5 id="栅源电压对导电沟道的控制"><a href="#栅源电压对导电沟道的控制" class="headerlink" title="栅源电压对导电沟道的控制"></a>栅源电压对导电沟道的控制</h5><p>栅源电压控制中间沟道的宽窄，当沟道宽度为0时，称为( <strong><em>夹断电压</em></strong> )</p><blockquote><p>比如说，栅源电压反相增大，PN结变宽，那沟道就变窄了</p></blockquote><h5 id="漏源电压对导电沟道的控制"><a href="#漏源电压对导电沟道的控制" class="headerlink" title="漏源电压对导电沟道的控制"></a>漏源电压对导电沟道的控制</h5><p>当漏源电压增大会改变沟道靠近正端的宽窄(因为电压高的话，反偏程度更大一点)，当沟道在漏端被夹断的时候称为( <strong><em>预夹断</em></strong> )</p><blockquote><p>预夹断之后还是可以导电，因为沟道还导通大部分，在最后一小部分，尽管被夹断，但是由于电场的作用电子还是会流通。</p></blockquote><h3 id="JFET的特性曲线"><a href="#JFET的特性曲线" class="headerlink" title="JFET的特性曲线"></a>JFET的特性曲线</h3><p>以共源极为例，由于PN结反偏，所以栅极输入端无电流，无输入特性曲线</p><h4 id="输出特性"><a href="#输出特性" class="headerlink" title="输出特性"></a>输出特性</h4><p>$$Id=F(Uds)$$</p><p><img src="https://www.dgzj.com/uploads/allimg/180320/1ZJ423L-1.jpg" alt=""></p><blockquote><p>nice啊，这张图看的很清楚</p></blockquote><h5 id="恒流区"><a href="#恒流区" class="headerlink" title="恒流区"></a>恒流区</h5><ol><li>条件：<br>$$Ugs&gt;Ugsoff$$<br>$$Uds&gt;=Ugs - Ugsoff$$<blockquote><p>就是使沟道处在预夹断状态和但没有完全夹断</p></blockquote></li><li>特点：<br>$Uds$增大时，$i_D$也会变大一点，因为长度变小，阻值变小，所以电流变大一点，因为增大很小所以可以看做恒流。<h5 id="可变电阻区"><a href="#可变电阻区" class="headerlink" title="可变电阻区"></a>可变电阻区</h5></li><li>条件：<br>$$Ugs&gt;Ugsoff$$<br>$$Uds&lt;Ugs - Ugsoff$$</li><li>特点<blockquote><p>处于预夹断状态之前，Ugs增大，电流id会近似线性增大，变化的斜率受到栅源电压的控制，等效为漏极和源极直接接了一个电阻</p></blockquote></li></ol><blockquote><p>实际上这个电阻是沟道的阻值</p></blockquote><h5 id="截止区"><a href="#截止区" class="headerlink" title="截止区"></a>截止区</h5><ol><li>条件：<br>$$Ugs&lt;=Ugsoff$$ <blockquote><p>就是使沟道完全夹断</p></blockquote></li><li>特点：<blockquote><p>沟道夹断，漏极和源极之间断开</p></blockquote><h5 id="击穿区"><a href="#击穿区" class="headerlink" title="击穿区"></a>击穿区</h5>$Uds$大到一定程度就会击穿，实际中要避免这个情况<h4 id="转移特性"><a href="#转移特性" class="headerlink" title="转移特性"></a>转移特性</h4>$$Id=F(Ugs)$$</li></ol><blockquote><p>和上图相似，两者成平方性的关系</p></blockquote><p>2020.3.6</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;JFET基本概念&quot;&gt;&lt;a href=&quot;#JFET基本概念&quot; class=&quot;headerlink&quot; title=&quot;JFET基本概念&quot;&gt;&lt;/a&gt;JFET基本概念&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;结型场效应管(Junction field effect tube —— JFET)，栅极(Grid)，漏极(Drain)，源极(Source)&lt;/li&gt;
&lt;li&gt;结构：长方体两侧连接栅极，中间沟道连接漏极和源极
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-8 双极型晶体管极限参数和工作状态判别举例</title>
    <link href="http://yoursite.com/2020/02/27/ch4-8%20%E5%8F%8C%E6%9E%81%E5%9E%8B%E6%99%B6%E4%BD%93%E7%AE%A1%E6%9E%81%E9%99%90%E5%8F%82%E6%95%B0%E5%92%8C%E5%B7%A5%E4%BD%9C%E7%8A%B6%E6%80%81%E5%88%A4%E5%88%AB%E4%B8%BE%E4%BE%8B/"/>
    <id>http://yoursite.com/2020/02/27/ch4-8%20%E5%8F%8C%E6%9E%81%E5%9E%8B%E6%99%B6%E4%BD%93%E7%AE%A1%E6%9E%81%E9%99%90%E5%8F%82%E6%95%B0%E5%92%8C%E5%B7%A5%E4%BD%9C%E7%8A%B6%E6%80%81%E5%88%A4%E5%88%AB%E4%B8%BE%E4%BE%8B/</id>
    <published>2020-02-27T07:50:32.149Z</published>
    <updated>2020-02-28T08:25:27.993Z</updated>
    
    <content type="html"><![CDATA[<h4 id="晶体管的极限参数"><a href="#晶体管的极限参数" class="headerlink" title="晶体管的极限参数"></a>晶体管的极限参数</h4><ol><li>击穿电压：因为有三个极，所以有三个反向击穿电压。</li><li>集电极最大允许电流$Icm$：随着$I_C$的增大，$\beta$会减小，当$\beta_0=\cfrac{2}{3}\beta$时候的电流称作$Icm$(最大允许电流)，集电极电流超过此值的时候，放大倍数会减小(因为$\beta$明显减小)<a id="more"></a></li><li>集电极最大允许耗散功率$Pcm$:当管子超过该功率的时候，会性能下降或者被烧坏<br>$$P_C=I_C*Uce$$</li><li>在上述的三个限制条件下，即是晶体三极管的安全工作区，使用管子的时候，尽量在此安全工作区下使用<h4 id="温度对晶体管参数的影响"><a href="#温度对晶体管参数的影响" class="headerlink" title="温度对晶体管参数的影响"></a>温度对晶体管参数的影响</h4>温度升高，$Ube$减小，$Icbo$增大，$\beta$增大，在输出特性曲线表示为，温度升高，曲线上移的间隔变大<h4 id="典型题目"><a href="#典型题目" class="headerlink" title="典型题目"></a>典型题目</h4></li><li>判断晶体管工作状态<blockquote><p>看两极是正偏还是反偏</p></blockquote></li><li>晶体管在放大状态下，判别管型和电机<blockquote><p>规律：<strong>NPN管：电流从e极流出，从b,c极流入；PNP管：电流从e极流入，从b,c极流出</strong></p></blockquote></li><li>根据晶体管的电位判别电极，管型和材料<blockquote><p>规律：<strong>e结电压为0.7V–硅管，0.3V–锗管；c极电位最高，e极电位最低，则为NPN管；e极电位最高，c极电位最低，则为PNP管；</strong></p></blockquote></li></ol><p>2020.2.28</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;晶体管的极限参数&quot;&gt;&lt;a href=&quot;#晶体管的极限参数&quot; class=&quot;headerlink&quot; title=&quot;晶体管的极限参数&quot;&gt;&lt;/a&gt;晶体管的极限参数&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;击穿电压：因为有三个极，所以有三个反向击穿电压。&lt;/li&gt;
&lt;li&gt;集电极最大允许电流$Icm$：随着$I_C$的增大，$\beta$会减小，当$\beta_0=\cfrac{2}{3}\beta$时候的电流称作$Icm$(最大允许电流)，集电极电流超过此值的时候，放大倍数会减小(因为$\beta$明显减小)
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-7 双极型晶体管特性曲线</title>
    <link href="http://yoursite.com/2020/02/27/ch4-7%20%E5%8F%8C%E6%9E%81%E5%9E%8B%E6%99%B6%E4%BD%93%E7%AE%A1%E7%89%B9%E6%80%A7%E6%9B%B2%E7%BA%BF/"/>
    <id>http://yoursite.com/2020/02/27/ch4-7%20%E5%8F%8C%E6%9E%81%E5%9E%8B%E6%99%B6%E4%BD%93%E7%AE%A1%E7%89%B9%E6%80%A7%E6%9B%B2%E7%BA%BF/</id>
    <published>2020-02-27T07:49:34.709Z</published>
    <updated>2020-02-28T06:30:34.428Z</updated>
    
    <content type="html"><![CDATA[<h4 id="共发射极输出特性曲线–输出电流和输出电压的关系"><a href="#共发射极输出特性曲线–输出电流和输出电压的关系" class="headerlink" title="共发射极输出特性曲线–输出电流和输出电压的关系"></a>共发射极输出特性曲线–输出电流和输出电压的关系</h4><a id="more"></a><p><img src="https://player.slidesplayer.com/101/17390235/slides/slide_2.jpg" alt=""></p><blockquote><p>如上图所示，输出特性曲线分成，放大区，截止区，饱和区。</p></blockquote><h5 id="放大区"><a href="#放大区" class="headerlink" title="放大区"></a>放大区</h5><blockquote><p>条件：e结正偏( $I_B&gt;0$ )，c结反偏（ $Uce&gt;=Ube$ ）</p></blockquote><p>特点：</p><ol><li>由图可见$I_B$对$I_C$有很强的控制力，当 $I_B$ 变化一点的时候，$I_C$就会有比较大的变化量,因此我们定义了共发射极交流电流放大倍数：<br>$$\beta=\cfrac{\Delta I_C}{\Delta I_B}$$<blockquote><p>当$Uce$为常数时候，上式成立。在特性曲线上反映是两条不同的$I_B$之间的间隔</p></blockquote></li><li>$Uce$ 对 $I_C$ 的影响很小，<strong><em>$I_C$略有上升，末尾不是平的，还是向上倾斜的。</em></strong><blockquote><p>原因：基区宽度调制效应，当 $Uce$ 变大后，集电区PN结的内电场变宽，所以基区变窄，基区复合的电流会少一点，但是由于 $I_B$ 不变，复合的电流很少</p></blockquote><h5 id="饱和区"><a href="#饱和区" class="headerlink" title="饱和区"></a>饱和区</h5><blockquote><p>条件：e结正偏，c结正偏，( Uce&lt; Ube) 即临界饱和线的左侧</p></blockquote></li></ol><p>特点：</p><ol><li>$I_C$不受到$I_B$的控制</li><li>$Uce$一定而$I_B$增大时，$I_C$基本不变，因此$\beta$趋近于0</li><li>$I_B$一定时，$I_C$的数值比放大时小<h5 id="截止区"><a href="#截止区" class="headerlink" title="截止区"></a>截止区</h5><blockquote><p>条件：e结和c结均处于反偏</p></blockquote></li></ol><p>特点：</p><ol><li>三个电极上的电流均为反向电流，即极间开路<blockquote><p>当$I_B=0$时，$I_C=Iceo$，小功率管$Iceo$很小，可以视$I_C=0$，当大功率管时，$Iceo$很大，则必须保证e结反偏</p></blockquote><h4 id="共发射极输入特性曲线–输入电流和输入电压的关系"><a href="#共发射极输入特性曲线–输入电流和输入电压的关系" class="headerlink" title="共发射极输入特性曲线–输入电流和输入电压的关系"></a>共发射极输入特性曲线–输入电流和输入电压的关系</h4><blockquote><p>随着$Uce$的增大，$I_B$也增大，类似二极管特性曲线，当$Ube&lt;0$时，晶体管截止，超过某值，e结也会反向击穿。</p></blockquote></li></ol><blockquote><p>同样有死区电压，硅管：$Ube:$ 0.6–0.7V,锗管$Ube:$ 0.1–0.3V</p></blockquote><h4 id="转移特性（输出电流-I-C-和输入电压-Ube-的关系）"><a href="#转移特性（输出电流-I-C-和输入电压-Ube-的关系）" class="headerlink" title="转移特性（输出电流$I_C$和输入电压$Ube$的关系）"></a>转移特性（输出电流$I_C$和输入电压$Ube$的关系）</h4><p><img src="https://player.slidesplayer.com/101/17390235/slides/slide_8.jpg" alt=""></p><p>2020.2.28</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;共发射极输出特性曲线–输出电流和输出电压的关系&quot;&gt;&lt;a href=&quot;#共发射极输出特性曲线–输出电流和输出电压的关系&quot; class=&quot;headerlink&quot; title=&quot;共发射极输出特性曲线–输出电流和输出电压的关系&quot;&gt;&lt;/a&gt;共发射极输出特性曲线–输出电流和输出电压的关系&lt;/h4&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ResNet-18</title>
    <link href="http://yoursite.com/2020/02/27/ResNet-18/"/>
    <id>http://yoursite.com/2020/02/27/ResNet-18/</id>
    <published>2020-02-27T03:54:50.026Z</published>
    <updated>2020-02-27T07:33:34.937Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p><strong>Deep Residual Learning for Image Recognition</strong></p><p>ResNet是何凯明（微软亚洲AI研究院工作）提出的残差神经网络，曾经在Kaggle等平台上获得多次大奖。</p><a id="more"></a><h3 id="为什么提出ResNet"><a href="#为什么提出ResNet" class="headerlink" title="为什么提出ResNet"></a>为什么提出ResNet</h3><p>众所周知，随着神经网络的发展，深度越大，网络的表达性能就越好，可实际训练的时候，随着深度的加大，网络出现了梯度弥散（也有叫梯度消失等）的情况。</p><blockquote><p>比如说在原始的网络当中，输入变量每经过一层就通过一次sigmoid激活函数，由于sigmoid函数只在0附近梯度变化明显，远离0附近梯度变化趋近于0，因此随着网络的深化，梯度变化趋于0，相当于线性恒等映射，深化的网络是做了无用功。</p></blockquote><p>为了解决该问题，人们想了一些办法，比如说改变激活函数使用relu，Leaky—relu，或者Batch Normalization等，但是不能从根本上解决问题，因此何凯明提出了ResNet（残差神经网络）</p><p>其他参考</p><blockquote><p>虽然通过Batch Normalization或者正则初始化等能够训练了，但是又会出现另一个问题，就是<strong>退化问题</strong>，网络层数增加，但是在训练集上的准确率却饱和甚至下降了。<br>退化问题说明了深度网络不能很简单地被很好地优化</p></blockquote><h3 id="ResNet是什么？怎么解决梯度弥散以及退化问题？"><a href="#ResNet是什么？怎么解决梯度弥散以及退化问题？" class="headerlink" title="ResNet是什么？怎么解决梯度弥散以及退化问题？"></a>ResNet是什么？怎么解决梯度弥散以及退化问题？</h3><h4 id="两种mapping"><a href="#两种mapping" class="headerlink" title="两种mapping"></a>两种mapping</h4><p>何凯明在ResNet中提出两种mapping：</p><ol><li>identity mapping(处理图像中也称叫feature map)：就是本身，即下图中的x</li><li>residual mapping：指的公式中的$F(x)$ <h4 id="残差函数"><a href="#残差函数" class="headerlink" title="残差函数"></a>残差函数</h4>ResNet中通过学习残差函数来解决问题，学习差值比学习梯度变化要容易的多,摘录知乎解释：<blockquote><p>F是求和前网络映射，H是从输入到求和后的网络映射。比如把5映射到5.1，那么引入残差前是F’(5)=5.1，引入残差后是H(5)=5.1, H(5)=F(5)+5, F(5)=0.1。这里的F’和F都表示网络参数映射，引入残差后的映射对输出的变化更敏感。比如s输出从5.1变到5.2，映射F’的输出增加了1/51=2%，而对于残差结构输出从5.1到5.2，映射F是从0.1到0.2，增加了100%。明显后者输出变化对权重的调整作用更大，所以效果更好。残差的思想都是去掉相同的主体部分，从而突出微小的变化，看到残差网络我第一反应就是差分放大器…（博主：哈哈，刚开始接触ResNet的时候我还没学到差分放大器）</p></blockquote></li></ol><p>上述中的$H(x)=F(x)+x$，$F(x)$为残差函数，如果$F(x)=0$，则为恒等映射，这样设计网络可以保证随着深度的加大，不论怎么训练，至少层数更深的网络训练的效果不会比层数浅的网络效果差，网络会一直处于最优状态(理论上)，且残差拟合更加容易，学习速度更快。</p><h4 id="Residual-Block"><a href="#Residual-Block" class="headerlink" title="Residual Block"></a>Residual Block</h4><p><img src="https://upload-images.jianshu.io/upload_images/6095626-49ac0caeb5525b93.png" alt=""></p><p>上图为Residual Block，可以看到输入变量x，通过两层网络和x（通过shortcut）进行element-wise add（就是对应元素加到一起，element-wise是对应元素相乘），然后再经过一个relu输出就是一个Residual Block。</p><p><img src="https://upload-images.jianshu.io/upload_images/6095626-287fc59a3cd86488.png" alt=""></p><p>这两种结构常用在ResNet18，ResNet34(左图)，ResNet50/101/152(右图)，其中右图又被称作”bottleneck”</p><h3 id="ResNet-18-Cifar-10"><a href="#ResNet-18-Cifar-10" class="headerlink" title="ResNet-18 Cifar-10"></a>ResNet-18 Cifar-10</h3><h4 id="Cifar-10"><a href="#Cifar-10" class="headerlink" title="Cifar-10"></a>Cifar-10</h4><p>这个数据集包含60000张32*32的彩色图片，这些图片一共被分成10类，有小猫，小狗等……详见：<a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></p><h4 id="ResNet-18网络结构"><a href="#ResNet-18网络结构" class="headerlink" title="ResNet-18网络结构"></a>ResNet-18网络结构</h4><p><img src="https://img-blog.csdn.net/20180426215052446?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1bnFpYW5kZTg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></p><blockquote><p>上图中虚线表示channel改变，实线表示channel不变</p></blockquote><h4 id="实现代码1"><a href="#实现代码1" class="headerlink" title="实现代码1"></a>实现代码1</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span>  torch</span><br><span class="line"><span class="keyword">from</span>    torch <span class="keyword">import</span>  nn</span><br><span class="line"><span class="keyword">from</span>    torch.nn <span class="keyword">import</span> functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span>    torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span>    torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span>    torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span>    torch <span class="keyword">import</span> nn, optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># from    torchvision.models import resnet18</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResBlk</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    resnet block</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ch_in, ch_out)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param ch_in:</span></span><br><span class="line"><span class="string">        :param ch_out:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(ResBlk, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(ch_in, ch_out, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(ch_out)</span><br><span class="line">        self.conv2 = nn.Conv2d(ch_out, ch_out, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(ch_out)</span><br><span class="line"></span><br><span class="line">        self.extra = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> ch_out != ch_in:</span><br><span class="line">            <span class="comment"># [b, ch_in, h, w] =&gt; [b, ch_out, h, w]</span></span><br><span class="line">            self.extra = nn.Sequential(</span><br><span class="line">                nn.Conv2d(ch_in, ch_out, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>),<span class="comment">#既然维度不一样，为什么用卷积而不用别的呢？</span></span><br><span class="line">                nn.BatchNorm2d(ch_out)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param x: [b, ch, h, w]</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        out = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        out = self.bn2(self.conv2(out))</span><br><span class="line">        <span class="comment"># short cut.</span></span><br><span class="line">        <span class="comment"># extra module: [b, ch_in, h, w] =&gt; [b, ch_out, h, w]</span></span><br><span class="line">        <span class="comment"># element-wise add:</span></span><br><span class="line">        out = self.extra(x) + out  </span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet18</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(ResNet18, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">16</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">16</span>)</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># followed 4 blocks</span></span><br><span class="line">        <span class="comment"># [b, 64, h, w] =&gt; [b, 128, h ,w]</span></span><br><span class="line">        self.blk1 = ResBlk(<span class="number">16</span>, <span class="number">16</span>)</span><br><span class="line">        <span class="comment"># [b, 128, h, w] =&gt; [b, 256, h, w]</span></span><br><span class="line">        self.blk2 = ResBlk(<span class="number">16</span>, <span class="number">32</span>)</span><br><span class="line">        <span class="comment"># # [b, 256, h, w] =&gt; [b, 512, h, w]</span></span><br><span class="line">        <span class="comment"># self.blk3 = ResBlk(128, 256)</span></span><br><span class="line">        <span class="comment"># # [b, 512, h, w] =&gt; [b, 1024, h, w]</span></span><br><span class="line">        <span class="comment"># self.blk4 = ResBlk(256, 512)</span></span><br><span class="line"></span><br><span class="line">        self.outlayer = nn.Linear(<span class="number">32</span>*<span class="number">32</span>*<span class="number">32</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :param x:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># [b, 64, h, w] =&gt; [b, 1024, h, w]</span></span><br><span class="line">        x = self.blk1(x)</span><br><span class="line">        x = self.blk2(x)</span><br><span class="line">        <span class="comment"># x = self.blk3(x)</span></span><br><span class="line">        <span class="comment"># x = self.blk4(x)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># print(x.shape)</span></span><br><span class="line">        x = x.view(x.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        x = self.outlayer(x)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    batchsz = <span class="number">32</span></span><br><span class="line"></span><br><span class="line">    cifar_train = datasets.CIFAR10(<span class="string">'cifar'</span>, <span class="literal">True</span>, transform=transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">    ]), download=<span class="literal">True</span>)</span><br><span class="line">    cifar_train = DataLoader(cifar_train, batch_size=batchsz, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    cifar_test = datasets.CIFAR10(<span class="string">'cifar'</span>, <span class="literal">False</span>, transform=transforms.Compose([</span><br><span class="line">        transforms.Resize((<span class="number">32</span>, <span class="number">32</span>)),</span><br><span class="line">        transforms.ToTensor()</span><br><span class="line">    ]), download=<span class="literal">True</span>)</span><br><span class="line">    cifar_test = DataLoader(cifar_test, batch_size=batchsz, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    x, label = iter(cifar_train).next()</span><br><span class="line">    print(<span class="string">'x:'</span>, x.shape, <span class="string">'label:'</span>, label.shape)</span><br><span class="line"></span><br><span class="line">    device = torch.device(<span class="string">'cuda'</span>)</span><br><span class="line">    <span class="comment"># model = Lenet5().to(device)</span></span><br><span class="line">    model = ResNet18().to(device)</span><br><span class="line"></span><br><span class="line">    criteon = nn.CrossEntropyLoss().to(device)</span><br><span class="line">    optimizer = optim.Adam(model.parameters(), lr=<span class="number">1e-3</span>)</span><br><span class="line">    print(model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line"></span><br><span class="line">        model.train()</span><br><span class="line">        <span class="keyword">for</span> batchidx, (x, label) <span class="keyword">in</span> enumerate(cifar_train):</span><br><span class="line">            <span class="comment"># [b, 3, 32, 32]</span></span><br><span class="line">            <span class="comment"># [b]</span></span><br><span class="line">            x, label = x.to(device), label.to(device)</span><br><span class="line"></span><br><span class="line">            logits = model(x)</span><br><span class="line">            <span class="comment"># logits: [b, 10]</span></span><br><span class="line">            <span class="comment"># label:  [b]</span></span><br><span class="line">            <span class="comment"># loss: tensor scalar</span></span><br><span class="line">            loss = criteon(logits, label)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># backprop</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        print(epoch, <span class="string">'loss:'</span>, loss.item())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        model.eval()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># test</span></span><br><span class="line">            total_correct = <span class="number">0</span></span><br><span class="line">            total_num = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> x, label <span class="keyword">in</span> cifar_test:</span><br><span class="line">                <span class="comment"># [b, 3, 32, 32]</span></span><br><span class="line">                <span class="comment"># [b]</span></span><br><span class="line">                x, label = x.to(device), label.to(device)</span><br><span class="line"></span><br><span class="line">                <span class="comment"># [b, 10]</span></span><br><span class="line">                logits = model(x)</span><br><span class="line">                <span class="comment"># [b]</span></span><br><span class="line">                pred = logits.argmax(dim=<span class="number">1</span>)</span><br><span class="line">                <span class="comment"># [b] vs [b] =&gt; scalar tensor</span></span><br><span class="line">                correct = torch.eq(pred, label).float().sum().item()</span><br><span class="line">                total_correct += correct</span><br><span class="line">                total_num += x.size(<span class="number">0</span>)</span><br><span class="line">                <span class="comment"># print(correct)</span></span><br><span class="line"></span><br><span class="line">            acc = total_correct / total_num</span><br><span class="line">            print(epoch, <span class="string">'acc:'</span>, acc)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><ol><li>该代码没有完整实现ResNet-18结构，只实现了两residual Block。后面我自己会补上</li><li>初学ResNet，这个代码还是很OK的。<h4 id="实现代码2"><a href="#实现代码2" class="headerlink" title="实现代码2"></a>实现代码2</h4><blockquote><p>注：这段代码摘录于CSDN，由作者所说，acc = 95.170%，是完整实现ResNet-18，且封装性优于上述代码，参考价值很高</p></blockquote><h5 id="Pytorch上搭建ResNet-18："><a href="#Pytorch上搭建ResNet-18：" class="headerlink" title="Pytorch上搭建ResNet-18："></a>Pytorch上搭建ResNet-18：</h5><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'''ResNet-18 Image classfication for cifar-10 with PyTorch </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Author 'Sun-qian'.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResidualBlock</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inchannel, outchannel, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(ResidualBlock, self).__init__()</span><br><span class="line">        self.left = nn.Sequential(</span><br><span class="line">            nn.Conv2d(inchannel, outchannel, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outchannel),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Conv2d(outchannel, outchannel, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(outchannel)</span><br><span class="line">        )</span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> inchannel != outchannel:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(inchannel, outchannel, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(outchannel)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.left(x)</span><br><span class="line">        out += self.shortcut(x)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ResNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, ResidualBlock, num_classes=<span class="number">10</span>)</span>:</span></span><br><span class="line">        super(ResNet, self).__init__()</span><br><span class="line">        self.inchannel = <span class="number">64</span></span><br><span class="line">        self.conv1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.BatchNorm2d(<span class="number">64</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">        )</span><br><span class="line">        self.layer1 = self.make_layer(ResidualBlock, <span class="number">64</span>,  <span class="number">2</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.layer2 = self.make_layer(ResidualBlock, <span class="number">128</span>, <span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self.make_layer(ResidualBlock, <span class="number">256</span>, <span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self.make_layer(ResidualBlock, <span class="number">512</span>, <span class="number">2</span>, stride=<span class="number">2</span>)</span><br><span class="line">        self.fc = nn.Linear(<span class="number">512</span>, num_classes)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_layer</span><span class="params">(self, block, channels, num_blocks, stride)</span>:</span></span><br><span class="line">        strides = [stride] + [<span class="number">1</span>] * (num_blocks - <span class="number">1</span>)   <span class="comment">#strides=[1,1]</span></span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> stride <span class="keyword">in</span> strides:</span><br><span class="line">            layers.append(block(self.inchannel, channels, stride))</span><br><span class="line">            self.inchannel = channels</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.layer1(out)</span><br><span class="line">        out = self.layer2(out)</span><br><span class="line">        out = self.layer3(out)</span><br><span class="line">        out = self.layer4(out)</span><br><span class="line">        out = F.avg_pool2d(out, <span class="number">4</span>)</span><br><span class="line">        out = out.view(out.size(<span class="number">0</span>), <span class="number">-1</span>)</span><br><span class="line">        out = self.fc(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ResNet18</span><span class="params">()</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ResNet(ResidualBlock)</span><br></pre></td></tr></table></figure><h5 id="Pytorch上训练："><a href="#Pytorch上训练：" class="headerlink" title="Pytorch上训练："></a>Pytorch上训练：</h5><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">from</span> resnet <span class="keyword">import</span> ResNet18</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义是否使用GPU</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 参数设置,使得我们能够手动输入命令行参数，就是让风格变得和Linux命令行差不多</span></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">'PyTorch CIFAR10 Training'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'--outf'</span>, default=<span class="string">'./model/'</span>, help=<span class="string">'folder to output images and model checkpoints'</span>) <span class="comment">#输出结果保存路径</span></span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数设置</span></span><br><span class="line">EPOCH = <span class="number">135</span>   <span class="comment">#遍历数据集次数</span></span><br><span class="line">pre_epoch = <span class="number">0</span>  <span class="comment"># 定义已经遍历数据集的次数</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span>      <span class="comment">#批处理尺寸(batch_size)</span></span><br><span class="line">LR = <span class="number">0.01</span>        <span class="comment">#学习率</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集并预处理</span></span><br><span class="line">transform_train = transforms.Compose([</span><br><span class="line">    transforms.RandomCrop(<span class="number">32</span>, padding=<span class="number">4</span>),  <span class="comment">#先四周填充0，在吧图像随机裁剪成32*32</span></span><br><span class="line">    transforms.RandomHorizontalFlip(),  <span class="comment">#图像一半的概率翻转，一半的概率不翻转</span></span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>)), <span class="comment">#R,G,B每层的归一化用到的均值和方差</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">transform_test = transforms.Compose([</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize((<span class="number">0.4914</span>, <span class="number">0.4822</span>, <span class="number">0.4465</span>), (<span class="number">0.2023</span>, <span class="number">0.1994</span>, <span class="number">0.2010</span>)),</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform_train) <span class="comment">#训练数据集</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)   <span class="comment">#生成一个个batch进行批训练，组成batch的时候顺序打乱取</span></span><br><span class="line"></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform_test)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">100</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Cifar-10的标签</span></span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>, <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型定义-ResNet</span></span><br><span class="line">net = ResNet18().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数和优化方式</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment">#损失函数为交叉熵，多用于多分类问题</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=LR, momentum=<span class="number">0.9</span>, weight_decay=<span class="number">5e-4</span>) <span class="comment">#优化方式为mini-batch momentum-SGD，并采用L2正则化（权重衰减）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(args.outf):</span><br><span class="line">os.makedirs(args.outf)</span><br><span class="line">    best_acc = <span class="number">85</span>  <span class="comment">#2 初始化best test accuracy</span></span><br><span class="line">    print(<span class="string">"Start Training, Resnet-18!"</span>)  <span class="comment"># 定义遍历数据集的次数</span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">"acc.txt"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">with</span> open(<span class="string">"log.txt"</span>, <span class="string">"w"</span>)<span class="keyword">as</span> f2:</span><br><span class="line">            <span class="keyword">for</span> epoch <span class="keyword">in</span> range(pre_epoch, EPOCH):</span><br><span class="line">                print(<span class="string">'\nEpoch: %d'</span> % (epoch + <span class="number">1</span>))</span><br><span class="line">                net.train()</span><br><span class="line">                sum_loss = <span class="number">0.0</span></span><br><span class="line">                correct = <span class="number">0.0</span></span><br><span class="line">                total = <span class="number">0.0</span></span><br><span class="line">                <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">                    <span class="comment"># 准备数据</span></span><br><span class="line">                    length = len(trainloader)</span><br><span class="line">                    inputs, labels = data</span><br><span class="line">                    inputs, labels = inputs.to(device), labels.to(device)</span><br><span class="line">                    optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># forward + backward</span></span><br><span class="line">                    outputs = net(inputs)</span><br><span class="line">                    loss = criterion(outputs, labels)</span><br><span class="line">                    loss.backward()</span><br><span class="line">                    optimizer.step()</span><br><span class="line"></span><br><span class="line">                    <span class="comment"># 每训练1个batch打印一次loss和准确率</span></span><br><span class="line">                    sum_loss += loss.item()</span><br><span class="line">                    _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">                    total += labels.size(<span class="number">0</span>)</span><br><span class="line">                    correct += predicted.eq(labels.data).cpu().sum()</span><br><span class="line">                    print(<span class="string">'[epoch:%d, iter:%d] Loss: %.03f | Acc: %.3f%% '</span></span><br><span class="line">                          % (epoch + <span class="number">1</span>, (i + <span class="number">1</span> + epoch * length), sum_loss / (i + <span class="number">1</span>), <span class="number">100.</span> * correct / total))</span><br><span class="line">                    f2.write(<span class="string">'%03d  %05d |Loss: %.03f | Acc: %.3f%% '</span></span><br><span class="line">                          % (epoch + <span class="number">1</span>, (i + <span class="number">1</span> + epoch * length), sum_loss / (i + <span class="number">1</span>), <span class="number">100.</span> * correct / total))</span><br><span class="line">                    f2.write(<span class="string">'\n'</span>)</span><br><span class="line">                    f2.flush()</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 每训练完一个epoch测试一下准确率</span></span><br><span class="line">                print(<span class="string">"Waiting Test!"</span>)</span><br><span class="line">                <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                    correct = <span class="number">0</span></span><br><span class="line">                    total = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">                        net.eval()</span><br><span class="line">                        images, labels = data</span><br><span class="line">                        images, labels = images.to(device), labels.to(device)</span><br><span class="line">                        outputs = net(images)</span><br><span class="line">                        <span class="comment"># 取得分最高的那个类 (outputs.data的索引号)</span></span><br><span class="line">                        _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">                        total += labels.size(<span class="number">0</span>)</span><br><span class="line">                        correct += (predicted == labels).sum()</span><br><span class="line">                    print(<span class="string">'测试分类准确率为：%.3f%%'</span> % (<span class="number">100</span> * correct / total))</span><br><span class="line">                    acc = <span class="number">100.</span> * correct / total</span><br><span class="line">                    <span class="comment"># 将每次测试结果实时写入acc.txt文件中</span></span><br><span class="line">                    print(<span class="string">'Saving model......'</span>)</span><br><span class="line">                    torch.save(net.state_dict(), <span class="string">'%s/net_%03d.pth'</span> % (args.outf, epoch + <span class="number">1</span>))</span><br><span class="line">                    f.write(<span class="string">"EPOCH=%03d,Accuracy= %.3f%%"</span> % (epoch + <span class="number">1</span>, acc))</span><br><span class="line">                    f.write(<span class="string">'\n'</span>)</span><br><span class="line">                    f.flush()</span><br><span class="line">                    <span class="comment"># 记录最佳测试分类准确率并写入best_acc.txt文件中</span></span><br><span class="line">                    <span class="keyword">if</span> acc &gt; best_acc:</span><br><span class="line">                        f3 = open(<span class="string">"best_acc.txt"</span>, <span class="string">"w"</span>)</span><br><span class="line">                        f3.write(<span class="string">"EPOCH=%d,best_acc= %.3f%%"</span> % (epoch + <span class="number">1</span>, acc))</span><br><span class="line">                        f3.close()</span><br><span class="line">                        best_acc = acc</span><br><span class="line">            print(<span class="string">"Training Finished, TotalEPOCH=%d"</span> % EPOCH)</span><br></pre></td></tr></table></figure><h5 id="实现效果"><a href="#实现效果" class="headerlink" title="实现效果"></a>实现效果</h5><img src="https://img-blog.csdn.net/20180426220936286?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1bnFpYW5kZTg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt=""></li></ol><blockquote><p>注：该图像是作者将数据下载到.txt文件，然后再matlab中进行生成</p></blockquote><h5 id="小结-1"><a href="#小结-1" class="headerlink" title="小结"></a>小结</h5><ol><li>将数据导入.txt文件，用matlab处理<blockquote><p>确实是好方法，visdom用起来也会很方便</p></blockquote></li><li>定义GPU是否使用的写法</li><li>ResNet-18模块封装性很好，完全符合结构<h5 id="困惑"><a href="#困惑" class="headerlink" title="困惑"></a>困惑</h5>在make_layer那里最后一句的return nn.Sequential(<em>layers)中的</em>layers是什么意思呢，上面加*是什么意思呢？<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3></li><li><a href="https://blog.csdn.net/sunqiande88/article/details/80100891">https://blog.csdn.net/sunqiande88/article/details/80100891</a></li><li><a href="http://www.jeepxie.net/article/601129.html">http://www.jeepxie.net/article/601129.html</a></li><li><a href="https://www.jianshu.com/p/e58437f39f65">https://www.jianshu.com/p/e58437f39f65</a></li><li><a href="https://www.zhihu.com/question/53224378/answer/159102095">https://www.zhihu.com/question/53224378/answer/159102095</a></li><li>Cifar-10：<a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Deep Residual Learning for Image Recognition&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;ResNet是何凯明（微软亚洲AI研究院工作）提出的残差神经网络，曾经在Kaggle等平台上获得多次大奖。&lt;/p&gt;
    
    </summary>
    
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-6 双极型晶体三管工作原理</title>
    <link href="http://yoursite.com/2020/02/26/ch4-6%20%E5%8F%8C%E6%9E%81%E5%9E%8B%E6%99%B6%E4%BD%93%E4%B8%89%E7%AE%A1%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/"/>
    <id>http://yoursite.com/2020/02/26/ch4-6%20%E5%8F%8C%E6%9E%81%E5%9E%8B%E6%99%B6%E4%BD%93%E4%B8%89%E7%AE%A1%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/</id>
    <published>2020-02-26T04:44:16.054Z</published>
    <updated>2020-02-26T08:37:13.978Z</updated>
    
    <content type="html"><![CDATA[<h4 id="晶体管的结构以及符号"><a href="#晶体管的结构以及符号" class="headerlink" title="晶体管的结构以及符号"></a>晶体管的结构以及符号</h4><ol><li><p>结构：发射区（e，emitter）集电区（c，collector），基区（b，basic）</p><blockquote><p>发射区重掺杂，基区轻掺杂，集电区面积大</p></blockquote></li><li><p>符号：箭头：从P区指向N区</p><a id="more"></a><h4 id="晶体管工作原理"><a href="#晶体管工作原理" class="headerlink" title="晶体管工作原理"></a>晶体管工作原理</h4><p>看图比较好理解，这里我不放图了，简单叙述一下工作过程</p><blockquote><p>在晶体管在发射结正偏，集电结反偏的情况下：</p></blockquote></li><li><p>发射结电子因为发射结正偏，扩散运动——&gt;基区，其中少部分电子和基结的空穴复合</p></li><li><p>由于基区轻掺杂，扩散运动过来的自由电子只有少部分在基区和空穴复合</p></li><li><p>由于集电区反偏，形成强大电场，产生漂移运动，基区大部分电子漂移到集电区</p></li><li><p>因为集电区反偏，所以基区和集电区的少子相互漂移，形成反向饱和电流（很小，和温度关系很大，与之前PN结的情况是一样的）</p><blockquote><p>在晶体三极管中在基区的作用下，把电子从发射区几乎全部传到了集电区，这是放大功能的体现（<strong><em>这哪里体现了放大？？睁眼说瞎</em></strong>）</p></blockquote><h4 id="比例系数-beta-alpha"><a href="#比例系数-beta-alpha" class="headerlink" title="比例系数 $\beta \alpha$"></a>比例系数 $\beta \alpha$</h4></li><li><p>$\overline{\beta}$:共发射极直流电流放大系数。忽略集电区和基区的空穴和自由电子的漂移运动产生的电流（因为电流很小嘛）所以可以近似看作：<br>$$\overline{\beta}=\cfrac{I_C}{I_B}$$</p><blockquote><p>当$I_B=0$时，$I_C$中仍有电流，我们称为穿透电流（是之前忽略的一小部分来的）<br>$$\overline{\beta}=\cfrac{I_C}{I_B}$$<br>$$I_C=\overline{\beta}I_B$$<br>$$I_E=(1+\overline{\beta})I_B$$<br>上式公式牢记</p></blockquote></li><li><p>$\overline{\alpha}$:共基极直流电流放大系数。同样忽略集电区和基区的空穴和自由电子的漂移运动产生的电流，所以可以近似看作：<br>$$\overline{\alpha}=\cfrac{I_C}{I_E}$$</p></li><li><p>$\overline{\alpha}$和$\overline{\beta}$的关系<br>$$\overline{\beta}=\cfrac{\overline{\alpha}}{1-\overline{\alpha}}$$<br>$$\overline{\alpha}=\cfrac{\overline{\beta}}{1+\overline{\beta}}$$</p></li></ol><p>2020.2.26</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;晶体管的结构以及符号&quot;&gt;&lt;a href=&quot;#晶体管的结构以及符号&quot; class=&quot;headerlink&quot; title=&quot;晶体管的结构以及符号&quot;&gt;&lt;/a&gt;晶体管的结构以及符号&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;结构：发射区（e，emitter）集电区（c，collector），基区（b，basic）&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;发射区重掺杂，基区轻掺杂，集电区面积大&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;符号：箭头：从P区指向N区&lt;/p&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-5 稳压管以及其它二极管</title>
    <link href="http://yoursite.com/2020/02/26/ch4-5%20%E7%A8%B3%E5%8E%8B%E7%AE%A1%E4%BB%A5%E5%8F%8A%E5%85%B6%E5%AE%83%E4%BA%8C%E6%9E%81%E7%AE%A1/"/>
    <id>http://yoursite.com/2020/02/26/ch4-5%20%E7%A8%B3%E5%8E%8B%E7%AE%A1%E4%BB%A5%E5%8F%8A%E5%85%B6%E5%AE%83%E4%BA%8C%E6%9E%81%E7%AE%A1/</id>
    <published>2020-02-26T04:43:24.185Z</published>
    <updated>2020-02-26T06:17:46.060Z</updated>
    
    <content type="html"><![CDATA[<h4 id="稳压二极管"><a href="#稳压二极管" class="headerlink" title="稳压二极管"></a>稳压二极管</h4><ol><li>符号：在二级管的符号上加一个小弯</li><li>稳定电压（$U_Z$）：工作在反向击穿状态时的稳定电压即稳定电压</li><li>额定功耗（$P_Z$）：使用时功耗不超过这个标准，这个功耗由管子本身的属性决定</li><li>稳压电流（$I_Z$）：稳压二极管工作时候有最大电流和最小电流，在这个范围区间的就是稳压电流，小于该值，管子失去稳压作用，大于此值，管子会烧坏<a id="more"></a>$$I_Z=\cfrac{U_Z}{I_Z}$$</li><li>动态电阻（$r_Z$）：一般在几欧姆到十几欧姆<br>$$r_Z=\cfrac{\Delta U_Z}{\Delta I_Z}$$</li><li>温度系数：稳定电压较高（十几伏）一般为雪崩击穿，稳定电压在6伏以下，一般为齐纳击穿，所以稳定电压一般在（5–7V）的稳压管兼有两种击穿，温度稳定性比较好</li><li>有温度补偿的稳压管：将两稳压管相互对接即可，利用一个工作在正向导通（负温度系数），一个工作在反向导通（正温度系数）即可相互补偿，改善温度稳定性。<br>$$U=\pm(U_Z+U_D)$$<blockquote><p>其中$U_D$是正向导通电压</p></blockquote></li><li>如果计算带有稳压二极管的电路，首先要判断稳压二极管能否被击穿，方法是先假设稳压管断开，看 $U_O$ 是否大于 $U_Z$ .<blockquote><p>具体题目翻一下MOOC，这里不记录了</p></blockquote></li><li>稳压电路限流电阻R的选择：就是要保证电流在稳压电流区工作，公式比较长不写了，遇到具体问题再翻书补充</li><li>由稳压二极管构成的限幅电路（单向，双向）：和二极管构成的限幅电路是相似的，但是由于正向导通特性，单向限幅的时候，负端会有0.7V的电压（硅管）<h4 id="其他二极管"><a href="#其他二极管" class="headerlink" title="其他二极管"></a>其他二极管</h4></li><li>变容二极管：利用了二极管的电容特性，具体在高频时候用的比较多，遇到在做深入了解</li><li>光电二极管：被光线照射后产生一个电流</li><li>发光二极管：……</li><li>肖特基二极管：结构是金属和N型材料掺杂，特点是速度快，常用在高频，遇到再深入了解</li></ol><p>2020.2.26</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;稳压二极管&quot;&gt;&lt;a href=&quot;#稳压二极管&quot; class=&quot;headerlink&quot; title=&quot;稳压二极管&quot;&gt;&lt;/a&gt;稳压二极管&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;符号：在二级管的符号上加一个小弯&lt;/li&gt;
&lt;li&gt;稳定电压（$U_Z$）：工作在反向击穿状态时的稳定电压即稳定电压&lt;/li&gt;
&lt;li&gt;额定功耗（$P_Z$）：使用时功耗不超过这个标准，这个功耗由管子本身的属性决定&lt;/li&gt;
&lt;li&gt;稳压电流（$I_Z$）：稳压二极管工作时候有最大电流和最小电流，在这个范围区间的就是稳压电流，小于该值，管子失去稳压作用，大于此值，管子会烧坏
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-4 晶体二极管应用</title>
    <link href="http://yoursite.com/2020/02/25/ch4-4%20%E6%99%B6%E4%BD%93%E4%BA%8C%E6%9E%81%E7%AE%A1%E5%BA%94%E7%94%A8/"/>
    <id>http://yoursite.com/2020/02/25/ch4-4%20%E6%99%B6%E4%BD%93%E4%BA%8C%E6%9E%81%E7%AE%A1%E5%BA%94%E7%94%A8/</id>
    <published>2020-02-25T13:56:18.113Z</published>
    <updated>2020-02-26T04:40:54.035Z</updated>
    
    <content type="html"><![CDATA[<h4 id="整流电路"><a href="#整流电路" class="headerlink" title="整流电路"></a>整流电路</h4><blockquote><p>这里最好有电路图，但是懒得加了，咱们就脑子里回忆吧，哈哈哈</p></blockquote><a id="more"></a><ol><li>二极管加负载<blockquote><p>利用二极管单向导电性，如果输入电压大于0.7V，就输出电压=输入电压，如果输入电压小于0.7V，则二极管截止，输出电压为0.</p></blockquote></li><li>整流桥电路(又称绝对值电路)<br>$$U_o=|U_i|$$<h4 id="限幅电路"><a href="#限幅电路" class="headerlink" title="限幅电路"></a>限幅电路</h4></li><li>上限幅电路</li><li>双向限幅电路<blockquote><p>当输入电压大于二极管和电压源，Uo即二极管管压降和电压源压降之和，当输入电压小于二极管和电压源，$U_o=U_i$</p></blockquote></li></ol><p>2020.2.25</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;整流电路&quot;&gt;&lt;a href=&quot;#整流电路&quot; class=&quot;headerlink&quot; title=&quot;整流电路&quot;&gt;&lt;/a&gt;整流电路&lt;/h4&gt;&lt;blockquote&gt;
&lt;p&gt;这里最好有电路图，但是懒得加了，咱们就脑子里回忆吧，哈哈哈&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-3 晶体二极管的特性及参数</title>
    <link href="http://yoursite.com/2020/02/25/ch4-3%20%E6%99%B6%E4%BD%93%E4%BA%8C%E6%9E%81%E7%AE%A1%E7%9A%84%E7%89%B9%E6%80%A7%E5%8F%8A%E5%8F%82%E6%95%B0/"/>
    <id>http://yoursite.com/2020/02/25/ch4-3%20%E6%99%B6%E4%BD%93%E4%BA%8C%E6%9E%81%E7%AE%A1%E7%9A%84%E7%89%B9%E6%80%A7%E5%8F%8A%E5%8F%82%E6%95%B0/</id>
    <published>2020-02-25T13:55:27.788Z</published>
    <updated>2020-02-25T14:38:58.286Z</updated>
    
    <content type="html"><![CDATA[<h4 id="二极管的伏安特性–指数特性"><a href="#二极管的伏安特性–指数特性" class="headerlink" title="二极管的伏安特性–指数特性"></a>二极管的伏安特性–指数特性</h4><p>$$i_D=I_S(e^\cfrac{U_D}{U_T}-1)$$</p><blockquote><p>注：其中 $I_S$ 为反向饱和电流，$U_T$为热电压，$U_D$为导通电压，$i_D$为正向导通电流</p></blockquote><a id="more"></a><ol><li>正向特性：<blockquote><p>死区电压（门限电压）：正向加电压的过程中，一开始电流很小，然后突然指数级的增加。在电流很小的区间，电压及是死区电压。</p></blockquote></li></ol><blockquote><p>死区电压：室温（27°C）下，硅管：0.5-0.7V，锗管：0.1-0.3V</p></blockquote><p>管压降：工业上–硅管：0.7V，锗管–0.3V<br>2. 反向特性:<br>电流很小，硅管：一般小于0.1uA，锗管小于几十微安</p><h4 id="二极管参数"><a href="#二极管参数" class="headerlink" title="二极管参数"></a>二极管参数</h4><ol><li>直流电阻<br>$$R_D=\cfrac{U_D}{I_D}$$<blockquote><p>正向电阻越小，反向电阻越大，二极管的导电性能越好</p></blockquote></li><li>交流电阻<br>$$r_D=\cfrac{\Delta U_D}{\Delta i_D}$$<blockquote><p>$r_D &lt;&lt; R_D$</p></blockquote></li><li>温度特性<blockquote><p>温度T增大:<br>反向饱和电流增大，死区电压减小<br>雪崩击穿电压增大（温度增加）<br>齐纳击穿电压减小（温度减小）</p></blockquote></li><li>最大整流电流<blockquote><p>二极管允许通过的最大正向平均电流</p></blockquote></li><li>最大反向工作电压</li><li>反向电流<blockquote><p>越小越好，通常反向电流与温度密切相关</p></blockquote></li><li>最高工作频率<br>超过该工作频率，二极管单向导电性能变坏<h4 id="二级管电路模型"><a href="#二级管电路模型" class="headerlink" title="二级管电路模型"></a>二级管电路模型</h4>通常看做，P端正，N端负，硅管的管压降为0.7V，锗管的管压降为0.3V即可。</li></ol><p>2020.2.25</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;二极管的伏安特性–指数特性&quot;&gt;&lt;a href=&quot;#二极管的伏安特性–指数特性&quot; class=&quot;headerlink&quot; title=&quot;二极管的伏安特性–指数特性&quot;&gt;&lt;/a&gt;二极管的伏安特性–指数特性&lt;/h4&gt;&lt;p&gt;$$i_D=I_S(e^\cfrac{U_D}{U_T}-1)$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注：其中 $I_S$ 为反向饱和电流，$U_T$为热电压，$U_D$为导通电压，$i_D$为正向导通电流&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-2 P--N结</title>
    <link href="http://yoursite.com/2020/02/24/ch4-2%20P--N%E7%BB%93/"/>
    <id>http://yoursite.com/2020/02/24/ch4-2%20P--N%E7%BB%93/</id>
    <published>2020-02-24T08:51:42.623Z</published>
    <updated>2020-02-25T13:52:47.678Z</updated>
    
    <content type="html"><![CDATA[<h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><ol><li>PN结：P型半导体和N型半导体交接面处会形成一个有特殊物理性质的薄层，称为PN结<a id="more"></a></li><li>PN结的形成：由于两边载流子浓度差————&gt;多子扩散（N型半导体–自由电子，P型半导体–空穴）————&gt;交接处复合，显露出正负原子————形成内电场————&gt;阻碍多子扩散，利于少子漂移————&gt;动态平衡，形成PN结</li><li>中间形成电场的区域称为<strong><em>耗尽区或势垒区或阻挡区</em></strong>，在掺杂浓度不对称的PN结中，掺杂浓度大的地方延伸小，掺杂浓度小的地方，延伸大（因为两边正负电荷数量相等）<h4 id="PN结的导电特性"><a href="#PN结的导电特性" class="headerlink" title="PN结的导电特性"></a>PN结的导电特性</h4></li><li>正向偏置：外部添加P正，N负的电场，与内电场相互抵消，耗尽层变窄，则扩散作用加强，漂移运动减弱，（也就是说N结处的自由电子向浓度低的P结移动，空穴从P向N移动），因此形成正向电流（从P——&gt;N）<blockquote><p>注：电流方向是正电荷移动方向，与负电荷移动方向相反，因为在定义电流方向的时候，还没有发现电子……</p></blockquote></li><li>反向偏置：外部添加N正，P负的电场，产生与内部电场方向相同的电场，则耗尽区变宽，扩散运动减弱，漂移运动加强，反向电流很小（少子提供电流）</li><li>可以认为是（<strong><em>正偏—导通，反偏—截止</em></strong>）<h4 id="PN结的击穿特性"><a href="#PN结的击穿特性" class="headerlink" title="PN结的击穿特性"></a>PN结的击穿特性</h4></li><li>当反向电压超过反向截止电压的时候，反向电流会急剧增大，有两种机理</li><li>雪崩击穿：加反向电压，耗尽区变大，少子漂移时，被加速，动能过大，撞击共价键，使自由电子挣脱束缚，然后蝴蝶效应，一起被加速，到处撞击……反向电流变大</li><li>齐纳击穿：在重掺杂的PN结中，耗尽区很窄，所以较小的反向电压会产生很大的电场，一定的强度就可以将共价键中的价电子直接拉出，产生大量自由电子和空穴对，使反向电流急剧增大。<blockquote><p>一般而言，对于硅材料的PN结，截止电压 &gt; 7V为雪崩击穿，截止电压 &lt; 5V为齐纳击穿；截止电压在 5 ~ 7 V时，两种击穿都有。</p></blockquote></li></ol><blockquote><p>只要限制击穿后的电流，击穿并不损坏 PN 结（可逆性）</p></blockquote><h4 id="PN结的电容特性"><a href="#PN结的电容特性" class="headerlink" title="PN结的电容特性"></a>PN结的电容特性</h4><ol><li>PN结有电容特性，由势垒电容和扩散电容两部分组成</li><li>势垒电容：高阻的耗尽区，与平板电容器相似，正偏电压越大，容值越小<br>$$C_T=\cfrac{\epsilon S}{d}$$<blockquote><p>d 是耗尽区长度，S是耗尽区横截面积</p></blockquote></li><li>扩散电容：正向偏置的PN结，由于多子扩散，会有电容特性<br>$$C_D=\cfrac{\tau I}{U_T}$$<blockquote><p>$C_T$ 和 $C_D$ 都随外加电压的变化而变化，且都非线性，低频时可以忽略，高频时需要考虑……</p></blockquote></li></ol><p>2020.2.24</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;PN结：P型半导体和N型半导体交接面处会形成一个有特殊物理性质的薄层，称为PN结
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch4-1 半导体物理基础</title>
    <link href="http://yoursite.com/2020/02/24/ch4-1%20%E5%8D%8A%E5%AF%BC%E4%BD%93%E7%89%A9%E7%90%86%E5%9F%BA%E7%A1%80/"/>
    <id>http://yoursite.com/2020/02/24/ch4-1%20%E5%8D%8A%E5%AF%BC%E4%BD%93%E7%89%A9%E7%90%86%E5%9F%BA%E7%A1%80/</id>
    <published>2020-02-24T06:15:33.978Z</published>
    <updated>2020-02-25T13:52:45.505Z</updated>
    
    <content type="html"><![CDATA[<h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><ol><li>物体分为：导体，半导体，绝缘体。其中半导体的导电能力随着温度，光照，掺杂等外界因素所改变（价带，禁带，导带）<a id="more"></a></li><li>本征半导体：纯净的硅和锗单晶体是本征半导体，硅和锗的最外层电子都是4个，则4个最外层电子又称作价电子（物理，化学性质主要取决于最外层电子）。</li><li>本征激发：价电子获得足够大的能力（即外界因素干扰），挣脱共价键的束缚，成为自由电子，产生了自由电子以及空穴，本征激发产生的空穴和自由电子的数目相等。</li><li>载流子：带负电的自由电子和带正电的空穴，都可以导电，统称为载流子。</li><li>复合：是激发的逆过程，即自由电子填入空穴，释放能量，从而消失一对载流子的过程</li><li>硅的温度温度性比锗要好，所以集成电路多用硅（因为硅的电子挣脱共价键的束缚需要的能力大于锗，硅1.21eV，锗0.78eV）<h4 id="N型半导体和P型半导体"><a href="#N型半导体和P型半导体" class="headerlink" title="N型半导体和P型半导体"></a>N型半导体和P型半导体</h4>由于本征半导体的导电能力比较弱，掺杂一些元素的原子可以提高半导体的导电能力，杂质半导体分为N型半导体和P型半导体。</li><li>N型半导体：在本征半导体中掺入5价原子，即N型半导体。提供自由电子（称为<strong><em>施主电离</em></strong>），其中多数载流子—自由电子，少数载流子—空穴，半导体仍然保持中性（自由电子为负，原子即表示正性，抵消）</li><li>多子：多数载流子，少子–少数载流子</li><li>少子浓度和温度关系很大，多子与温度关系不大</li><li>P型半导体：在本征半导体中掺入3价原子，即P型半导体。提供空穴（称为<strong><em>受主电离</em></strong>），其中多数载流子—空穴，少数载流子—自由电子，半导体仍然保持中性（空穴显正性，原子显负，抵消）<h4 id="漂移电流和扩散电流"><a href="#漂移电流和扩散电流" class="headerlink" title="漂移电流和扩散电流"></a>漂移电流和扩散电流</h4></li><li>半导体电流：即自由电子形成的电流加上空穴形成的电流。</li><li>漂移电流：在电场的作用下，自由电子逆电场方向漂移，空穴顺着电场方向漂移，这样产生的电流称为漂移电流（受到<strong><em>载流子浓度，迁移率，电场强度</em></strong>的影响）</li><li>迁移率：单位电场的作用下，自由电子的移动速度比空穴快</li><li>扩散电流：半导体中浓度不均匀分布的时候，载流子会从高浓度区域向低浓度区域扩散，从而形成扩散电流，（受到<strong><em>载流子的浓度差或者叫浓度梯度</em></strong>的影响）</li></ol><p>2020.2.24</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;物体分为：导体，半导体，绝缘体。其中半导体的导电能力随着温度，光照，掺杂等外界因素所改变（价带，禁带，导带）
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch2-6 微分器</title>
    <link href="http://yoursite.com/2020/02/20/ch2-6%20%E5%BE%AE%E5%88%86%E5%99%A8/"/>
    <id>http://yoursite.com/2020/02/20/ch2-6%20%E5%BE%AE%E5%88%86%E5%99%A8/</id>
    <published>2020-02-20T11:27:34.939Z</published>
    <updated>2020-02-21T14:08:34.108Z</updated>
    
    <content type="html"><![CDATA[<h3 id="微分器"><a href="#微分器" class="headerlink" title="微分器"></a>微分器</h3><blockquote><p>目标：$Uo(t)=k\cfrac{dui(t)}{dt}$</p></blockquote><blockquote><p>即输出与输入的微分成正比，微分是积分的逆运算。</p></blockquote><a id="more"></a><h4 id="微分器电路"><a href="#微分器电路" class="headerlink" title="微分器电路"></a>微分器电路</h4><p>(A)时域分析</p><blockquote><p>由于微分和积分是互逆的关系，所以把反相输入端的电容和反馈网络中电阻的位置互换即是，微分器电路，分析与积分器相似：<br>$$U_o(t)=-U_R(t)=-i_R(t)R$$<br>$$i_R(t)=i_c(t)=C\cfrac{dUc(t)}{dt}$$<br>$$Uo(t)=-RC\cfrac{dUi(t)}{dt}=-\tau \cfrac{dUi(t)}{dt}$$<br>(B)频域分析<br>$$Au(j\omega)=\cfrac{Uo(j\omega)}{Ui(j\omega)}=-j\omega RC=-j\omega\tau$$<br>因为是反相输入端，抵消一个负号，所以是 j，超前90度</p></blockquote><ol><li>相位超前90度</li><li>增益的模 $|Au(j\omega)|=\omega RC$</li><li>有个结论：频率越高增益越大，且有一个90度的超前相移。<h4 id="微分器的高频增益"><a href="#微分器的高频增益" class="headerlink" title="微分器的高频增益"></a>微分器的高频增益</h4><blockquote><p>由于电容是通高频，阻低频，所以，若干反相输入端有高频信号（高频噪声）输入，电容相当于是短路，高频信号会被反相放大，影响正常输出信号。</p></blockquote><h5 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h5></li><li>在反相端电容的前面加一个小电阻$R_2$，要远远小于反馈网络中的负载电阻Rf阻值：<br>$$A_u(j\omega)=\cfrac{U_o(j\omega)}{U_i(j\omega)}=-\cfrac{R}{R_2+\cfrac{1}{j\omega C}}=-\cfrac{j\omega RC}{1+j\omega R_2C}$$<blockquote><p>当$R_2$非常小的时候，可以忽略，就相当于$-j\omega RC$</p></blockquote></li><li>因为高频噪声的原因，所以微分器通常在工程中会被积分器所取代，通过解微分方程就可以，举课上面的例子：<br>$$\cfrac{d^2U_o(t)}{dt^2}+10\cfrac{dU_o(t)}{dt}+2U_o(t)=Ui(t)$$<br>$$\cfrac{dU_o(t)}{dt}=\int[u_i(t)-10\cfrac{dU_o(t)}{dt}-2U_o(t)]dt$$<br>$$U_o(t)=\iint U_i(t)dt-2\iint U_o(t)dt-10\int U_o(t)dt$$<h4 id="微分器实验"><a href="#微分器实验" class="headerlink" title="微分器实验"></a>微分器实验</h4></li><li>三角波方波变化<blockquote><p>微分嘛，三角波下降的时候，斜率不变且小于0，即 $\cfrac{U_i(t)}{dt}&lt;0$,和前面负号抵消，所以是输出高电平，同理，三角波上升时，斜率不变且大于0，即 $\cfrac{U_i(t)}{dt}&gt;0$,前面加一个负号，所以输出低电平，最后输出的是方波。</p></blockquote></li><li>方波变化尖脉冲<blockquote><p>输入方波为高低电平的时候，输入电平没有变化，斜率为0，高低电平直接是突变，斜率变化很大，所以有个尖脉冲。</p></blockquote></li></ol><p>2020.2.21</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;微分器&quot;&gt;&lt;a href=&quot;#微分器&quot; class=&quot;headerlink&quot; title=&quot;微分器&quot;&gt;&lt;/a&gt;微分器&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;目标：$Uo(t)=k\cfrac{dui(t)}{dt}$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;即输出与输入的微分成正比，微分是积分的逆运算。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch2-5 积分器</title>
    <link href="http://yoursite.com/2020/02/20/ch2-5%20%E7%A7%AF%E5%88%86%E5%99%A8/"/>
    <id>http://yoursite.com/2020/02/20/ch2-5%20%E7%A7%AF%E5%88%86%E5%99%A8/</id>
    <published>2020-02-20T11:26:55.734Z</published>
    <updated>2020-02-21T14:11:44.501Z</updated>
    
    <content type="html"><![CDATA[<h3 id="积分器"><a href="#积分器" class="headerlink" title="积分器"></a>积分器</h3><blockquote><p>目标：$Uo(t)=k\int Ui(t)dt$</p></blockquote><p>即输出与输入的积分成正比</p><a id="more"></a><h4 id="反相积分电路"><a href="#反相积分电路" class="headerlink" title="反相积分电路"></a>反相积分电路</h4><blockquote><p>就是将反相比例放大器中的 Rf 换成电容</p></blockquote><p>(A)时域分析<br>$$Uo(t)=-Uc(t)=-\cfrac{Q}{C}=-\cfrac{1}{C}\int ic(t)dt$$<br>$$=-\cfrac{1}{C}\int \cfrac{Ui(t)}{R}dt=-\cfrac{1}{RC}\int Ui(t)dt=-\cfrac{1}{\tau}\int Ui(t)dt$$<br>其中 $\tau$ = RC 是积分常数</p><p>(B)频域分析</p><blockquote><p>这里和反相比例放大器是一个分析方法<br>$$Au(j\omega)=\cfrac{Uo(j\omega)}{Ui(j\omega)}=-\cfrac{1}{j\omega RC}=-\cfrac{1}{j\omega \tau}$$<br>因为这里有一个$\cfrac{1}{j}=-j$ ,又是反相输入端，最外面那个负号抵消，所以：</p></blockquote><ol><li>相位滞后90度，</li><li>增益的模：$|Au(j\omega)|=\cfrac{1}{\omega RC}$</li><li>这里有个结论：频率越高，衰减越大(<strong><em>这里是真的不明白为什么了？？</em></strong>)<h4 id="差分积分器电路"><a href="#差分积分器电路" class="headerlink" title="差分积分器电路"></a>差分积分器电路</h4><blockquote><p>这里和简单减法器是相似，把 Rf 和下面对应的电阻换成电容即是差分积分器啦<br>$$Uo(j\omega)=\cfrac{1}{j\omega RC}[Ui1(j\omega)-Ui2(j\omega)]$$<br>$$Uo(j\omega)=\cfrac{1}{RC}\int[Ui1(t)-Ui2(t)]dt$$<br>上面一个是频域形式，一个是时域形式啦</p></blockquote><h4 id="同相积分电路"><a href="#同相积分电路" class="headerlink" title="同相积分电路"></a>同相积分电路</h4><blockquote><p>就是将反相输入端接地，Ui2 = 0<br>$$Uo(j\omega)=\cfrac{1}{j\omega RC}Ui1(j\omega)$$<br>$$Uo(j\omega)=\cfrac{1}{RC}\int Ui1(t)dt$$</p></blockquote><h4 id="积分器设计"><a href="#积分器设计" class="headerlink" title="积分器设计"></a>积分器设计</h4><blockquote><p>这里注意一下设计积分器首先要确定时间常数 $\tau$ ,又因为电容的可选类型比较少，电阻相对要多的多，所以，先确定电容的值，在确定电阻的阻值。</p></blockquote></li></ol><blockquote><p>电容单位 F 换算：1F = 1 * 10^6 $\mu$F(微法)_…………_1 $\mu$F = 1 * 10^3 nF_…………1 $\mu$F = 1 * 10^6 pF</p></blockquote><h4 id="单级积分器电路"><a href="#单级积分器电路" class="headerlink" title="单级积分器电路"></a>单级积分器电路</h4><blockquote><p>由于电容是通交流隔直流的，所以只在负反馈网络加电容，会使直流无法反馈，造成积分器不正常工作，或有噪声的情况，（<strong><em>这里加入的电阻要保证R &gt;&gt;&gt; 10</em>Ri</strong>）,不然对增益会很大的影响。</p></blockquote><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><ol><li>方波转换成三角波<blockquote><p>反相输入端输入方波，当输入为高电平，输入端高电位，输出端拉低，反馈回去，电位下降；同理反相端输入为低电平，输出端拉高电位上升，反馈回去。输出端就是三角波了。</p></blockquote></li><li>正弦波和余弦波互相转换（<strong><em>因为相位互相差90度</em></strong>）</li></ol><p>2020.2.20</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;积分器&quot;&gt;&lt;a href=&quot;#积分器&quot; class=&quot;headerlink&quot; title=&quot;积分器&quot;&gt;&lt;/a&gt;积分器&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;目标：$Uo(t)=k\int Ui(t)dt$&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;即输出与输入的积分成正比&lt;/p&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>生不逢时—曾国藩</title>
    <link href="http://yoursite.com/2020/02/20/%E7%94%9F%E4%B8%8D%E9%80%A2%E6%97%B6%E2%80%94%E6%9B%BE%E5%9B%BD%E8%97%A9/"/>
    <id>http://yoursite.com/2020/02/20/%E7%94%9F%E4%B8%8D%E9%80%A2%E6%97%B6%E2%80%94%E6%9B%BE%E5%9B%BD%E8%97%A9/</id>
    <published>2020-02-20T07:37:10.015Z</published>
    <updated>2020-02-21T14:12:19.008Z</updated>
    
    <content type="html"><![CDATA[<h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>寒假时间转瞬即逝，为数不多的收获之一是在受到白岩松的启发后，将《曾国藩》全书（一共上中下三册）看完，略有启发，这里记录下来。</p><a id="more"></a><h3 id="正文"><a href="#正文" class="headerlink" title="正文"></a>正文</h3><p>本书讲诉曾国藩年轻气盛之时，历经回乡守孝，出山创建湘军，官场不顺，三败石达开，至攻克江宁（九弟-曾国荃），平定太平天国，整饬两江，被迫与捻军作战不利，到处理天津教案名声败北，最后开始新办洋务，送幼童出国学习技术，最后在矛盾与怅然中离去。</p><h4 id="时代所限的“晚清名臣”"><a href="#时代所限的“晚清名臣”" class="headerlink" title="时代所限的“晚清名臣”"></a>时代所限的“晚清名臣”</h4><p>曾国藩在晚年多次提及天命，用了比喻，与康福提到，自己和皇上都是棋盘中的棋子，那谁是执子人呢，曾国藩答：天命。这里的天命是什么，是时代命运，清朝末年，妇人垂帘，皇子孱弱，国运衰微，到了什么地步呢，堂堂江宁，汉唐繁盛至极，现今竟然城内屋檐下，饿殍，乞丐随处可见，比起太平天国时期，竟然是比之远远不如，皇室中，恭亲王和西宫太后内斗，内忧外患，西方列强制度先进，技术先进，压迫清政府。用李鸿章的话是“三千年之未有大变局”。</p><p>曾国藩从小受到程朱理学理学，孔孟熏陶，心中把忠于皇室作为立人为本的标杆，自创湘军，希望也是帮助君主，使天下太平，百姓过上好日子，心只是可惜，天命不从，若是汉唐鼎盛时代，曾国藩一定可有所为，流芳百世，可惜时代变化，恰恰是忠于皇室，忠于国运衰微的皇室让他走上了一条错误的道路。</p><p>曾国藩在晚年的时候心病严重，一心希望世间太平，让人民过上好日子，可事与愿违，街头百姓无饭可吃，山头绿林强盗与官府打为一片，官僚腐败彻底，细细一想，自己用了十几年的生涯创立湘军，最后攻下江宁，平定太平天国，又是为了什么呢。书中说曾国藩晚年在与郭嵩焘的交谈中，郭嵩焘大力批评曾国藩居于忠于皇室的“小节”，而不顾民族和中华命运的“大节”，不知道是不是真的有这次交谈，但是相信若有机会与曾国藩对话，这也是作者心中想表达出来的意思吧。</p><p>曾国藩也只是时代轮换下的一个可怜人了。</p><p>李鸿章是曾国藩的弟子，在书中，曾国藩知道自己时日不多，在晚年与所有健在的人，一一作了告别，包括他最得意的弟子——李鸿章，此后书中再无李鸿章出现，我感兴趣的是李鸿章是怎么想的呢，众人熟知的是李鸿章也是晚清四大名臣之一，被称作是“中国的俾斯麦”，在曾国藩去世之后，本书已经结束，但历史没有，李鸿章一直作为晚清的执掌大臣，签署了各个辱国条约，李鸿章又是怎样的心理变化呢，还有军事天才，左宗棠，他在面临外忧内患的情况下又做了什么，，思考了什么呢，历史的趣味大概就在这里了。</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h3&gt;&lt;p&gt;寒假时间转瞬即逝，为数不多的收获之一是在受到白岩松的启发后，将《曾国藩》全书（一共上中下三册）看完，略有启发，这里记录下来。&lt;/p&gt;
    
    </summary>
    
    
      <category term="生活感想" scheme="http://yoursite.com/categories/%E7%94%9F%E6%B4%BB%E6%84%9F%E6%83%B3/"/>
    
    
  </entry>
  
  <entry>
    <title>ch2-4 相减器</title>
    <link href="http://yoursite.com/2020/02/19/ch2-4%20%E7%9B%B8%E5%87%8F%E5%99%A8/"/>
    <id>http://yoursite.com/2020/02/19/ch2-4%20%E7%9B%B8%E5%87%8F%E5%99%A8/</id>
    <published>2020-02-19T13:46:17.617Z</published>
    <updated>2020-02-21T14:11:43.415Z</updated>
    
    <content type="html"><![CDATA[<h3 id="相减器"><a href="#相减器" class="headerlink" title="相减器"></a>相减器</h3><p>目标：使输入的两个信号相减输出<br>$$Uo = a Ui1 - b Ui2$$</p><blockquote><p>那么令同相端为被减数（Ui1），反相端为减数（Ui2），即可</p></blockquote><a id="more"></a><h4 id="a-b-1"><a href="#a-b-1" class="headerlink" title="$a = b + 1$ :"></a>$a = b + 1$ :</h4><blockquote><p>将Ui1和Ui2分别接地，可得到同相比例和反相比例放大器的 Uo，然后叠加原理，相加即是输出电压</p></blockquote><p>推导一下：</p><blockquote><p>令Ui2 接地,则为同相比例放大器<br>$$Uo = (1 + \cfrac{Rf}{R2})Ui1$$<br>令Ui1接地，则为反相比例放大器<br>$$Uo = -\cfrac{Rf}{R2}Ui2$$<br>用叠加定理合起来就是<br>$$Uo = (1 + \cfrac{Rf}{R2})Ui1 -\cfrac{Rf}{R2}Ui2 $$</p></blockquote><h4 id="a-lt-b-1"><a href="#a-lt-b-1" class="headerlink" title="a &lt; b + 1"></a>a &lt; b + 1</h4><blockquote><p>要减小a，减小同相输入端的电压（分压即可）</p></blockquote><p>在U+处接上一个电阻，这个电阻接地即可<br>$$Uo = (1 + \cfrac{Rf}{R2})(\cfrac{R4}{R1+R4})Ui1 -\cfrac{Rf}{R2}Ui2 $$</p><h4 id="a-gt-b-1"><a href="#a-gt-b-1" class="headerlink" title="a &gt; b + 1"></a>a &gt; b + 1</h4><blockquote><p>同理增加a，减小Ui2的放大倍数就可以了，Rf不变，增加分母上的值即可，即，在R2上面并联一个电阻即可<br>$$Uo = (1+\cfrac{Rf}{R2+R})Ui1-\cfrac{Rf}{R2}Ui2$$<br>由于虚断的原因，在反相比例放大器的情况下，R两端电压都是0，所以R不起作用</p></blockquote><h4 id="a-b-k"><a href="#a-b-k" class="headerlink" title="a = b = k"></a>a = b = k</h4><blockquote><p>常用的模型是使R2 = R1，Rf = R3 ，就可以满足 a = b = k</p></blockquote><p>则：<br>$$Uo = \cfrac{R3}{R2}(Ui1-Ui2) = \cfrac{Rf}{R2}(Ui1-Ui2)$$</p><h4 id="相减器设计实例"><a href="#相减器设计实例" class="headerlink" title="相减器设计实例"></a>相减器设计实例</h4><ol><li>实现：Uo = 5 ( Ui1 - Ui2 )<blockquote><p>放大倍数相同，说明Rf = R4，R1 = R2，……</p></blockquote></li><li>实现：Uo = 5 Ui1 - 8 Ui2<blockquote><p>写出 a &lt; b 的情况（公式）</p></blockquote></li><li>实现：Uo = 8 Ui1 - 5 Ui2</li></ol><h4 id="相减器的应用"><a href="#相减器的应用" class="headerlink" title="相减器的应用"></a>相减器的应用</h4><ol><li>直流电平移位 <blockquote><p>案例里面是将信号接到反相输入端，同相输入端接直流信号，当然这个就反相放大了。同相放大的话就是，将信号接入同相输入端即可</p></blockquote></li><li>抑制共模干扰<blockquote><p>共模干扰——百度百科：共模干扰指的是干扰电压在信号线及其信号地线上的幅度相同。(<strong><em>额？还是觉得哪里不明白</em></strong>)  </p></blockquote></li></ol><blockquote><p>这里孙老师是举了例子，是把两个相同信号源的输入信号放入相减器中间，使两个相同的noise相减，抵消后输入仪器(<strong><em>妙极了</em></strong>)</p></blockquote><h4 id="简单相减器存在的问题"><a href="#简单相减器存在的问题" class="headerlink" title="简单相减器存在的问题"></a>简单相减器存在的问题</h4><ol><li>如果要求两个输入信号的放大倍数相同的话，那么要求上下对应的电阻阻值相同——————&gt;调节增益困难，要同步调节上下两个电阻</li><li>输入电阻偏小(注意哦，不是放大器内部的输入电阻，是同反相输入端上的电阻)，对两个信号源的影响不同(<strong><em>这个是为什么呢？</em></strong>)</li></ol><h4 id="仪表放大器"><a href="#仪表放大器" class="headerlink" title="仪表放大器"></a>仪表放大器</h4><blockquote><p>你好，放大器一书中把仪表放大器归于功能放大器当中哦~</p></blockquote><blockquote><p>结构就是在简单的相减器的前面增加两个同相比例放大器</p></blockquote><h6 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h6><ol><li>两个同相比例放大器由于放大器内部输入电阻阻值趋于无穷大，所以对信号输入起到了很好的隔绝作用。</li><li>通过调节两个同相比例放大器中间的电阻阻值可以很方便的改变仪表放大器的增益，为啥呢，也比较简单，我推导一下：<blockquote><p>首先由于虚断，同相比例放大器的两个信号输入端无电流(阻值太大嘛~),由于虚端，两个信号输入端的电压相同，即信号相同，则：<br>$$Uo1-Uo2=ix(R+R+Rx)=\cfrac{Ui1-Ui2}{Rx}(2R+Rx)$$<br>其中Rx是两个同相比例放大器之间的电压，Ui1和Ui2是后半部分简单减法器的输入信号端<br>$$Uo=\cfrac{R4}{R3}(Uo1-Uo2)=\cfrac{R4}{R3}(1+\cfrac{2R}{Rx}(Ui1-Ui2)$$<br>$$Au=\cfrac{Uo}{Ui1-Ui2}=\cfrac{R4}{R3}(1+\cfrac{2R}{Rx})$$<br>所以增益可以由Rx改变啦</p></blockquote></li></ol><p>2020.2.20</p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;相减器&quot;&gt;&lt;a href=&quot;#相减器&quot; class=&quot;headerlink&quot; title=&quot;相减器&quot;&gt;&lt;/a&gt;相减器&lt;/h3&gt;&lt;p&gt;目标：使输入的两个信号相减输出&lt;br&gt;$$Uo = a Ui1 - b Ui2$$&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;那么令同相端为被减数（Ui1），反相端为减数（Ui2），即可&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
  <entry>
    <title>ch2-3 相加器</title>
    <link href="http://yoursite.com/2020/02/19/ch2-3%20%E7%9B%B8%E5%8A%A0%E5%99%A8/"/>
    <id>http://yoursite.com/2020/02/19/ch2-3%20%E7%9B%B8%E5%8A%A0%E5%99%A8/</id>
    <published>2020-02-19T13:46:17.612Z</published>
    <updated>2020-02-21T14:11:42.356Z</updated>
    
    <content type="html"><![CDATA[<h3 id="由运放构成的相加器"><a href="#由运放构成的相加器" class="headerlink" title="由运放构成的相加器"></a>由运放构成的相加器</h3><h4 id="传统电阻分压"><a href="#传统电阻分压" class="headerlink" title="传统电阻分压"></a>传统电阻分压</h4><blockquote><p>缺点：1. 信号会衰减，无法放大；2. 加上负载的话，分压系数会变化；3. 两端初始的信号源也会互相干扰（可能有电场等相互干扰吧）————&gt;加上运算放大器解决问题</p></blockquote><a id="more"></a><h4 id="同相加法器"><a href="#同相加法器" class="headerlink" title="同相加法器"></a>同相加法器</h4><blockquote><p>运放输入电阻趋近于无穷大，输出电阻趋近于0，起到隔离放大的作用（<strong><em>这里的隔离是指的隔离什么呢？？是输入信号的噪声吗？？？</em></strong>）</p></blockquote><blockquote><p>首先这里是在同相比例放大器的基础上，在输入端加入两个分开的电源以及负载（这里也要记住在脑子里），负载分别是R1和R2，输入电压为U+<br>$$U+=\cfrac{R2}{R1+R2}Ui1+\cfrac{R1}{R1+R2}Ui2$$<br>这个公式怎么推导的，这里要记住，对Ui1和Ui2分开考虑，当对Ui1考虑时，就把Ui2接地，当对Ui2考虑时就把Ui1接地，分别求出Ui1和Ui2对U+的影响，然后叠加起来即为U+处的电压<br>$$Uo = (1+\cfrac{Rf}{R})U+$$<br>$$Uo = (1+\cfrac{Rf}{R})(\cfrac{R2}{R1+R2}Ui1+\cfrac{R1}{R1+R2}Ui2)$$<br>若R1 = R2<br>$$Uo = \cfrac{1}{2}(1+\cfrac{Rf}{R})(Ui1 + Ui2)$$<br>缺点：两信号源之间依然会互相影响（接在一起肯定会互相影响）</p></blockquote><h4 id="反相相加器"><a href="#反相相加器" class="headerlink" title="反相相加器"></a>反相相加器</h4><blockquote><p>这里是在反相比例放大器的基础上，将反相输入端接上分别的信号源以及负载(<strong><em>图请记在脑子里，请记得反相输入端是虚地</em></strong>)</p></blockquote><blockquote><p>根据叠加原理，可得：<br>$$Uo = -(\cfrac{Rf}{R1}Ui1+\cfrac{Rf}{R2}Ui2+\cfrac{Rf}{R3}Ui3)$$<br>当R1 = R2 = R3 时<br>$$Uo = -\cfrac{Rf}{R}(Ui1+Ui2+Ui3)$$<br>优点：因为反相端虚地，各电流值由该支路信号源和电阻独立决定，各个信号源之间就不会影响。(<strong><em>这里我就奇怪了，不管是虚地还是啥的，两个连接上的信号源对这个节点就没有影响了？？？</em></strong>)</p></blockquote><h4 id="反相加法器的设计"><a href="#反相加法器的设计" class="headerlink" title="反相加法器的设计"></a>反相加法器的设计</h4><p>(<strong><em>记牢反相比例，同相比例放大器的闭环增益公式很重要</em></strong>)<br>$$Auf = -\cfrac{Rf}{R}$$</p><ol><li>实现：Uo = -6( Ui1 + Ui2 )———要求闭环输入电阻Rif大于等于30k<blockquote><p>注：脑子里算算就行了</p></blockquote></li><li>实现：Uo = - 5<em>Ui1 - 8</em>Ui2 - 3*Ui3<blockquote><p>由于反馈网络负载是固定的，所以先确定Rf，剩下各自算算就行，脑子里过一下</p></blockquote><h4 id="同相加法器的设计"><a href="#同相加法器的设计" class="headerlink" title="同相加法器的设计"></a>同相加法器的设计</h4>(<strong><em>记牢反相比例，同相比例放大器的闭环增益公式很重要</em></strong>)<br>$$Auf = (1 + \cfrac{Rf}{R})$$<br>对于同相加法器，对信号的放大倍数还取决于R1 ，R2<br>$$Uo = (1+\cfrac{Rf}{R})(\cfrac{R2}{R1+R2}Ui1+\cfrac{R1}{R1+R2}Ui2)$$</li><li>实现：Uo = 9(Ui1 + Ui2)<blockquote><p>对两个信号放大倍数相同，有公式可知，由于增益不变，放大倍数也不变，所以R1 = R2<br>$$Uo = \cfrac{1}{2}(1+\cfrac{Rf}{R})(Ui1 + Ui2)$$<br>剩下参数自己设一下都行</p></blockquote></li><li>实现：Uo = 2 Ui1 + 5 Ui2<br>同相加法器，增益不变嘛，所以放大倍数由R1 ，R2（信号源上的负载决定），所以 R1和R2求比值，剩下参数自己选择代入即可</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;由运放构成的相加器&quot;&gt;&lt;a href=&quot;#由运放构成的相加器&quot; class=&quot;headerlink&quot; title=&quot;由运放构成的相加器&quot;&gt;&lt;/a&gt;由运放构成的相加器&lt;/h3&gt;&lt;h4 id=&quot;传统电阻分压&quot;&gt;&lt;a href=&quot;#传统电阻分压&quot; class=&quot;headerlink&quot; title=&quot;传统电阻分压&quot;&gt;&lt;/a&gt;传统电阻分压&lt;/h4&gt;&lt;blockquote&gt;
&lt;p&gt;缺点：1. 信号会衰减，无法放大；2. 加上负载的话，分压系数会变化；3. 两端初始的信号源也会互相干扰（可能有电场等相互干扰吧）————&amp;gt;加上运算放大器解决问题&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="模电笔记" scheme="http://yoursite.com/categories/%E6%A8%A1%E7%94%B5%E7%AC%94%E8%AE%B0/"/>
    
    
  </entry>
  
</feed>
